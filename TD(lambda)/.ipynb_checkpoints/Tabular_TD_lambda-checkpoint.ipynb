{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "491034fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from standard.helper import initialize_grids, initialize_q_table, initialize_state_dict, get_closest_in_grid\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class TabularTDLambda:\n",
    "#     def __init__(self, env, lmbda=0.5, gamma=0.99, lr=0.1):\n",
    "#         self.lmbda = lmbda\n",
    "#         self.gamma = gamma\n",
    "#         self.lr = lr\n",
    "#         self.values = np.zeros(env.observation_space.n, np.float64)\n",
    "#         self.eligibility_trace = np.zeros_like(self.values, np.float64)\n",
    "#\n",
    "#     def get_action(self, env, epsilon):\n",
    "#         if random.uniform(0, 1) < epsilon:\n",
    "#             return env.action_space.sample()\n",
    "#\n",
    "#     def update(self, env, state, reward, next_state):\n",
    "#         delta = reward + self.gamma * self.values[next_state] - self.values[state]\n",
    "#         for x in range(env.observation_space.n):\n",
    "#             self.eligibility_trace[x] = self.gamma * self.lmbda * self.eligibility_trace[x]\n",
    "#             if state == x:\n",
    "#                 self.eligibility_trace[x] = 1\n",
    "#             self.values[x] += self.lr * delta * self.eligibility_trace[x]\n",
    "\n",
    "def epsilon_greedy(env, state, epsilon, state_to_qtable, q):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    s = state_to_qtable[get_closest_in_grid(state, grid_x, grid_v)]\n",
    "    return np.argmax(q[s])\n",
    "\n",
    "\n",
    "def train(q, env, epsilon, num_training_episodes, gamma, lmbda, grid_x, grid_v, lr, debug=True):\n",
    "\n",
    "    steps_list = []\n",
    "\n",
    "    for episode in range(num_training_episodes):\n",
    "        state = env.reset()[0]\n",
    "        z = np.zeros(q.shape)\n",
    "        steps = 0\n",
    "\n",
    "        while True:\n",
    "            action = epsilon_greedy(env, state, epsilon, state_to_qtable, q)\n",
    "\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Update eligibility trace\n",
    "            z = gamma * lmbda * z\n",
    "            z[state_to_qtable[get_closest_in_grid(state, grid_x, grid_v)]][action] += 1\n",
    "\n",
    "            delta = reward + gamma * (q[state_to_qtable[get_closest_in_grid(new_state, grid_x, grid_v)]]\n",
    "                                      - q[state_to_qtable[get_closest_in_grid(state, grid_x, grid_v)]])\n",
    "\n",
    "            q = q + lr * delta * z\n",
    "\n",
    "            # If done, finish the episode\n",
    "            if terminated:\n",
    "                steps_list.append(steps)\n",
    "                if debug and (episode+1) % 10 == 0:\n",
    "                    print(f\"Training episode: {episode}, lr: {lr}, lambda: {lmbda} - steps: {steps}\")\n",
    "                break\n",
    "\n",
    "            steps += 1\n",
    "            state = new_state\n",
    "\n",
    "    return steps_list, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1c8aa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode: 9, lr: 0.1, lambda: 0.5 - steps: 1167\n",
      "Training episode: 19, lr: 0.1, lambda: 0.5 - steps: 1065\n",
      "Training episode: 29, lr: 0.1, lambda: 0.5 - steps: 569\n",
      "Training episode: 39, lr: 0.1, lambda: 0.5 - steps: 1728\n",
      "Training episode: 49, lr: 0.1, lambda: 0.5 - steps: 630\n",
      "Training episode: 59, lr: 0.1, lambda: 0.5 - steps: 535\n",
      "Training episode: 69, lr: 0.1, lambda: 0.5 - steps: 574\n",
      "Training episode: 79, lr: 0.1, lambda: 0.5 - steps: 509\n",
      "Training episode: 89, lr: 0.1, lambda: 0.5 - steps: 506\n",
      "Training episode: 99, lr: 0.1, lambda: 0.5 - steps: 410\n",
      "Training episode: 109, lr: 0.1, lambda: 0.5 - steps: 315\n",
      "Training episode: 119, lr: 0.1, lambda: 0.5 - steps: 371\n",
      "Training episode: 129, lr: 0.1, lambda: 0.5 - steps: 365\n",
      "Training episode: 139, lr: 0.1, lambda: 0.5 - steps: 347\n",
      "Training episode: 149, lr: 0.1, lambda: 0.5 - steps: 230\n",
      "Training episode: 159, lr: 0.1, lambda: 0.5 - steps: 227\n",
      "Training episode: 169, lr: 0.1, lambda: 0.5 - steps: 188\n",
      "Training episode: 179, lr: 0.1, lambda: 0.5 - steps: 162\n",
      "Training episode: 189, lr: 0.1, lambda: 0.5 - steps: 223\n",
      "Training episode: 199, lr: 0.1, lambda: 0.5 - steps: 197\n",
      "Training episode: 209, lr: 0.1, lambda: 0.5 - steps: 232\n",
      "Training episode: 219, lr: 0.1, lambda: 0.5 - steps: 232\n",
      "Training episode: 229, lr: 0.1, lambda: 0.5 - steps: 376\n",
      "Training episode: 239, lr: 0.1, lambda: 0.5 - steps: 176\n",
      "Training episode: 249, lr: 0.1, lambda: 0.5 - steps: 478\n",
      "Training episode: 259, lr: 0.1, lambda: 0.5 - steps: 309\n",
      "Training episode: 269, lr: 0.1, lambda: 0.5 - steps: 541\n",
      "Training episode: 279, lr: 0.1, lambda: 0.5 - steps: 439\n",
      "Training episode: 289, lr: 0.1, lambda: 0.5 - steps: 226\n",
      "Training episode: 299, lr: 0.1, lambda: 0.5 - steps: 189\n",
      "Training episode: 309, lr: 0.1, lambda: 0.5 - steps: 239\n",
      "Training episode: 319, lr: 0.1, lambda: 0.5 - steps: 219\n",
      "Training episode: 329, lr: 0.1, lambda: 0.5 - steps: 212\n",
      "Training episode: 339, lr: 0.1, lambda: 0.5 - steps: 233\n",
      "Training episode: 349, lr: 0.1, lambda: 0.5 - steps: 217\n",
      "Training episode: 359, lr: 0.1, lambda: 0.5 - steps: 303\n",
      "Training episode: 369, lr: 0.1, lambda: 0.5 - steps: 156\n",
      "Training episode: 379, lr: 0.1, lambda: 0.5 - steps: 214\n",
      "Training episode: 389, lr: 0.1, lambda: 0.5 - steps: 165\n",
      "Training episode: 399, lr: 0.1, lambda: 0.5 - steps: 280\n",
      "Training episode: 409, lr: 0.1, lambda: 0.5 - steps: 377\n",
      "Training episode: 419, lr: 0.1, lambda: 0.5 - steps: 211\n",
      "Training episode: 429, lr: 0.1, lambda: 0.5 - steps: 191\n",
      "Training episode: 439, lr: 0.1, lambda: 0.5 - steps: 161\n",
      "Training episode: 449, lr: 0.1, lambda: 0.5 - steps: 283\n",
      "Training episode: 459, lr: 0.1, lambda: 0.5 - steps: 214\n",
      "Training episode: 469, lr: 0.1, lambda: 0.5 - steps: 153\n",
      "Training episode: 479, lr: 0.1, lambda: 0.5 - steps: 150\n",
      "Training episode: 489, lr: 0.1, lambda: 0.5 - steps: 188\n",
      "Training episode: 499, lr: 0.1, lambda: 0.5 - steps: 505\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "# initialize size of state and action space\n",
    "state_space = 20 * 20\n",
    "action_space = env.action_space.n\n",
    "\n",
    "# initialize discretization and mapping from state to q table\n",
    "grid_x, grid_v = initialize_grids()\n",
    "state_to_qtable = initialize_state_dict()\n",
    "\n",
    "# Training hyperparameters\n",
    "epsilon = 0.05\n",
    "num_training_episodes = 500\n",
    "gamma = 0.99\n",
    "lmbda = 0.5\n",
    "lr = 0.1\n",
    "\n",
    "# Training\n",
    "q_car = initialize_q_table(state_space, action_space)\n",
    "total_steps, q_car = train(q_car, env, epsilon, num_training_episodes, gamma, lmbda, grid_x, grid_v, lr)\n",
    "\n",
    "\n",
    "actions = np.max(q_car, axis=1)\n",
    "actions = actions.reshape((20, 20))\n",
    "plt.figure(figsize=(16, 12))\n",
    "ax = plt.subplot(111)\n",
    "ax = sns.heatmap(actions, annot=True)\n",
    "plt.ylim(0, 20)\n",
    "plt.xlabel(\"Position\", fontsize=20)\n",
    "plt.ylabel(\"Velocity\", fontsize=20)\n",
    "plt.title(\"Q values for optimal action - Tabular TD lambda\", fontdict={'fontsize': 25})\n",
    "plt.savefig('plots/q_values_Tabular_TD_L')\n",
    "plt.close()\n",
    "\n",
    "# Plot Steps\n",
    "plt.plot(np.arange(len(total_steps)) + 1, total_steps)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps')\n",
    "plt.title('Steps per Episode - Tabular TD lambda')\n",
    "plt.savefig('plots/steps_Tabular_TD_L.png')\n",
    "plt.close()\n",
    "\n",
    "np.savetxt(\"data/q_TD_L\", q_car)\n",
    "np.savetxt(\"data/steps_Tabular_TD_L\", total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb94656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
