{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f97e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from helper import initialize_grids, initialize_q_table, initialize_state_dict, initialize_random_start, \\\n",
    "    epsilon_greedy_policy, get_closest_in_grid, plot_rewards, plot_steps, evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "def train(q, rand_init=True):\n",
    "    # Initialize variables to track rewards\n",
    "    reward_list = []\n",
    "    avg_reward_list = []\n",
    "    total_steps = []\n",
    "    file = open('trace/trace_train.txt', 'w')\n",
    "\n",
    "    for episode in range(n_training_episodes):\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "        state = env.reset()\n",
    "        if rand_init:\n",
    "            state = initialize_random_start(grid_x, grid_v)\n",
    "        steps = 0\n",
    "        tot_reward, reward = 0, 0\n",
    "        terminated = False\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Choose the action At using epsilon greedy policy\n",
    "            action = epsilon_greedy_policy(q, state, epsilon, grid_x, grid_v, state_to_qtable, env)\n",
    "\n",
    "            file.write(f'{state[0]},{state[1]},{action}\\n')\n",
    "\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            s = state_to_qtable[get_closest_in_grid(state, grid_x, grid_v)]\n",
    "            ns = state_to_qtable[get_closest_in_grid(new_state, grid_x, grid_v)]\n",
    "\n",
    "            # Calculate current learning rate\n",
    "            lr = max(min_lr, initial_lr * np.exp(-k * episode))\n",
    "\n",
    "            # Update Q table\n",
    "            q[s][action] = q[s][action] + lr * (reward + gamma * np.max(q[ns]) - q[s][action])\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            # If done, finish the episode\n",
    "            if terminated:  # or truncated:\n",
    "                total_steps.append(steps)\n",
    "                break\n",
    "\n",
    "            # Our state is the new state\n",
    "            state = new_state\n",
    "\n",
    "            # Update total reward\n",
    "            tot_reward += reward\n",
    "\n",
    "            # Track rewards\n",
    "            reward_list.append(tot_reward)\n",
    "\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(reward_list)\n",
    "            avg_reward_list.append(avg_reward)\n",
    "            reward_list = []\n",
    "            print(f\"episode: {episode}\\t average reward: {avg_reward}\\t avg steps: {np.mean(total_steps[-10:])}\"\n",
    "                  f\"\\t lr: {lr}\\t epsilon: {epsilon}\")\n",
    "\n",
    "    return q, avg_reward_list, total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e8b58f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9\t average reward: -4517.1130121801725\t avg steps: 4171.5\t lr: 0.19119949636662\t epsilon: 0.9182346260076668\n",
      "episode: 19\t average reward: -2343.918956637315\t avg steps: 3010.5\t lr: 0.1818745868936463\t epsilon: 0.8356111772461942\n",
      "episode: 29\t average reward: -787.7177692807156\t avg steps: 1387.1\t lr: 0.17300445862214828\t epsilon: 0.760850389199637\n",
      "episode: 39\t average reward: -1197.462803179564\t avg steps: 1472.9\t lr: 0.1645669316112037\t epsilon: 0.6932040307732564\n",
      "episode: 49\t average reward: -570.3968795205622\t avg steps: 968.8\t lr: 0.15654090764837364\t epsilon: 0.6319950744751953\n",
      "episode: 59\t average reward: -521.9560201050948\t avg steps: 876.4\t lr: 0.1489063174931819\t epsilon: 0.5766109204977817\n",
      "episode: 69\t average reward: -353.3511923688394\t avg steps: 630.0\t lr: 0.14164407069356\t epsilon: 0.5264972656127527\n",
      "episode: 79\t average reward: -315.1616005495449\t avg steps: 583.3\t lr: 0.13473600784977355\t epsilon: 0.481152555518238\n",
      "episode: 89\t average reward: -249.9315988250105\t avg steps: 477.6\t lr: 0.12816485520646376\t epsilon: 0.4401229651147282\n",
      "episode: 99\t average reward: -227.75136159128581\t avg steps: 423.3\t lr: 0.12191418145926186\t epsilon: 0.4029978564709434\n",
      "episode: 109\t average reward: -206.5579637096774\t avg steps: 397.8\t lr: 0.11596835666796929\t epsilon: 0.36940566902139665\n",
      "episode: 119\t average reward: -230.6634705332086\t avg steps: 428.6\t lr: 0.11031251317356597\t epsilon: 0.33901020086336886\n",
      "episode: 129\t average reward: -258.424691921497\t avg steps: 439.2\t lr: 0.10493250842131858\t epsilon: 0.3115072439352647\n",
      "episode: 139\t average reward: -249.2083435582822\t avg steps: 408.5\t lr: 0.0998148895970272\t epsilon: 0.2866215394000847\n",
      "episode: 149\t average reward: -154.05507051712559\t avg steps: 298.8\t lr: 0.09494685998798248\t epsilon: 0.26410402276246675\n",
      "episode: 159\t average reward: -167.70429252782193\t avg steps: 315.5\t lr: 0.09031624698451846\t epsilon: 0.24372933114750273\n",
      "episode: 169\t average reward: -195.68259982512387\t avg steps: 344.1\t lr: 0.08591147164214784\t epsilon: 0.22529354779333982\n",
      "episode: 179\t average reward: -132.7948817371074\t avg steps: 258.9\t lr: 0.0817215197281697\t epsilon: 0.20861216118368864\n",
      "episode: 189\t average reward: -159.16738625041515\t avg steps: 302.1\t lr: 0.0777359141803506\t epsilon: 0.1935182183945523\n",
      "episode: 199\t average reward: -175.25103857566765\t avg steps: 338.0\t lr: 0.0739446889088118\t epsilon: 0.17986065417324765\n",
      "episode: 209\t average reward: -224.30639730639732\t avg steps: 387.1\t lr: 0.07033836387561339\t epsilon: 0.1675027790265821\n",
      "episode: 219\t average reward: -165.75074775672982\t avg steps: 301.9\t lr: 0.06690792138972153\t epsilon: 0.15632091118646244\n",
      "episode: 229\t average reward: -135.4228187919463\t avg steps: 254.3\t lr: 0.06364478355808383\t epsilon: 0.14620313876118923\n",
      "episode: 239\t average reward: -125.68747433264888\t avg steps: 244.5\t lr: 0.060540790836428576\t epsilon: 0.13704819968362958\n",
      "episode: 249\t average reward: -193.5326052039833\t avg steps: 312.3\t lr: 0.05758818162615405\t epsilon: 0.12876446824641402\n",
      "episode: 259\t average reward: -142.97672727272726\t avg steps: 276.0\t lr: 0.05477957286628912\t epsilon: 0.12126903808106063\n",
      "episode: 269\t average reward: -121.04268032437047\t avg steps: 235.3\t lr: 0.05210794157199512\t epsilon: 0.11448689240317338\n",
      "episode: 279\t average reward: -122.48442760942761\t avg steps: 238.6\t lr: 0.04956660727344575\t epsilon: 0.10835015321925012\n",
      "episode: 289\t average reward: -122.67830109335576\t avg steps: 238.8\t lr: 0.04714921531117271\t epsilon: 0.1027974019809089\n",
      "episode: 299\t average reward: -127.64869281045752\t avg steps: 245.8\t lr: 0.04484972094610706\t epsilon: 0.09777306488741228\n",
      "episode: 309\t average reward: -127.88486055776893\t avg steps: 252.0\t lr: 0.04266237424458305\t epsilon: 0.09322685668439049\n",
      "episode: 319\t average reward: -125.96455900120822\t avg steps: 249.3\t lr: 0.04058170570050881\t epsilon: 0.08911327739211436\n",
      "episode: 329\t average reward: -142.95524578136462\t avg steps: 273.6\t lr: 0.03860251255875235\t epsilon: 0.08539115692640502\n",
      "episode: 339\t average reward: -138.82337467117625\t avg steps: 267.1\t lr: 0.03671984580554357\t epsilon: 0.08202324305459377\n",
      "episode: 349\t average reward: -144.13804116394607\t avg steps: 282.8\t lr: 0.03492899779336217\t epsilon: 0.07897582856265661\n",
      "episode: 359\t average reward: -146.85644599303137\t avg steps: 288.0\t lr: 0.03322549046936662\t epsilon: 0.07621841390208682\n",
      "episode: 369\t average reward: -123.0784479122733\t avg steps: 238.1\t lr: 0.03160506417792956\t epsilon: 0.07372340194016234\n",
      "episode: 379\t average reward: -128.58876221498372\t avg steps: 246.6\t lr: 0.030063667009280065\t epsilon: 0.07146582175856578\n",
      "episode: 389\t average reward: -111.60734200743494\t avg steps: 216.2\t lr: 0.028597444667618578\t epsilon: 0.06942307873604077\n",
      "episode: 399\t average reward: -114.14930085701398\t avg steps: 222.7\t lr: 0.027202730833369833\t epsilon: 0.06757472841382828\n",
      "episode: 409\t average reward: -127.78909669717468\t avg steps: 252.3\t lr: 0.025876037995474218\t epsilon: 0.0659022718806516\n",
      "episode: 419\t average reward: -127.30194676201828\t avg steps: 252.7\t lr: 0.024614048730793543\t epsilon: 0.06438897062939464\n",
      "episode: 429\t average reward: -109.58881118881119\t avg steps: 215.5\t lr: 0.023413607408825275\t epsilon: 0.0630196790324967\n",
      "episode: 439\t average reward: -126.86024096385542\t avg steps: 250.0\t lr: 0.022271712300982526\t epsilon: 0.06178069275942124\n",
      "episode: 449\t average reward: -105.14895681707908\t avg steps: 207.1\t lr: 0.021185508074709074\t epsilon: 0.060659611619109634\n",
      "episode: 459\t average reward: -122.47132927888792\t avg steps: 231.2\t lr: 0.020152278653660746\t epsilon: 0.05964521545470128\n",
      "episode: 469\t average reward: -126.09795570698466\t avg steps: 235.8\t lr: 0.01916944042609973\t epsilon: 0.05872735184843243\n",
      "episode: 479\t average reward: -133.76461538461538\t avg steps: 261.0\t lr: 0.01823453578451957\t epsilon: 0.05789683451282696\n",
      "episode: 489\t average reward: -119.86509274873525\t avg steps: 238.2\t lr: 0.01734522698034623\t epsilon: 0.05714535135124361\n",
      "episode: 499\t average reward: -126.68112350268484\t avg steps: 243.1\t lr: 0.016499290278348996\t epsilon: 0.05646538126761902\n",
      "episode: 509\t average reward: -103.88793103448276\t avg steps: 198.2\t lr: 0.015694610396144146\t epsilon: 0.055850118892810455\n",
      "episode: 519\t average reward: -108.2161243322001\t avg steps: 206.9\t lr: 0.014929175214887114\t epsilon: 0.055293406474173995\n",
      "episode: 529\t average reward: -133.23250942605782\t avg steps: 239.7\t lr: 0.014201070747927397\t epsilon: 0.05478967224670643\n",
      "episode: 539\t average reward: -112.80754892823859\t avg steps: 215.6\t lr: 0.013508476354844904\t epsilon: 0.054333874668948345\n",
      "episode: 549\t average reward: -95.68218298555377\t avg steps: 187.9\t lr: 0.012849660188900616\t epsilon: 0.05392145196554267\n",
      "episode: 559\t average reward: -103.37862137862138\t avg steps: 201.2\t lr: 0.012222974866517673\t epsilon: 0.05354827647145367\n",
      "episode: 569\t average reward: -88.73827446438911\t avg steps: 173.7\t lr: 0.011626853347964294\t epsilon: 0.05321061332090788\n",
      "episode: 579\t average reward: -90.08615738081562\t avg steps: 175.1\t lr: 0.011059805018938277\t epsilon: 0.052905083067602145\n",
      "episode: 589\t average reward: -94.67238516878804\t avg steps: 181.7\t lr: 0.010520411963254768\t epsilon: 0.05262862786206911\n",
      "episode: 599\t average reward: -348.68971389645776\t avg steps: 294.6\t lr: 0.010007325417317256\t epsilon: 0.052378480847692\n",
      "episode: 609\t average reward: -82.29538461538462\t avg steps: 163.5\t lr: 0.009519262397506063\t epsilon: 0.05215213846907361\n",
      "episode: 619\t average reward: -93.66832229580574\t avg steps: 182.2\t lr: 0.009055002492050978\t epsilon: 0.051947335415612424\n",
      "episode: 629\t average reward: -90.6676072234763\t avg steps: 178.2\t lr: 0.008613384809366185\t epsilon: 0.05176202194951273\n",
      "episode: 639\t average reward: -89.25684333139196\t avg steps: 172.7\t lr: 0.008193305075216586\t epsilon: 0.05159434339131979\n",
      "episode: 649\t average reward: -113.35125628140703\t avg steps: 200.0\t lr: 0.007793712871457053\t epsilon: 0.05144262155766449\n",
      "episode: 659\t average reward: -78.323264781491\t avg steps: 156.6\t lr: 0.007413609009439901\t epsilon: 0.05130533796544015\n",
      "episode: 669\t average reward: -83.89311594202898\t avg steps: 166.6\t lr: 0.007052043031522824\t epsilon: 0.05118111863431318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 679\t average reward: -81.87037037037037\t avg steps: 163.0\t lr: 0.006708110834429727\t epsilon: 0.0510687203354661\n",
      "episode: 689\t average reward: -81.22291407222914\t avg steps: 161.6\t lr: 0.006380952408521593\t epsilon: 0.05096701814894567\n",
      "episode: 699\t average reward: -83.77480219111382\t avg steps: 165.3\t lr: 0.006069749687324441\t epsilon: 0.05087499420508591\n",
      "episode: 709\t average reward: -82.0203578038248\t avg steps: 163.1\t lr: 0.005773724501937017\t epsilon: 0.05079172749732636\n",
      "episode: 719\t average reward: -81.61728395061728\t avg steps: 163.0\t lr: 0.005492136635203219\t epsilon: 0.05071638466446886\n",
      "episode: 729\t average reward: -91.50198976691301\t avg steps: 176.9\t lr: 0.005224281970783647\t epsilon: 0.05064821165011856\n",
      "episode: 739\t average reward: -83.00182704019488\t avg steps: 165.2\t lr: 0.005\t epsilon: 0.05058652615583411\n",
      "episode: 749\t average reward: -83.21367521367522\t avg steps: 164.8\t lr: 0.005\t epsilon: 0.05053071081245549\n",
      "episode: 759\t average reward: -84.39180229053646\t avg steps: 166.9\t lr: 0.005\t epsilon: 0.05048020700126599\n",
      "episode: 769\t average reward: -85.04305639781685\t avg steps: 165.9\t lr: 0.005\t epsilon: 0.05043450926314831\n",
      "episode: 779\t average reward: -77.14824619457313\t avg steps: 152.1\t lr: 0.005\t epsilon: 0.05039316023977983\n",
      "episode: 789\t average reward: -85.04355716878403\t avg steps: 166.3\t lr: 0.005\t epsilon: 0.050355746096236775\n",
      "episode: 799\t average reward: -92.86063298167684\t avg steps: 181.1\t lr: 0.005\t epsilon: 0.05032189237919526\n",
      "episode: 809\t average reward: -86.83534378769602\t avg steps: 166.8\t lr: 0.005\t epsilon: 0.05029126026927649\n",
      "episode: 819\t average reward: -87.09451575262544\t avg steps: 172.4\t lr: 0.005\t epsilon: 0.0502635431900286\n",
      "episode: 829\t average reward: -85.6573799644339\t avg steps: 169.7\t lr: 0.005\t epsilon: 0.050238463739606436\n",
      "episode: 839\t average reward: -91.12507271669575\t avg steps: 172.9\t lr: 0.005\t epsilon: 0.05021577091444069\n",
      "episode: 849\t average reward: -109.35208535402522\t avg steps: 207.2\t lr: 0.005\t epsilon: 0.05019523759710977\n",
      "episode: 859\t average reward: -100.1078125\t avg steps: 193.0\t lr: 0.005\t epsilon: 0.05017665828327235\n",
      "episode: 869\t average reward: -84.84326579261025\t avg steps: 168.8\t lr: 0.005\t epsilon: 0.05015984702491082\n",
      "episode: 879\t average reward: -89.12863795110594\t avg steps: 172.8\t lr: 0.005\t epsilon: 0.05014463556930104\n",
      "episode: 889\t average reward: -86.53488372093024\t avg steps: 168.7\t lr: 0.005\t epsilon: 0.05013087167508251\n",
      "episode: 899\t average reward: -99.28270042194093\t avg steps: 190.6\t lr: 0.005\t epsilon: 0.0501184175885757\n",
      "episode: 909\t average reward: -107.04867256637168\t avg steps: 204.4\t lr: 0.005\t epsilon: 0.05010714866509688\n",
      "episode: 919\t average reward: -104.90718562874251\t avg steps: 201.4\t lr: 0.005\t epsilon: 0.05009695212147226\n",
      "episode: 929\t average reward: -90.42285382830626\t avg steps: 173.4\t lr: 0.005\t epsilon: 0.05008772590726607\n",
      "episode: 939\t average reward: -94.1304347826087\t avg steps: 182.7\t lr: 0.005\t epsilon: 0.05007937768342549\n",
      "episode: 949\t average reward: -82.76603543066585\t avg steps: 164.7\t lr: 0.005\t epsilon: 0.0500718238981204\n",
      "episode: 959\t average reward: -80.30257376020087\t avg steps: 160.3\t lr: 0.005\t epsilon: 0.05006498895052854\n",
      "episode: 969\t average reward: -82.81890243902438\t avg steps: 165.0\t lr: 0.005\t epsilon: 0.050058804434197114\n",
      "episode: 979\t average reward: -97.21307901907358\t avg steps: 184.5\t lr: 0.005\t epsilon: 0.05005320845240798\n",
      "episode: 989\t average reward: -84.3863361547763\t avg steps: 166.4\t lr: 0.005\t epsilon: 0.050048144998694526\n",
      "episode: 999\t average reward: -82.6226065472514\t avg steps: 162.9\t lr: 0.005\t epsilon: 0.0500435633963101\n",
      "episode: 1009\t average reward: -76.46485411140584\t avg steps: 151.8\t lr: 0.005\t epsilon: 0.050039417791038106\n",
      "episode: 1019\t average reward: -80.81003134796238\t avg steps: 160.5\t lr: 0.005\t epsilon: 0.050035666692267604\n",
      "episode: 1029\t average reward: -94.15986769570011\t avg steps: 182.4\t lr: 0.005\t epsilon: 0.0500322725577413\n",
      "episode: 1039\t average reward: -77.18038183015142\t avg steps: 152.9\t lr: 0.005\t epsilon: 0.05002920141782006\n",
      "episode: 1049\t average reward: -82.56418092909536\t avg steps: 164.6\t lr: 0.005\t epsilon: 0.05002642253550329\n",
      "episode: 1059\t average reward: -98.0111919418019\t avg steps: 179.7\t lr: 0.005\t epsilon: 0.050023908098802757\n",
      "episode: 1069\t average reward: -96.95054007959068\t avg steps: 176.9\t lr: 0.005\t epsilon: 0.050021632942390835\n",
      "episode: 1079\t average reward: -98.6393351800554\t avg steps: 181.5\t lr: 0.005\t epsilon: 0.050019574295737446\n",
      "episode: 1089\t average reward: -94.25806451612904\t avg steps: 183.9\t lr: 0.005\t epsilon: 0.05001771155521494\n",
      "episode: 1099\t average reward: -185.8763440860215\t avg steps: 280.0\t lr: 0.005\t epsilon: 0.05001602607789009\n",
      "episode: 1109\t average reward: -126.2296389588581\t avg steps: 239.2\t lr: 0.005\t epsilon: 0.050014500994939315\n",
      "episode: 1119\t average reward: -132.23970473970473\t avg steps: 258.4\t lr: 0.005\t epsilon: 0.05001312104281984\n",
      "episode: 1129\t average reward: -131.27265625\t avg steps: 257.0\t lr: 0.005\t epsilon: 0.050011872410507044\n",
      "episode: 1139\t average reward: -127.2772041302621\t avg steps: 252.8\t lr: 0.005\t epsilon: 0.05001074260126906\n",
      "episode: 1149\t average reward: -135.0198799699925\t avg steps: 267.6\t lr: 0.005\t epsilon: 0.05000972030759528\n",
      "episode: 1159\t average reward: -118.23432055749129\t avg steps: 230.6\t lr: 0.005\t epsilon: 0.050008795298027035\n",
      "episode: 1169\t average reward: -120.37231804795961\t avg steps: 238.7\t lr: 0.005\t epsilon: 0.050007958314757635\n",
      "episode: 1179\t average reward: -120.95348837209302\t avg steps: 241.8\t lr: 0.005\t epsilon: 0.05000720098097722\n",
      "episode: 1189\t average reward: -121.48202341137124\t avg steps: 240.2\t lr: 0.005\t epsilon: 0.05000651571703475\n",
      "episode: 1199\t average reward: -126.16253002401922\t avg steps: 250.8\t lr: 0.005\t epsilon: 0.05000589566457838\n",
      "episode: 1209\t average reward: -118.24508966695133\t avg steps: 235.2\t lr: 0.005\t epsilon: 0.050005334617914704\n",
      "episode: 1219\t average reward: -109.96194895591647\t avg steps: 216.5\t lr: 0.005\t epsilon: 0.05000482696190015\n",
      "episode: 1229\t average reward: -117.49612403100775\t avg steps: 233.2\t lr: 0.005\t epsilon: 0.050004367615742694\n",
      "episode: 1239\t average reward: -111.79508196721312\t avg steps: 220.6\t lr: 0.005\t epsilon: 0.05000395198215159\n",
      "episode: 1249\t average reward: -109.31627906976745\t avg steps: 216.0\t lr: 0.005\t epsilon: 0.05000357590132617\n",
      "episode: 1259\t average reward: -118.42495711835335\t avg steps: 234.2\t lr: 0.005\t epsilon: 0.05000323560932312\n",
      "episode: 1269\t average reward: -125.89377289377289\t avg steps: 246.7\t lr: 0.005\t epsilon: 0.05000292770038571\n",
      "episode: 1279\t average reward: -129.0889154704944\t avg steps: 251.8\t lr: 0.005\t epsilon: 0.05000264909285779\n",
      "episode: 1289\t average reward: -117.91389599317988\t avg steps: 235.6\t lr: 0.005\t epsilon: 0.05000239699834158\n",
      "episode: 1299\t average reward: -114.15150176678445\t avg steps: 227.4\t lr: 0.005\t epsilon: 0.05000216889379043\n",
      "episode: 1309\t average reward: -118.49763644177052\t avg steps: 233.7\t lr: 0.005\t epsilon: 0.050001962496257324\n",
      "episode: 1319\t average reward: -111.89169675090253\t avg steps: 222.6\t lr: 0.005\t epsilon: 0.050001775740046384\n",
      "episode: 1329\t average reward: -118.46462467644521\t avg steps: 232.8\t lr: 0.005\t epsilon: 0.05000160675603867\n",
      "episode: 1339\t average reward: -115.96135475466782\t avg steps: 231.3\t lr: 0.005\t epsilon: 0.05000145385298545\n",
      "episode: 1349\t average reward: -121.2791771620487\t avg steps: 239.2\t lr: 0.005\t epsilon: 0.05000131550058156\n",
      "episode: 1359\t average reward: -122.24968944099379\t avg steps: 242.5\t lr: 0.005\t epsilon: 0.050001190314149645\n",
      "episode: 1369\t average reward: -112.5542600896861\t avg steps: 224.0\t lr: 0.005\t epsilon: 0.05000107704078181\n",
      "episode: 1379\t average reward: -118.23673997412678\t avg steps: 232.9\t lr: 0.005\t epsilon: 0.05000097454680014\n",
      "episode: 1389\t average reward: -115.80236117184084\t avg steps: 229.7\t lr: 0.005\t epsilon: 0.05000088180641039\n",
      "episode: 1399\t average reward: -122.19319148936171\t avg steps: 236.0\t lr: 0.005\t epsilon: 0.050000797891435585\n",
      "episode: 1409\t average reward: -121.75716694772343\t avg steps: 238.2\t lr: 0.005\t epsilon: 0.05000072196202645\n",
      "episode: 1419\t average reward: -122.99169090153718\t avg steps: 241.7\t lr: 0.005\t epsilon: 0.05000065325825593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1429\t average reward: -113.45636687444346\t avg steps: 225.6\t lr: 0.005\t epsilon: 0.050000591092513605\n",
      "episode: 1439\t average reward: -113.88141592920354\t avg steps: 227.0\t lr: 0.005\t epsilon: 0.05000053484262383\n",
      "episode: 1449\t average reward: -119.62521222410867\t avg steps: 236.6\t lr: 0.005\t epsilon: 0.050000483945618804\n",
      "episode: 1459\t average reward: -118.78107764106916\t avg steps: 236.7\t lr: 0.005\t epsilon: 0.05000043789210419\n",
      "episode: 1469\t average reward: -118.67713194739075\t avg steps: 236.7\t lr: 0.005\t epsilon: 0.05000039622116093\n",
      "episode: 1479\t average reward: -114.47138350608382\t avg steps: 222.9\t lr: 0.005\t epsilon: 0.05000035851573223\n",
      "episode: 1489\t average reward: -112.52986079928155\t avg steps: 223.7\t lr: 0.005\t epsilon: 0.05000032439844948\n",
      "episode: 1499\t average reward: -115.86470078057242\t avg steps: 231.6\t lr: 0.005\t epsilon: 0.05000029352785544\n",
      "episode: 1509\t average reward: -113.69707705934455\t avg steps: 226.8\t lr: 0.005\t epsilon: 0.05000026559498684\n",
      "episode: 1519\t average reward: -114.53362637362638\t avg steps: 228.5\t lr: 0.005\t epsilon: 0.050000240320282135\n",
      "episode: 1529\t average reward: -110.69108280254777\t avg steps: 220.8\t lr: 0.005\t epsilon: 0.05000021745078359\n",
      "episode: 1539\t average reward: -111.00955414012739\t avg steps: 220.8\t lr: 0.005\t epsilon: 0.05000019675760557\n",
      "episode: 1549\t average reward: -114.08749447635881\t avg steps: 227.3\t lr: 0.005\t epsilon: 0.0500001780336438\n",
      "episode: 1559\t average reward: -115.84331597222223\t avg steps: 231.4\t lr: 0.005\t epsilon: 0.050000161091502585\n",
      "episode: 1569\t average reward: -111.53366470854044\t avg steps: 222.3\t lr: 0.005\t epsilon: 0.050000145761619263\n",
      "episode: 1579\t average reward: -116.6257562662057\t avg steps: 232.4\t lr: 0.005\t epsilon: 0.05000013189056723\n",
      "episode: 1589\t average reward: -113.8858407079646\t avg steps: 227.0\t lr: 0.005\t epsilon: 0.05000011933952031\n",
      "episode: 1599\t average reward: -119.340370529944\t avg steps: 233.1\t lr: 0.005\t epsilon: 0.05000010798286343\n",
      "episode: 1609\t average reward: -113.72601517179831\t avg steps: 225.1\t lr: 0.005\t epsilon: 0.05000009770693534\n",
      "episode: 1619\t average reward: -113.88736141906874\t avg steps: 226.5\t lr: 0.005\t epsilon: 0.05000008840889109\n",
      "episode: 1629\t average reward: -116.14904679376083\t avg steps: 231.8\t lr: 0.005\t epsilon: 0.05000007999567275\n",
      "episode: 1639\t average reward: -114.3716457960644\t avg steps: 224.6\t lr: 0.005\t epsilon: 0.050000072383077986\n",
      "episode: 1649\t average reward: -100.16873706004141\t avg steps: 194.2\t lr: 0.005\t epsilon: 0.050000065494917395\n",
      "episode: 1659\t average reward: -111.64561077566186\t avg steps: 216.3\t lr: 0.005\t epsilon: 0.05000005926225195\n",
      "episode: 1669\t average reward: -109.63118228921337\t avg steps: 213.3\t lr: 0.005\t epsilon: 0.050000053622703045\n",
      "episode: 1679\t average reward: -113.27016129032258\t avg steps: 224.2\t lr: 0.005\t epsilon: 0.05000004851982817\n",
      "episode: 1689\t average reward: -113.37683089214381\t avg steps: 226.3\t lr: 0.005\t epsilon: 0.050000043902556045\n",
      "episode: 1699\t average reward: -123.87366694011484\t avg steps: 244.8\t lr: 0.005\t epsilon: 0.050000039724675455\n",
      "episode: 1709\t average reward: -115.52047038327527\t avg steps: 230.6\t lr: 0.005\t epsilon: 0.05000003594437277\n",
      "episode: 1719\t average reward: -126.42833539859562\t avg steps: 243.1\t lr: 0.005\t epsilon: 0.05000003252381345\n",
      "episode: 1729\t average reward: -113.27495543672015\t avg steps: 225.4\t lr: 0.005\t epsilon: 0.05000002942876339\n",
      "episode: 1739\t average reward: -121.43599334995844\t avg steps: 241.6\t lr: 0.005\t epsilon: 0.050000026628246284\n",
      "episode: 1749\t average reward: -116.81389252948885\t avg steps: 229.9\t lr: 0.005\t epsilon: 0.05000002409423361\n",
      "episode: 1759\t average reward: -115.13722051731696\t avg steps: 229.1\t lr: 0.005\t epsilon: 0.05000002180136413\n",
      "episode: 1769\t average reward: -106.53513770180437\t avg steps: 211.6\t lr: 0.005\t epsilon: 0.050000019726690034\n",
      "episode: 1779\t average reward: -114.72042068361087\t avg steps: 229.2\t lr: 0.005\t epsilon: 0.05000001784944728\n",
      "episode: 1789\t average reward: -117.76137339055794\t avg steps: 234.0\t lr: 0.005\t epsilon: 0.05000001615084779\n",
      "episode: 1799\t average reward: -115.23353819139597\t avg steps: 228.8\t lr: 0.005\t epsilon: 0.05000001461389141\n",
      "episode: 1809\t average reward: -106.69976359338061\t avg steps: 212.5\t lr: 0.005\t epsilon: 0.05000001322319577\n",
      "episode: 1819\t average reward: -127.19585921325051\t avg steps: 242.5\t lr: 0.005\t epsilon: 0.05000001196484232\n",
      "episode: 1829\t average reward: -111.68786648624267\t avg steps: 222.7\t lr: 0.005\t epsilon: 0.050000010826237035\n",
      "episode: 1839\t average reward: -152.25531077891424\t avg steps: 255.2\t lr: 0.005\t epsilon: 0.05000000979598436\n",
      "episode: 1849\t average reward: -131.71318770226537\t avg steps: 248.2\t lr: 0.005\t epsilon: 0.0500000088637732\n",
      "episode: 1859\t average reward: -173.6828290193219\t avg steps: 275.3\t lr: 0.005\t epsilon: 0.050000008020273656\n",
      "episode: 1869\t average reward: -114.27135231316726\t avg steps: 225.8\t lr: 0.005\t epsilon: 0.050000007257043705\n",
      "episode: 1879\t average reward: -108.39990328820116\t avg steps: 207.8\t lr: 0.005\t epsilon: 0.05000000656644469\n",
      "episode: 1889\t average reward: -131.9061475409836\t avg steps: 245.0\t lr: 0.005\t epsilon: 0.05000000594156486\n",
      "episode: 1899\t average reward: -112.38084001887682\t avg steps: 212.9\t lr: 0.005\t epsilon: 0.05000000537615021\n",
      "episode: 1909\t average reward: -102.31200396825396\t avg steps: 202.6\t lr: 0.005\t epsilon: 0.05000000486454187\n",
      "episode: 1919\t average reward: -810.1217282179992\t avg steps: 558.8\t lr: 0.005\t epsilon: 0.05000000440161951\n",
      "episode: 1929\t average reward: -111.13448432530667\t avg steps: 221.1\t lr: 0.005\t epsilon: 0.05000000398275003\n",
      "episode: 1939\t average reward: -110.698035632709\t avg steps: 219.9\t lr: 0.005\t epsilon: 0.05000000360374125\n",
      "episode: 1949\t average reward: -120.95311049210771\t avg steps: 216.4\t lr: 0.005\t epsilon: 0.05000000326079993\n",
      "episode: 1959\t average reward: -99.52686489306208\t avg steps: 192.7\t lr: 0.005\t epsilon: 0.05000000295049379\n",
      "episode: 1969\t average reward: -100.59223300970874\t avg steps: 196.7\t lr: 0.005\t epsilon: 0.050000002669717186\n",
      "episode: 1979\t average reward: -115.64879852125694\t avg steps: 217.4\t lr: 0.005\t epsilon: 0.05000000241566001\n",
      "episode: 1989\t average reward: -92.38777777777777\t avg steps: 181.0\t lr: 0.005\t epsilon: 0.05000000218577956\n",
      "episode: 1999\t average reward: -103.8835\t avg steps: 201.0\t lr: 0.005\t epsilon: 0.050000001977775134\n",
      "episode: 2009\t average reward: -99.20186818889465\t avg steps: 193.7\t lr: 0.005\t epsilon: 0.05000000178956495\n",
      "episode: 2019\t average reward: -111.56370474847202\t avg steps: 213.7\t lr: 0.005\t epsilon: 0.05000000161926533\n",
      "episode: 2029\t average reward: -110.65080135988345\t avg steps: 206.9\t lr: 0.005\t epsilon: 0.050000001465171855\n",
      "episode: 2039\t average reward: -100.25520833333333\t avg steps: 193.0\t lr: 0.005\t epsilon: 0.05000000132574232\n",
      "episode: 2049\t average reward: -113.84808329389493\t avg steps: 212.3\t lr: 0.005\t epsilon: 0.05000000119958126\n",
      "episode: 2059\t average reward: -177.42203389830507\t avg steps: 296.0\t lr: 0.005\t epsilon: 0.05000000108542601\n",
      "episode: 2069\t average reward: -118.42718873735151\t avg steps: 228.3\t lr: 0.005\t epsilon: 0.05000000098213407\n",
      "episode: 2079\t average reward: -120.63667970447632\t avg steps: 231.1\t lr: 0.005\t epsilon: 0.05000000088867165\n",
      "episode: 2089\t average reward: -143.30194552529184\t avg steps: 258.0\t lr: 0.005\t epsilon: 0.05000000080410336\n",
      "episode: 2099\t average reward: -130.2480869915425\t avg steps: 249.3\t lr: 0.005\t epsilon: 0.050000000727582816\n",
      "episode: 2109\t average reward: -100.69847928683797\t avg steps: 191.7\t lr: 0.005\t epsilon: 0.05000000065834415\n",
      "episode: 2119\t average reward: -183.1247408431237\t avg steps: 290.4\t lr: 0.005\t epsilon: 0.05000000059569443\n",
      "episode: 2129\t average reward: -155.13922728854857\t avg steps: 288.3\t lr: 0.005\t epsilon: 0.050000000539006606\n",
      "episode: 2139\t average reward: -108.86505190311419\t avg steps: 203.3\t lr: 0.005\t epsilon: 0.050000000487713345\n",
      "episode: 2149\t average reward: -94.53875545851528\t avg steps: 184.2\t lr: 0.005\t epsilon: 0.05000000044130128\n",
      "episode: 2159\t average reward: -92.35082872928177\t avg steps: 182.0\t lr: 0.005\t epsilon: 0.05000000039930592\n",
      "episode: 2169\t average reward: -104.18756218905473\t avg steps: 202.0\t lr: 0.005\t epsilon: 0.05000000036130693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2179\t average reward: -122.70704845814979\t avg steps: 228.0\t lr: 0.005\t epsilon: 0.05000000032692403\n",
      "episode: 2189\t average reward: -110.51392281387396\t avg steps: 205.7\t lr: 0.005\t epsilon: 0.0500000002958131\n",
      "episode: 2199\t average reward: -99.66776496430532\t avg steps: 183.1\t lr: 0.005\t epsilon: 0.05000000026766276\n",
      "episode: 2209\t average reward: -92.73742399115534\t avg steps: 181.9\t lr: 0.005\t epsilon: 0.050000000242191284\n",
      "episode: 2219\t average reward: -95.60010706638116\t avg steps: 187.8\t lr: 0.005\t epsilon: 0.050000000219143734\n",
      "episode: 2229\t average reward: -126.4221635883905\t avg steps: 228.4\t lr: 0.005\t epsilon: 0.05000000019828945\n",
      "episode: 2239\t average reward: -98.93249607535321\t avg steps: 192.1\t lr: 0.005\t epsilon: 0.05000000017941972\n",
      "episode: 2249\t average reward: -193.17757009345794\t avg steps: 311.3\t lr: 0.005\t epsilon: 0.050000000162345674\n",
      "episode: 2259\t average reward: -113.15130023640661\t avg steps: 212.5\t lr: 0.005\t epsilon: 0.05000000014689644\n",
      "episode: 2269\t average reward: -90.53894258101194\t avg steps: 176.9\t lr: 0.005\t epsilon: 0.050000000132917395\n",
      "episode: 2279\t average reward: -88.2729371032891\t avg steps: 174.3\t lr: 0.005\t epsilon: 0.05000000012026863\n",
      "episode: 2289\t average reward: -97.59083728278041\t avg steps: 190.9\t lr: 0.005\t epsilon: 0.05000000010882356\n",
      "episode: 2299\t average reward: -93.12162905888827\t avg steps: 182.7\t lr: 0.005\t epsilon: 0.050000000098467626\n",
      "episode: 2309\t average reward: -96.10836909871244\t avg steps: 187.4\t lr: 0.005\t epsilon: 0.0500000000890972\n",
      "episode: 2319\t average reward: -97.89046121593292\t avg steps: 191.8\t lr: 0.005\t epsilon: 0.05000000008061847\n",
      "episode: 2329\t average reward: -106.43838285435947\t avg steps: 206.3\t lr: 0.005\t epsilon: 0.05000000007294662\n",
      "episode: 2339\t average reward: -89.69270239452679\t avg steps: 176.4\t lr: 0.005\t epsilon: 0.05000000006600483\n",
      "episode: 2349\t average reward: -94.30162162162162\t avg steps: 186.0\t lr: 0.005\t epsilon: 0.05000000005972364\n",
      "episode: 2359\t average reward: -89.1784472769409\t avg steps: 173.6\t lr: 0.005\t epsilon: 0.050000000054040185\n",
      "episode: 2369\t average reward: -89.43408175014393\t avg steps: 174.7\t lr: 0.005\t epsilon: 0.050000000048897576\n",
      "episode: 2379\t average reward: -92.7132556849695\t avg steps: 181.3\t lr: 0.005\t epsilon: 0.05000000004424436\n",
      "episode: 2389\t average reward: -89.0296803652968\t avg steps: 176.2\t lr: 0.005\t epsilon: 0.05000000004003395\n",
      "episode: 2399\t average reward: -98.88256977356504\t avg steps: 190.9\t lr: 0.005\t epsilon: 0.05000000003622422\n",
      "episode: 2409\t average reward: -89.89941860465116\t avg steps: 173.0\t lr: 0.005\t epsilon: 0.05000000003277703\n",
      "episode: 2419\t average reward: -97.33832175307322\t avg steps: 188.1\t lr: 0.005\t epsilon: 0.05000000002965788\n",
      "episode: 2429\t average reward: -85.05661501787843\t avg steps: 168.8\t lr: 0.005\t epsilon: 0.050000000026835564\n",
      "episode: 2439\t average reward: -86.40035906642728\t avg steps: 168.1\t lr: 0.005\t epsilon: 0.05000000002428182\n",
      "episode: 2449\t average reward: -85.07976190476191\t avg steps: 169.0\t lr: 0.005\t epsilon: 0.0500000000219711\n",
      "episode: 2459\t average reward: -82.10947109471094\t avg steps: 163.6\t lr: 0.005\t epsilon: 0.050000000019880274\n",
      "episode: 2469\t average reward: -89.11058545239503\t avg steps: 170.1\t lr: 0.005\t epsilon: 0.05000000001798842\n",
      "episode: 2479\t average reward: -84.74432497013142\t avg steps: 168.4\t lr: 0.005\t epsilon: 0.050000000016276594\n",
      "episode: 2489\t average reward: -83.62575574365175\t avg steps: 166.4\t lr: 0.005\t epsilon: 0.050000000014727666\n",
      "episode: 2499\t average reward: -85.13289630512514\t avg steps: 168.8\t lr: 0.005\t epsilon: 0.05000000001332615\n",
      "episode: 2509\t average reward: -90.85157593123209\t avg steps: 175.5\t lr: 0.005\t epsilon: 0.050000000012057996\n",
      "episode: 2519\t average reward: -90.28242560359348\t avg steps: 179.1\t lr: 0.005\t epsilon: 0.050000000010910525\n",
      "episode: 2529\t average reward: -100.59276980329612\t avg steps: 189.1\t lr: 0.005\t epsilon: 0.05000000000987225\n",
      "episode: 2539\t average reward: -83.48752282410226\t avg steps: 165.3\t lr: 0.005\t epsilon: 0.05000000000893278\n",
      "episode: 2549\t average reward: -89.14770114942529\t avg steps: 175.0\t lr: 0.005\t epsilon: 0.05000000000808272\n",
      "episode: 2559\t average reward: -90.90692395005675\t avg steps: 177.2\t lr: 0.005\t epsilon: 0.05000000000731355\n",
      "episode: 2569\t average reward: -94.8305268875611\t avg steps: 185.1\t lr: 0.005\t epsilon: 0.05000000000661757\n",
      "episode: 2579\t average reward: -95.57834602829162\t avg steps: 184.8\t lr: 0.005\t epsilon: 0.050000000005987824\n",
      "episode: 2589\t average reward: -112.74769372693727\t avg steps: 217.8\t lr: 0.005\t epsilon: 0.05000000000541801\n",
      "episode: 2599\t average reward: -96.70566037735848\t avg steps: 186.5\t lr: 0.005\t epsilon: 0.050000000004902415\n",
      "episode: 2609\t average reward: -90.1367816091954\t avg steps: 175.0\t lr: 0.005\t epsilon: 0.05000000000443589\n",
      "episode: 2619\t average reward: -93.18176764869372\t avg steps: 180.9\t lr: 0.005\t epsilon: 0.05000000000401376\n",
      "episode: 2629\t average reward: -79.3250319284802\t avg steps: 157.6\t lr: 0.005\t epsilon: 0.0500000000036318\n",
      "episode: 2639\t average reward: -86.90532544378698\t avg steps: 170.0\t lr: 0.005\t epsilon: 0.05000000000328619\n",
      "episode: 2649\t average reward: -105.09095427435388\t avg steps: 202.2\t lr: 0.005\t epsilon: 0.050000000002973465\n",
      "episode: 2659\t average reward: -93.14141414141415\t avg steps: 179.2\t lr: 0.005\t epsilon: 0.0500000000026905\n",
      "episode: 2669\t average reward: -95.62195812110923\t avg steps: 177.7\t lr: 0.005\t epsilon: 0.05000000000243447\n",
      "episode: 2679\t average reward: -102.2100667693888\t avg steps: 195.7\t lr: 0.005\t epsilon: 0.050000000002202796\n",
      "episode: 2689\t average reward: -108.48640915593705\t avg steps: 210.7\t lr: 0.005\t epsilon: 0.05000000000199317\n",
      "episode: 2699\t average reward: -95.10125204137181\t avg steps: 184.7\t lr: 0.005\t epsilon: 0.0500000000018035\n",
      "episode: 2709\t average reward: -116.19596668128014\t avg steps: 229.1\t lr: 0.005\t epsilon: 0.05000000000163188\n",
      "episode: 2719\t average reward: -114.21182266009852\t avg steps: 224.3\t lr: 0.005\t epsilon: 0.05000000000147658\n",
      "episode: 2729\t average reward: -114.14614343707713\t avg steps: 222.7\t lr: 0.005\t epsilon: 0.050000000001336066\n",
      "episode: 2739\t average reward: -114.59806081974438\t avg steps: 227.9\t lr: 0.005\t epsilon: 0.050000000001208925\n",
      "episode: 2749\t average reward: -116.84702797202797\t avg steps: 229.8\t lr: 0.005\t epsilon: 0.05000000000109388\n",
      "episode: 2759\t average reward: -101.80636743215031\t avg steps: 192.6\t lr: 0.005\t epsilon: 0.05000000000098978\n",
      "episode: 2769\t average reward: -103.70499243570347\t avg steps: 199.3\t lr: 0.005\t epsilon: 0.05000000000089559\n",
      "episode: 2779\t average reward: -110.84611851171337\t avg steps: 218.7\t lr: 0.005\t epsilon: 0.05000000000081037\n",
      "episode: 2789\t average reward: -103.59266700150678\t avg steps: 200.1\t lr: 0.005\t epsilon: 0.05000000000073325\n",
      "episode: 2799\t average reward: -97.02746365105008\t avg steps: 186.7\t lr: 0.005\t epsilon: 0.05000000000066347\n",
      "episode: 2809\t average reward: -104.56656804733728\t avg steps: 203.8\t lr: 0.005\t epsilon: 0.050000000000600335\n",
      "episode: 2819\t average reward: -102.50506585612969\t avg steps: 198.4\t lr: 0.005\t epsilon: 0.05000000000054321\n",
      "episode: 2829\t average reward: -100.91224382553862\t avg steps: 191.3\t lr: 0.005\t epsilon: 0.05000000000049151\n",
      "episode: 2839\t average reward: -101.20791556728233\t avg steps: 190.5\t lr: 0.005\t epsilon: 0.05000000000044474\n",
      "episode: 2849\t average reward: -75.05043712172159\t avg steps: 149.7\t lr: 0.005\t epsilon: 0.05000000000040242\n",
      "episode: 2859\t average reward: -78.42765273311898\t avg steps: 156.5\t lr: 0.005\t epsilon: 0.05000000000036412\n",
      "episode: 2869\t average reward: -74.80926430517711\t avg steps: 147.8\t lr: 0.005\t epsilon: 0.050000000000329475\n",
      "episode: 2879\t average reward: -109.52037914691942\t avg steps: 212.0\t lr: 0.005\t epsilon: 0.05000000000029812\n",
      "episode: 2889\t average reward: -96.78469656992084\t avg steps: 190.5\t lr: 0.005\t epsilon: 0.05000000000026975\n",
      "episode: 2899\t average reward: -97.34621578099839\t avg steps: 187.3\t lr: 0.005\t epsilon: 0.05000000000024408\n",
      "episode: 2909\t average reward: -96.6030534351145\t avg steps: 184.4\t lr: 0.005\t epsilon: 0.050000000000220854\n",
      "episode: 2919\t average reward: -116.43845462713388\t avg steps: 223.6\t lr: 0.005\t epsilon: 0.050000000000199836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2929\t average reward: -89.54623779437105\t avg steps: 175.1\t lr: 0.005\t epsilon: 0.050000000000180816\n",
      "episode: 2939\t average reward: -106.15907953976989\t avg steps: 200.9\t lr: 0.005\t epsilon: 0.050000000000163615\n",
      "episode: 2949\t average reward: -117.13966725043782\t avg steps: 229.4\t lr: 0.005\t epsilon: 0.050000000000148044\n",
      "episode: 2959\t average reward: -116.33558254610887\t avg steps: 223.3\t lr: 0.005\t epsilon: 0.05000000000013396\n",
      "episode: 2969\t average reward: -109.39419291338582\t avg steps: 204.2\t lr: 0.005\t epsilon: 0.050000000000121204\n",
      "episode: 2979\t average reward: -121.66597767672592\t avg steps: 242.9\t lr: 0.005\t epsilon: 0.05000000000010967\n",
      "episode: 2989\t average reward: -116.35466900482244\t avg steps: 229.1\t lr: 0.005\t epsilon: 0.050000000000099236\n",
      "episode: 2999\t average reward: -111.38361266294227\t avg steps: 215.8\t lr: 0.005\t epsilon: 0.05000000000008979\n",
      "episode: 3009\t average reward: -109.24883504193849\t avg steps: 215.6\t lr: 0.005\t epsilon: 0.05000000000008125\n",
      "episode: 3019\t average reward: -121.04232578024796\t avg steps: 234.9\t lr: 0.005\t epsilon: 0.05000000000007352\n",
      "episode: 3029\t average reward: -118.73876166242579\t avg steps: 236.8\t lr: 0.005\t epsilon: 0.05000000000006652\n",
      "episode: 3039\t average reward: -118.87489397794741\t avg steps: 236.8\t lr: 0.005\t epsilon: 0.05000000000006019\n",
      "episode: 3049\t average reward: -126.32264529058116\t avg steps: 250.5\t lr: 0.005\t epsilon: 0.050000000000054466\n",
      "episode: 3059\t average reward: -125.07439375256884\t avg steps: 244.3\t lr: 0.005\t epsilon: 0.05000000000004928\n",
      "episode: 3069\t average reward: -117.56493784826404\t avg steps: 234.3\t lr: 0.005\t epsilon: 0.05000000000004459\n",
      "episode: 3079\t average reward: -118.3384288747346\t avg steps: 236.5\t lr: 0.005\t epsilon: 0.050000000000040346\n",
      "episode: 3089\t average reward: -114.43579595426561\t avg steps: 228.4\t lr: 0.005\t epsilon: 0.05000000000003651\n",
      "episode: 3099\t average reward: -111.21995464852608\t avg steps: 221.5\t lr: 0.005\t epsilon: 0.05000000000003303\n",
      "episode: 3109\t average reward: -117.29336188436831\t avg steps: 234.5\t lr: 0.005\t epsilon: 0.05000000000002989\n",
      "episode: 3119\t average reward: -123.09759136212625\t avg steps: 241.8\t lr: 0.005\t epsilon: 0.05000000000002705\n",
      "episode: 3129\t average reward: -122.50145288501453\t avg steps: 241.9\t lr: 0.005\t epsilon: 0.050000000000024476\n",
      "episode: 3139\t average reward: -109.69737448180562\t avg steps: 218.1\t lr: 0.005\t epsilon: 0.050000000000022145\n",
      "episode: 3149\t average reward: -109.44403330249769\t avg steps: 217.2\t lr: 0.005\t epsilon: 0.050000000000020035\n",
      "episode: 3159\t average reward: -109.83479061205706\t avg steps: 218.3\t lr: 0.005\t epsilon: 0.050000000000018134\n",
      "episode: 3169\t average reward: -112.69420745397396\t avg steps: 223.7\t lr: 0.005\t epsilon: 0.050000000000016406\n",
      "episode: 3179\t average reward: -113.37226928221132\t avg steps: 225.3\t lr: 0.005\t epsilon: 0.050000000000014845\n",
      "episode: 3189\t average reward: -109.8697653014266\t avg steps: 218.3\t lr: 0.005\t epsilon: 0.05000000000001343\n",
      "episode: 3199\t average reward: -109.64792626728111\t avg steps: 218.0\t lr: 0.005\t epsilon: 0.05000000000001215\n",
      "episode: 3209\t average reward: -108.8147113594041\t avg steps: 215.8\t lr: 0.005\t epsilon: 0.050000000000011\n",
      "episode: 3219\t average reward: -110.38264840182649\t avg steps: 220.0\t lr: 0.005\t epsilon: 0.05000000000000995\n",
      "episode: 3229\t average reward: -122.2720465890183\t avg steps: 241.4\t lr: 0.005\t epsilon: 0.050000000000009\n",
      "episode: 3239\t average reward: -108.08430913348946\t avg steps: 214.5\t lr: 0.005\t epsilon: 0.05000000000000815\n",
      "episode: 3249\t average reward: -112.60053739364085\t avg steps: 224.3\t lr: 0.005\t epsilon: 0.05000000000000737\n",
      "episode: 3259\t average reward: -108.50885368126747\t avg steps: 215.6\t lr: 0.005\t epsilon: 0.05000000000000667\n",
      "episode: 3269\t average reward: -113.0249776984835\t avg steps: 225.2\t lr: 0.005\t epsilon: 0.05000000000000604\n",
      "episode: 3279\t average reward: -113.12238540275924\t avg steps: 225.7\t lr: 0.005\t epsilon: 0.050000000000005464\n",
      "episode: 3289\t average reward: -110.17293577981651\t avg steps: 219.0\t lr: 0.005\t epsilon: 0.05000000000000494\n",
      "episode: 3299\t average reward: -113.61603188662534\t avg steps: 226.8\t lr: 0.005\t epsilon: 0.05000000000000447\n",
      "episode: 3309\t average reward: -107.06370929683813\t avg steps: 212.9\t lr: 0.005\t epsilon: 0.05000000000000405\n",
      "episode: 3319\t average reward: -107.86384976525821\t avg steps: 214.0\t lr: 0.005\t epsilon: 0.05000000000000366\n",
      "episode: 3329\t average reward: -114.80393873085339\t avg steps: 229.5\t lr: 0.005\t epsilon: 0.05000000000000331\n",
      "episode: 3339\t average reward: -111.93882141250562\t avg steps: 223.3\t lr: 0.005\t epsilon: 0.050000000000003\n",
      "episode: 3349\t average reward: -116.53818984547462\t avg steps: 227.5\t lr: 0.005\t epsilon: 0.050000000000002716\n",
      "episode: 3359\t average reward: -110.02521779000459\t avg steps: 219.1\t lr: 0.005\t epsilon: 0.05000000000000246\n",
      "episode: 3369\t average reward: -111.73071718538566\t avg steps: 222.7\t lr: 0.005\t epsilon: 0.05000000000000222\n",
      "episode: 3379\t average reward: -109.57914167051223\t avg steps: 217.7\t lr: 0.005\t epsilon: 0.05000000000000201\n",
      "episode: 3389\t average reward: -108.27694470477975\t avg steps: 214.4\t lr: 0.005\t epsilon: 0.05000000000000182\n",
      "episode: 3399\t average reward: -102.3286783042394\t avg steps: 201.5\t lr: 0.005\t epsilon: 0.05000000000000165\n",
      "episode: 3409\t average reward: -114.48944591029024\t avg steps: 228.4\t lr: 0.005\t epsilon: 0.05000000000000149\n",
      "episode: 3419\t average reward: -109.54976744186047\t avg steps: 216.0\t lr: 0.005\t epsilon: 0.05000000000000135\n",
      "episode: 3429\t average reward: -114.64879120879121\t avg steps: 228.5\t lr: 0.005\t epsilon: 0.050000000000001224\n",
      "episode: 3439\t average reward: -114.96747252747252\t avg steps: 228.5\t lr: 0.005\t epsilon: 0.050000000000001106\n",
      "episode: 3449\t average reward: -114.48526176858778\t avg steps: 228.3\t lr: 0.005\t epsilon: 0.050000000000001\n",
      "episode: 3459\t average reward: -111.50068027210884\t avg steps: 221.5\t lr: 0.005\t epsilon: 0.050000000000000905\n",
      "episode: 3469\t average reward: -115.09325744308231\t avg steps: 229.4\t lr: 0.005\t epsilon: 0.05000000000000082\n",
      "episode: 3479\t average reward: -117.32176520994001\t avg steps: 234.4\t lr: 0.005\t epsilon: 0.05000000000000074\n",
      "episode: 3489\t average reward: -116.10731285157941\t avg steps: 232.1\t lr: 0.005\t epsilon: 0.05000000000000067\n",
      "episode: 3499\t average reward: -111.0635497049478\t avg steps: 221.3\t lr: 0.005\t epsilon: 0.050000000000000606\n",
      "episode: 3509\t average reward: -111.64530685920577\t avg steps: 222.6\t lr: 0.005\t epsilon: 0.05000000000000055\n",
      "episode: 3519\t average reward: -112.53157187639947\t avg steps: 224.3\t lr: 0.005\t epsilon: 0.050000000000000495\n",
      "episode: 3529\t average reward: -107.60056523787094\t avg steps: 213.3\t lr: 0.005\t epsilon: 0.050000000000000454\n",
      "episode: 3539\t average reward: -119.1363051864552\t avg steps: 234.3\t lr: 0.005\t epsilon: 0.050000000000000405\n",
      "episode: 3549\t average reward: -109.83239962651727\t avg steps: 215.2\t lr: 0.005\t epsilon: 0.05000000000000037\n",
      "episode: 3559\t average reward: -114.44532279314888\t avg steps: 228.7\t lr: 0.005\t epsilon: 0.050000000000000336\n",
      "episode: 3569\t average reward: -124.67280334728034\t avg steps: 240.0\t lr: 0.005\t epsilon: 0.0500000000000003\n",
      "episode: 3579\t average reward: -111.38858695652173\t avg steps: 221.8\t lr: 0.005\t epsilon: 0.05000000000000027\n",
      "episode: 3589\t average reward: -108.42223282442748\t avg steps: 210.6\t lr: 0.005\t epsilon: 0.050000000000000246\n",
      "episode: 3599\t average reward: -108.63224893917963\t avg steps: 213.1\t lr: 0.005\t epsilon: 0.050000000000000225\n",
      "episode: 3609\t average reward: -112.72739541160594\t avg steps: 223.3\t lr: 0.005\t epsilon: 0.050000000000000204\n",
      "episode: 3619\t average reward: -98.88179419525066\t avg steps: 190.5\t lr: 0.005\t epsilon: 0.05000000000000018\n",
      "episode: 3629\t average reward: -92.96532120523024\t avg steps: 176.9\t lr: 0.005\t epsilon: 0.05000000000000017\n",
      "episode: 3639\t average reward: -91.77413890457369\t avg steps: 178.1\t lr: 0.005\t epsilon: 0.050000000000000155\n",
      "episode: 3649\t average reward: -83.07330482590103\t avg steps: 164.7\t lr: 0.005\t epsilon: 0.050000000000000135\n",
      "episode: 3659\t average reward: -83.93042952208107\t avg steps: 166.3\t lr: 0.005\t epsilon: 0.05000000000000013\n",
      "episode: 3669\t average reward: -80.02714646464646\t avg steps: 159.4\t lr: 0.005\t epsilon: 0.050000000000000114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3679\t average reward: -110.9856215213358\t avg steps: 216.6\t lr: 0.005\t epsilon: 0.0500000000000001\n",
      "episode: 3689\t average reward: -105.025\t avg steps: 205.0\t lr: 0.005\t epsilon: 0.05000000000000009\n",
      "episode: 3699\t average reward: -90.84584755403868\t avg steps: 176.8\t lr: 0.005\t epsilon: 0.050000000000000086\n",
      "episode: 3709\t average reward: -98.5308056872038\t avg steps: 190.9\t lr: 0.005\t epsilon: 0.05000000000000008\n",
      "episode: 3719\t average reward: -95.82088744588745\t avg steps: 185.8\t lr: 0.005\t epsilon: 0.05000000000000007\n",
      "episode: 3729\t average reward: -113.17052154195011\t avg steps: 221.5\t lr: 0.005\t epsilon: 0.050000000000000065\n",
      "episode: 3739\t average reward: -105.0166163141994\t avg steps: 199.6\t lr: 0.005\t epsilon: 0.05000000000000006\n",
      "episode: 3749\t average reward: -85.54928741092637\t avg steps: 169.4\t lr: 0.005\t epsilon: 0.05000000000000005\n",
      "episode: 3759\t average reward: -97.60720130932897\t avg steps: 184.3\t lr: 0.005\t epsilon: 0.050000000000000044\n",
      "episode: 3769\t average reward: -103.33502796136248\t avg steps: 197.7\t lr: 0.005\t epsilon: 0.050000000000000044\n",
      "episode: 3779\t average reward: -106.20446818844098\t avg steps: 206.9\t lr: 0.005\t epsilon: 0.05000000000000004\n",
      "episode: 3789\t average reward: -105.5152271592611\t avg steps: 201.3\t lr: 0.005\t epsilon: 0.05000000000000004\n",
      "episode: 3799\t average reward: -99.5883891213389\t avg steps: 192.2\t lr: 0.005\t epsilon: 0.05000000000000003\n",
      "episode: 3809\t average reward: -94.44942591580099\t avg steps: 183.9\t lr: 0.005\t epsilon: 0.05000000000000003\n",
      "episode: 3819\t average reward: -92.65910348644162\t avg steps: 181.7\t lr: 0.005\t epsilon: 0.05000000000000003\n",
      "episode: 3829\t average reward: -82.28290282902829\t avg steps: 163.6\t lr: 0.005\t epsilon: 0.050000000000000024\n",
      "episode: 3839\t average reward: -92.74307862679956\t avg steps: 181.6\t lr: 0.005\t epsilon: 0.050000000000000024\n",
      "episode: 3849\t average reward: -96.3762914627515\t avg steps: 184.9\t lr: 0.005\t epsilon: 0.050000000000000024\n",
      "episode: 3859\t average reward: -88.29833620195066\t avg steps: 175.3\t lr: 0.005\t epsilon: 0.05000000000000002\n",
      "episode: 3869\t average reward: -87.5043630017452\t avg steps: 172.9\t lr: 0.005\t epsilon: 0.05000000000000002\n",
      "episode: 3879\t average reward: -106.57868275515334\t avg steps: 199.9\t lr: 0.005\t epsilon: 0.05000000000000002\n",
      "episode: 3889\t average reward: -96.1890869800108\t avg steps: 186.1\t lr: 0.005\t epsilon: 0.05000000000000002\n",
      "episode: 3899\t average reward: -170.44255150554676\t avg steps: 253.4\t lr: 0.005\t epsilon: 0.05000000000000002\n",
      "episode: 3909\t average reward: -112.08144578313254\t avg steps: 208.5\t lr: 0.005\t epsilon: 0.05000000000000001\n",
      "episode: 3919\t average reward: -198.99710144927536\t avg steps: 311.5\t lr: 0.005\t epsilon: 0.05000000000000001\n",
      "episode: 3929\t average reward: -97.64955474070194\t avg steps: 191.9\t lr: 0.005\t epsilon: 0.05000000000000001\n",
      "episode: 3939\t average reward: -183.32663812973328\t avg steps: 304.7\t lr: 0.005\t epsilon: 0.05000000000000001\n",
      "episode: 3949\t average reward: -97.9236074270557\t avg steps: 189.5\t lr: 0.005\t epsilon: 0.05000000000000001\n",
      "episode: 3959\t average reward: -93.72297297297297\t avg steps: 178.6\t lr: 0.005\t epsilon: 0.05000000000000001\n",
      "episode: 3969\t average reward: -103.77641277641278\t avg steps: 204.5\t lr: 0.005\t epsilon: 0.05000000000000001\n",
      "episode: 3979\t average reward: -118.91544281263907\t avg steps: 225.7\t lr: 0.005\t epsilon: 0.05000000000000001\n",
      "episode: 3989\t average reward: -116.72907083716652\t avg steps: 218.4\t lr: 0.005\t epsilon: 0.05000000000000001\n",
      "episode: 3999\t average reward: -333.2625250501002\t avg steps: 350.3\t lr: 0.005\t epsilon: 0.05000000000000001\n",
      "episode: 4009\t average reward: -104.39990089197225\t avg steps: 202.8\t lr: 0.005\t epsilon: 0.05000000000000001\n",
      "episode: 4019\t average reward: -94.77433387710713\t avg steps: 184.9\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4029\t average reward: -89.40356937248129\t avg steps: 174.7\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4039\t average reward: -94.65151515151516\t avg steps: 185.8\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4049\t average reward: -104.06162324649299\t avg steps: 200.6\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4059\t average reward: -85.58657878969443\t avg steps: 167.9\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4069\t average reward: -93.73939393939393\t avg steps: 182.5\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4079\t average reward: -95.69022316684378\t avg steps: 189.2\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4089\t average reward: -131.0590195145169\t avg steps: 211.1\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4099\t average reward: -99.19652265542676\t avg steps: 190.8\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4109\t average reward: -98.6270920502092\t avg steps: 192.2\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4119\t average reward: -97.98972972972973\t avg steps: 186.0\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4129\t average reward: -84.55188962207558\t avg steps: 167.7\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4139\t average reward: -91.39018824871648\t avg steps: 176.3\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4149\t average reward: -101.72167216721672\t avg steps: 182.8\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4159\t average reward: -85.65978128797084\t avg steps: 165.6\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4169\t average reward: -104.89872773536895\t avg steps: 197.5\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4179\t average reward: -81.2510849349039\t avg steps: 162.3\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4189\t average reward: -96.83042789223455\t avg steps: 190.3\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4199\t average reward: -94.32336956521739\t avg steps: 185.0\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4209\t average reward: -89.89467658843732\t avg steps: 175.7\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4219\t average reward: -97.09694415173867\t avg steps: 190.8\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4229\t average reward: -98.73141361256545\t avg steps: 192.0\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4239\t average reward: -90.37229190421893\t avg steps: 176.4\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4249\t average reward: -100.16340206185566\t avg steps: 195.0\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4259\t average reward: -116.22169587873046\t avg steps: 212.1\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4269\t average reward: -221.85522450692403\t avg steps: 239.3\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4279\t average reward: -100.04734945959856\t avg steps: 195.3\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4289\t average reward: -93.99238302502721\t avg steps: 184.8\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4299\t average reward: -94.66031573217202\t avg steps: 184.7\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4309\t average reward: -92.63586054233537\t avg steps: 181.7\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4319\t average reward: -105.65741728922092\t avg steps: 188.4\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4329\t average reward: -93.43519029233315\t avg steps: 182.3\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4339\t average reward: -95.01235230934479\t avg steps: 187.2\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4349\t average reward: -91.93023255813954\t avg steps: 177.3\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4359\t average reward: -100.94704049844236\t avg steps: 193.6\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4369\t average reward: -93.24835164835164\t avg steps: 183.0\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4379\t average reward: -96.27759740259741\t avg steps: 185.8\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4389\t average reward: -98.66318537859007\t avg steps: 192.5\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4399\t average reward: -90.17757541263518\t avg steps: 176.7\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4409\t average reward: -193.74179164977707\t avg steps: 247.7\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4419\t average reward: -94.64184782608696\t avg steps: 185.0\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4429\t average reward: -134.11897654584223\t avg steps: 235.5\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4439\t average reward: -92.72581536760642\t avg steps: 181.9\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4449\t average reward: -103.69661266568482\t avg steps: 204.7\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4459\t average reward: -216.61250454380226\t avg steps: 276.1\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4469\t average reward: -107.1864573110893\t avg steps: 204.8\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4479\t average reward: -86.70840787119857\t avg steps: 168.7\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4489\t average reward: -80.41959798994975\t avg steps: 160.2\t lr: 0.005\t epsilon: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 4499\t average reward: -83.15455100794135\t avg steps: 164.7\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4509\t average reward: -80.23139974779319\t avg steps: 159.6\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4519\t average reward: -84.40720720720721\t avg steps: 167.5\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4529\t average reward: -78.77493606138107\t avg steps: 157.4\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4539\t average reward: -90.45650938032973\t avg steps: 176.9\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4549\t average reward: -88.85514834205934\t avg steps: 172.9\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4559\t average reward: -84.2748643761302\t avg steps: 166.9\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4569\t average reward: -80.74528301886792\t avg steps: 160.0\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4579\t average reward: -85.39595719381688\t avg steps: 169.2\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4589\t average reward: -80.10056568196103\t avg steps: 160.1\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4599\t average reward: -77.60739779364049\t avg steps: 155.1\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4609\t average reward: -82.46163290362185\t avg steps: 163.9\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4619\t average reward: -79.59821428571429\t avg steps: 157.8\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4629\t average reward: -80.67084639498432\t avg steps: 160.5\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4639\t average reward: -90.21292556260819\t avg steps: 174.3\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4649\t average reward: -81.53966271080574\t avg steps: 161.1\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4659\t average reward: -86.89461358313817\t avg steps: 171.8\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4669\t average reward: -81.90993788819875\t avg steps: 162.0\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4679\t average reward: -87.9195335276968\t avg steps: 172.5\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4689\t average reward: -89.25844004656578\t avg steps: 172.8\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4699\t average reward: -78.65791159513132\t avg steps: 157.1\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4709\t average reward: -86.64209905660377\t avg steps: 170.6\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4719\t average reward: -83.95487364620939\t avg steps: 167.2\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4729\t average reward: -99.63500533617929\t avg steps: 188.4\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4739\t average reward: -99.37739872068231\t avg steps: 188.6\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4749\t average reward: -89.89927623642943\t avg steps: 166.8\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4759\t average reward: -91.18707482993197\t avg steps: 177.4\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4769\t average reward: -88.75890736342043\t avg steps: 169.4\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4779\t average reward: -92.62759394279304\t avg steps: 179.3\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4789\t average reward: -95.43704936217415\t avg steps: 181.3\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4799\t average reward: -99.49127589967284\t avg steps: 184.4\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4809\t average reward: -77.93892137751787\t avg steps: 154.9\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4819\t average reward: -78.22892347600519\t avg steps: 155.2\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4829\t average reward: -83.79332073093887\t avg steps: 159.7\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4839\t average reward: -84.2366504854369\t avg steps: 165.8\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4849\t average reward: -93.3211334120425\t avg steps: 170.4\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4859\t average reward: -82.2423676012461\t avg steps: 161.5\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4869\t average reward: -82.767252195734\t avg steps: 160.4\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4879\t average reward: -83.09730069052102\t avg steps: 160.3\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4889\t average reward: -81.04020100502512\t avg steps: 160.2\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4899\t average reward: -77.98445595854922\t avg steps: 155.4\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4909\t average reward: -74.03909465020575\t avg steps: 146.8\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4919\t average reward: -75.30073677160081\t avg steps: 150.3\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4929\t average reward: -93.38275862068966\t avg steps: 175.0\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4939\t average reward: -86.54298082869512\t avg steps: 162.7\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4949\t average reward: -76.06378737541529\t avg steps: 151.5\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4959\t average reward: -74.67095463777929\t avg steps: 148.7\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4969\t average reward: -79.98421052631579\t avg steps: 153.0\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4979\t average reward: -91.94489092996555\t avg steps: 175.2\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4989\t average reward: -76.92261507671782\t avg steps: 150.9\t lr: 0.005\t epsilon: 0.05\n",
      "episode: 4999\t average reward: -97.26597582037996\t avg steps: 174.7\t lr: 0.005\t epsilon: 0.05\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "# initialize size of state and action space\n",
    "state_space = 20 * 20\n",
    "action_space = env.action_space.n\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "# Training parameters\n",
    "n_training_episodes = 5000  # Total training episodes\n",
    "initial_lr = 0.2  # Learning rate       # old: 1\n",
    "k = 0.005  # lr decay                    # old: 0.005\n",
    "min_lr = 0.005\n",
    "\n",
    "\n",
    "# Environment parameters\n",
    "max_steps = 10000  # Max steps per episode\n",
    "gamma = 0.99  # Discounting rate\n",
    "eval_seed = []  # The evaluation seed of the environment\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1#0.95   # Exploration probability at start                     # old: 1\n",
    "min_epsilon = 0.05  # Minimum exploration probability\n",
    "decay_rate = 0.01  # Exponential decay rate for exploration prob           # old: 0.002\n",
    "\n",
    "\n",
    "grid_x, grid_v = initialize_grids()\n",
    "state_to_qtable = initialize_state_dict()\n",
    "\n",
    "# Training\n",
    "q_car = initialize_q_table(state_space, action_space)\n",
    "q_car, avg_rewards, total_steps = train(q_car)\n",
    "\n",
    "\n",
    "#\n",
    "# Plot V values for random init training\n",
    "actions = np.max(q_car, axis=1)\n",
    "actions = actions.reshape((20, 20))\n",
    "plt.figure(figsize=(16, 12))\n",
    "ax = plt.subplot(111)\n",
    "ax = sns.heatmap(actions, annot=True)\n",
    "plt.ylim(0, 20)\n",
    "plt.xlabel(\"Position\", fontsize=20)\n",
    "plt.ylabel(\"Velocity\", fontsize=20)\n",
    "plt.title(\"Q values for optimal action - random init w/ lr decay\", fontdict={'fontsize': 25})\n",
    "# plt.savefig('plots/v_values_rand_decay')\n",
    "with open('plots/q_values.pkl','wb') as fid:\n",
    "    pickle.dump(ax, fid)\n",
    "plt.savefig('plots/q_values_rand_decay.png')\n",
    "plt.close()\n",
    "\n",
    "plot_steps(total_steps, 'rand_decay')\n",
    "plot_rewards(avg_rewards, 'rand_decay')\n",
    "\n",
    "# # Save Q table\n",
    "np.savetxt('data/q_rand_decay.txt', q_car)\n",
    "np.array(avg_rewards)\n",
    "np.savetxt('data/avg_rewards_rand_decay.txt', avg_rewards)\n",
    "np.array(total_steps)\n",
    "np.savetxt('data/total_steps_rand_decay.txt', total_steps)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10ca530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import greedy_policy, initialize_grids, initialize_state_dict, get_closest_in_grid\n",
    "import matplotlib.image as img\n",
    "\n",
    "\n",
    "# Evaluation Parameters\n",
    "seed = []\n",
    "n_eval_episodes = 1\n",
    "max_steps = 10000\n",
    "\n",
    "# load q table\n",
    "q = np.loadtxt('data/q_rand_decay.txt')\n",
    "\n",
    "# initialize discretization\n",
    "grid_x, grid_v = initialize_grids()\n",
    "state_to_qtable = initialize_state_dict()\n",
    "\n",
    "# trace\n",
    "file = open('trace/trace_eval.txt', 'w')\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "for episode in range(n_eval_episodes):\n",
    "    if seed:\n",
    "        state = env.reset(seed=seed[episode])\n",
    "    else:\n",
    "        state = env.reset()[0]\n",
    "    step = 0\n",
    "    terminated = False\n",
    "    total_rewards_ep = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = greedy_policy(q, state, grid_x, grid_v, state_to_qtable)\n",
    "\n",
    "        # write current state and action taken to trace\n",
    "        file.write(f'{state[0]},{state[1]},{action}\\n')\n",
    "\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        total_rewards_ep += reward\n",
    "\n",
    "        if terminated:  # or truncated:\n",
    "            break\n",
    "        state = new_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40daa0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trace = []\n",
    "actions = []\n",
    "\n",
    "# load trace and actions into list\n",
    "with open('trace/trace_eval.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        x, v, a = line.split(',')\n",
    "        trace.append((float(x), float(v)))\n",
    "        actions.append(int(a))\n",
    "\n",
    "# discretize trace\n",
    "trace_discrete = []\n",
    "grid_x, grid_v = initialize_grids()\n",
    "state_to_q = initialize_state_dict()\n",
    "for trac in trace:\n",
    "    # state = state_to_q[get_closest_in_grid(np.asarray(trac), grid_x, grid_v)]\n",
    "    x, v = get_closest_in_grid(np.asarray(trac), grid_x, grid_v)\n",
    "    state = [grid_x.index(x)+0.5, grid_v.index(v)+0.5]\n",
    "    trace_discrete.append(state)\n",
    "\n",
    "# load v values plot\n",
    "plot = img.imread('plots/q_values_rand_decay.png')\n",
    "\n",
    "# plot trace onto plot\n",
    "fig, ax = plt.subplots()\n",
    "# ax.imshow(plot, extent=[0, 20, 0, 20])\n",
    "with open('plots/q_values.pkl', 'rb') as fid:\n",
    "    ax = pickle.load(fid)\n",
    "ax.plot(*zip(*trace_discrete[::1]))\n",
    "\n",
    "# plot starting point\n",
    "sns.lineplot(x=[trace_discrete[0][0]], y=[trace_discrete[0][1]], marker='o', markersize=25, markeredgecolor='green', color='green', markeredgewidth=2)\n",
    "# plot finish point\n",
    "sns.lineplot(x=[trace_discrete[-1][0]+1], y=[trace_discrete[-1][1]], marker='o', markersize=25, markeredgecolor='red', color='red', markeredgewidth=2)\n",
    "\n",
    "ax.set_xticklabels(grid_x)\n",
    "ax.set_yticklabels(grid_v)\n",
    "plt.savefig('trace/trace.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ffdddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
