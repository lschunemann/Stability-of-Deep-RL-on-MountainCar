{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc04ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helper_DQN import running_mean, scale_and_resize, ExperienceMemory, PrioritizedExperienceReplayBuffer\n",
    "import collections\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from models import MLP_state\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "\n",
    "class TrainMountainCar:\n",
    "    def __init__(self, n_training_episodes=200, gamma=0.99, learning_rate=0.1, epsilon_max=0.5,\n",
    "                 epsilon_min=0.05, max_steps=10000, batch_size=32, fixed_target=False, min_replay=80000,\n",
    "                 copy_target=10000, replay_size=100000, double=False, dueling=False, prioritized=False, debug=False,\n",
    "                 eval_epsilon=None, eval_episodes=10, eval_every=50, noisy=False, distributional=False, env=None,\n",
    "                 epsilon_frame=500000):\n",
    "        self.n_training_episodes = n_training_episodes\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon_max = epsilon_max\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_frame = epsilon_frame\n",
    "        self.max_steps = max_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.fixed_target = fixed_target\n",
    "        self.copy_target = copy_target\n",
    "        self.replay_size = replay_size\n",
    "        self.double = double\n",
    "        self.dueling = dueling\n",
    "        self.debug = debug\n",
    "        self.eval_epsilon = eval_epsilon if eval_epsilon is not None else epsilon_min\n",
    "        self.eval_episodes = eval_episodes\n",
    "        self.eval_every = eval_every\n",
    "        self.prioritized = prioritized\n",
    "        self.min_replay = min_replay\n",
    "\n",
    "    def epsilon_greedy_policy(self, policy: torch.nn.Module, X, epsilon: float, env: gym.envs):\n",
    "        \"\"\"\n",
    "        Samples a random action with probability epsilon and picks the maximum action under policy network otherwise.\n",
    "        :param policy: Policy Network under which to take action\n",
    "        :param X: stacked tensor of shape (4,80,120)\n",
    "        :param epsilon: float probability of sampling a random action\n",
    "        :param env: Gymnasium environment\n",
    "        :return: Randomly sampled action or maximum action under policy network\n",
    "        \"\"\"\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                X = np.vstack(X).astype(np.float32)\n",
    "                X = torch.tensor(X).squeeze(1)\n",
    "                return policy(X).max(0)[1].view(1, 1).item()\n",
    "\n",
    "\n",
    "#     def get_action(self, policy, s, eps=0.1):\n",
    "#         with torch.no_grad():\n",
    "#             self.seq.pop(0)\n",
    "#             self.seq.append(s)\n",
    "#             if np.random.random() >= eps:\n",
    "#                 X = torch.tensor(np.vstack(self.seq).astype(np.float32), device=device, dtype=torch.float)\n",
    "#                 a = policy(X.unsqueeze(0))\n",
    "#                 # a = a[:, -1, :]  # select last element of seq\n",
    "#                 a = a.max(1)[1]\n",
    "#                 return a.item()\n",
    "#             else:\n",
    "#                 return env.action_space.sample()\n",
    "\n",
    "    # def initialize_measuring_states(self, env):\n",
    "    #     \"\"\"\n",
    "    #     Randomly samples 200 states by taking random actions\n",
    "    #     :param env: Gymnasium environment\n",
    "    #     :return: list of states that were visited by random walk\n",
    "    #     \"\"\"\n",
    "    #     measuring_states = []\n",
    "    #     env.reset()\n",
    "    #     for i in range(200):\n",
    "    #         action = env.action_space.sample()\n",
    "    #         env.step(action)\n",
    "    #         img = env.render()\n",
    "    #         img = transforms.ToTensor()(img)\n",
    "    #         measuring_states.append(transform(img))\n",
    "    #     env.reset()\n",
    "    #     return measuring_states\n",
    "\n",
    "    def eval(self, policy: torch.nn.Module, env: gym.envs):\n",
    "        \"\"\"\n",
    "        Evaluate a policy and return the average reward over self.eval_episodes trials with maximum 10000 steps each\n",
    "        :param policy: The policy to be evaluated\n",
    "        :param env: The Gymnasium environment\n",
    "        :return: average over rewards collected turing trials\n",
    "        \"\"\"\n",
    "        rewards_list = []\n",
    "        for episode in range(self.eval_episodes):\n",
    "            state = env.reset()[0]\n",
    "\n",
    "            # up to 30 no-op actions\n",
    "            noop = random.randint(0, 30)\n",
    "            for i in range(noop):\n",
    "                action = env.action_space.sample()\n",
    "                state,_,_,_,_ = env.step(action)\n",
    "\n",
    "            rewards = 0\n",
    "\n",
    "            for i in range(0, self.max_steps):      # max episode length 10000\n",
    "                action = self.epsilon_greedy_policy(policy, state, self.eval_epsilon, env)\n",
    "#                 action = self.get_action(policy, state, self.eval_epsilon)\n",
    "                state, reward, terminated, _, _ = env.step(action)\n",
    "                rewards += reward\n",
    "\n",
    "                if terminated:\n",
    "                    break\n",
    "\n",
    "            rewards_list.append(rewards)\n",
    "\n",
    "        return np.mean(rewards_list)\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        trains DQN using a fixed target network if self.fixed_target == True, otherwise with the policy network.\n",
    "        :return: list of total rewards, list of steps in each episode, q values over sampled states\n",
    "        \"\"\"\n",
    "\n",
    "        # keep track of total steps and rewards\n",
    "        total_steps = 0\n",
    "        total_rewards = []\n",
    "        total_steps_list = []\n",
    "        evaluations = []\n",
    "        td_errors = []\n",
    "\n",
    "        # initialize states in which Q value is measured every X episodes to track progress\n",
    "        # measuring_states = self.initialize_measuring_states(env)\n",
    "        # q_measures = []\n",
    "\n",
    "        # Initialize Experience Memory\n",
    "        if self.prioritized:\n",
    "            beta_start = 0.5\n",
    "            beta_frames = 1000\n",
    "            beta_by_frame = lambda total_steps: min(1.0, beta_start + total_steps * (1.0 - beta_start) / beta_frames)\n",
    "            experience_memory = PrioritizedExperienceReplayBuffer(alpha=0.7, batch_size=self.batch_size,\n",
    "                                                                  buffer_size=self.replay_size)\n",
    "        else:\n",
    "            experience_memory = ExperienceMemory(self.replay_size)\n",
    "\n",
    "        # initialize policy (and target) network\n",
    "        if self.dueling:\n",
    "            policy = MLP_Dueling_state(env.action_space.n).to(device)\n",
    "            if self.fixed_target:\n",
    "                target = MLP_Dueling_state(env.action_space.n).to(device)\n",
    "                target.load_state_dict(policy.state_dict())\n",
    "                target.eval()\n",
    "        else:\n",
    "            policy = MLP_state(env.action_space.n).to(device)\n",
    "            if self.fixed_target:\n",
    "                target = MLP_state(env.action_space.n).to(device)\n",
    "                target.load_state_dict(policy.state_dict())\n",
    "                target.eval()\n",
    "\n",
    "        # Best values found during evaluation\n",
    "        best_reward = - float('inf')\n",
    "        best_policy = policy.state_dict()\n",
    "\n",
    "        optimizer = torch.optim.RMSprop(policy.parameters(), lr=self.learning_rate, weight_decay=0.99, momentum=0.95) \n",
    "\n",
    "        for episode in range(self.n_training_episodes):\n",
    "            steps = 0\n",
    "            total_steps += 1\n",
    "            rewards = 0\n",
    "\n",
    "            state = env.reset()[0]\n",
    "\n",
    "            # up to 30 no-op actions\n",
    "            noop = random.randint(0, 30)\n",
    "            for i in range(noop):\n",
    "                action = env.action_space.sample()\n",
    "                state, _, _, _, _ = env.step(action)\n",
    "\n",
    "            while True:\n",
    "                # linear epsilon decay based on steps\n",
    "                epsilon = max(self.epsilon_max - ((self.epsilon_max - self.epsilon_min)/self.epsilon_frame) *\n",
    "                              total_steps, self.epsilon_min)\n",
    "\n",
    "                # Choose the action At using epsilon greedy policy\n",
    "                action = self.epsilon_greedy_policy(policy, state, epsilon, env)\n",
    "#                 action = self.get_action(policy, X, epsilon)\n",
    "                # take action\n",
    "                new_state, reward, terminated, _, _ = env.step(action)\n",
    "\n",
    "                experience_memory.add((state, action, reward, new_state, terminated))\n",
    "\n",
    "                steps += 1\n",
    "                total_steps += 1\n",
    "\n",
    "                if len(experience_memory) >= self.min_replay:  # self.batch_size:\n",
    "                    if self.prioritized:\n",
    "                        beta = beta_by_frame(total_steps)\n",
    "                        idxs, experiences, weights = experience_memory.sample(beta)\n",
    "#                         states, actions, _rewards, next_states, terminations = (i for i in\n",
    "#                                                                                 zip(*experiences))  # (torch.Tensor(vs).to(device) for vs in\n",
    "                        # zip(*experiences))\n",
    "                        weights = torch.tensor(weights).to(device)\n",
    "                    else:\n",
    "                        experiences = experience_memory.sample(self.batch_size)\n",
    "#                     print(list(len(i) for i in zip(*experiences)))\n",
    "                    states, actions, _rewards, next_states, terminations = (i for i in zip(*experiences))\n",
    "                    a = torch.tensor(actions).long().unsqueeze(dim=1).to(device)\n",
    "                    r = torch.tensor(_rewards).unsqueeze(dim=1).to(device)\n",
    "                    states = np.vstack(states).astype(np.float32)\n",
    "                    states = torch.tensor(states).to(device)\n",
    "                    next_states = np.vstack(states).astype(np.float32)\n",
    "                    next_states = torch.tensor(next_states).to(device)\n",
    "#                     states = np.vstack(states).astype(np.float32)\n",
    "#                     states = torch.from_numpy(states)\n",
    "#                     next_states = np.vstack(next_states).astype(np.float32)\n",
    "#                     next_states = torch.from_numpy(next_states)\n",
    "#                     states = torch.reshape(states, (self.batch_size, 1, 84, 84)).to(device)  # 80,120\n",
    "#                     next_states = torch.reshape(next_states, (self.batch_size, 1, 84, 84)).to(device)\n",
    "                    mask = [i for i, x in enumerate(terminations) if not x]  # get all non-final states\n",
    "                    \n",
    "#                     reward = torch.tensor([reward]).to(device)\n",
    "#                     action = torch.tensor([action]).unsqueeze(0).to(device)\n",
    "#                     state = X.unsqueeze(0)\n",
    "#                     new_state = X_new.unsqueeze(0)\n",
    "                    \n",
    "#                     steps += 1\n",
    "#                     total_steps += 1\n",
    "                    \n",
    "                    state_action_values = policy(states).gather(1, a)\n",
    "                    \n",
    "                    next_state_values = torch.zeros(self.batch_size, device=device)\n",
    "                    \n",
    "                    # update network\n",
    "                    if self.double:\n",
    "                        max_next_action = policy(next_states).max(1)[1].view(-1, 1)\n",
    "                        next_state_values[mask] = target(next_states[mask]).gather(1, max_next_action[mask]).squeeze(1)\n",
    "                    elif self.fixed_target:\n",
    "                        next_state_values[mask] = target(next_states[mask]).max(1)[0].detach()\n",
    "                    else:\n",
    "                        next_state_values[mask] = policy(next_states[mask]).max(1)[0].detach()\n",
    "                    # Compute the expected Q values\n",
    "                    expected_state_action_values = (next_state_values * self.gamma) + r.squeeze(1)\n",
    "                    \n",
    "\n",
    "                    if self.prioritized:\n",
    "                            diff = expected_state_action_values.unsqueeze(1) - state_action_values\n",
    "                            experience_memory.update_priority(idxs, diff.cpu().detach().squeeze().abs().numpy().tolist())\n",
    "\n",
    "                            loss = torch.nn.MSELoss()(state_action_values,\n",
    "                                                      expected_state_action_values.unsqueeze(1)).squeeze() * weights\n",
    "                    else:\n",
    "                    # loss = torch.nn.MSELoss()(state_action_values, expected_state_action_values.unsqueeze(1)).squeeze()\n",
    "                        loss = torch.nn.MSELoss()(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "                    loss = loss.mean()\n",
    "                    td_errors.append(loss.detach().cpu())  # save td errors\n",
    "\n",
    "                    # Optimize the model\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "\n",
    "                    torch.nn.utils.clip_grad_norm_(policy.parameters(), 10.)    # clip gradients\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Update total reward\n",
    "                rewards += reward\n",
    "\n",
    "                # update current state to be next state\n",
    "                state = new_state\n",
    "\n",
    "                # If done, finish the episode\n",
    "                if terminated or steps >= self.max_steps-1:  # or truncated:\n",
    "                    # Track rewards\n",
    "                    total_rewards.append(rewards)\n",
    "                    total_steps_list.append(steps)\n",
    "\n",
    "                    # # measure Q values in selected states\n",
    "                    # Q_states = torch.stack(measuring_states).to(device)\n",
    "                    # Q_states = torch.unique(Q_states, dim=0, sorted=False)  # eliminate duplicate states\n",
    "                    # with torch.no_grad():\n",
    "                    #     q_measures.append(torch.mean(policy(Q_states).max(1)[0]).item())\n",
    "\n",
    "                    # Evaluate current policy and save optimal policy weights\n",
    "                    if episode == self.n_training_episodes-1 or (episode > 0 and episode % self.eval_every == 0):\n",
    "                        eval_reward = self.eval(policy, env)\n",
    "                        if eval_reward > best_reward:\n",
    "                            best_reward = eval_reward\n",
    "                            best_policy = policy.state_dict()\n",
    "                        print(f\"Evaluation: {int(episode/self.eval_every)}\\t average reward: {eval_reward}\")\n",
    "                        evaluations.append(eval_reward)\n",
    "\n",
    "                    # print training information\n",
    "                    if self.debug:\n",
    "                        print(f\"episode: {episode + 1:03d}\\t steps: {steps + 1:05d}\\t total steps:\"\n",
    "                              f\"{total_steps + 1:06d}\\t epsilon: {epsilon:.2f}\")#\\t average Q: {q_measures[-1]:.3f}\")\n",
    "                    break\n",
    "\n",
    "                if self.fixed_target:\n",
    "                    # copy policy network weights to target net every copy_target steps\n",
    "                    if total_steps % self.copy_target <= 4:\n",
    "                        target.load_state_dict(policy.state_dict())\n",
    "\n",
    "        return total_rewards, total_steps_list, best_policy, evaluations, td_errors, policy.state_dict()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89234e0a",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08516760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 001\t steps: 10000\t total steps:010001\t epsilon: 0.99\n",
      "episode: 002\t steps: 10000\t total steps:020001\t epsilon: 0.98\n",
      "episode: 003\t steps: 03565\t total steps:023566\t epsilon: 0.98\n",
      "episode: 004\t steps: 10000\t total steps:033566\t epsilon: 0.97\n",
      "episode: 005\t steps: 10000\t total steps:043566\t epsilon: 0.96\n",
      "episode: 006\t steps: 10000\t total steps:053566\t epsilon: 0.95\n",
      "episode: 007\t steps: 10000\t total steps:063566\t epsilon: 0.94\n",
      "episode: 008\t steps: 10000\t total steps:073566\t epsilon: 0.93\n",
      "episode: 009\t steps: 10000\t total steps:083566\t epsilon: 0.92\n",
      "episode: 010\t steps: 10000\t total steps:093566\t epsilon: 0.92\n",
      "episode: 011\t steps: 10000\t total steps:103566\t epsilon: 0.91\n",
      "episode: 012\t steps: 10000\t total steps:113566\t epsilon: 0.90\n",
      "episode: 013\t steps: 10000\t total steps:123566\t epsilon: 0.89\n",
      "episode: 014\t steps: 10000\t total steps:133566\t epsilon: 0.88\n",
      "episode: 015\t steps: 10000\t total steps:143566\t epsilon: 0.87\n",
      "episode: 016\t steps: 10000\t total steps:153566\t epsilon: 0.86\n",
      "episode: 017\t steps: 05515\t total steps:159081\t epsilon: 0.86\n",
      "episode: 018\t steps: 09281\t total steps:168362\t epsilon: 0.85\n",
      "episode: 019\t steps: 03734\t total steps:172096\t epsilon: 0.85\n",
      "episode: 020\t steps: 10000\t total steps:182096\t epsilon: 0.84\n",
      "episode: 021\t steps: 06278\t total steps:188374\t epsilon: 0.83\n",
      "episode: 022\t steps: 10000\t total steps:198374\t epsilon: 0.82\n",
      "episode: 023\t steps: 10000\t total steps:208374\t epsilon: 0.81\n",
      "episode: 024\t steps: 10000\t total steps:218374\t epsilon: 0.80\n",
      "episode: 025\t steps: 10000\t total steps:228374\t epsilon: 0.79\n",
      "episode: 026\t steps: 10000\t total steps:238374\t epsilon: 0.79\n",
      "episode: 027\t steps: 02194\t total steps:240568\t epsilon: 0.78\n",
      "episode: 028\t steps: 10000\t total steps:250568\t epsilon: 0.77\n",
      "episode: 029\t steps: 10000\t total steps:260568\t epsilon: 0.77\n",
      "episode: 030\t steps: 10000\t total steps:270568\t epsilon: 0.76\n",
      "episode: 031\t steps: 10000\t total steps:280568\t epsilon: 0.75\n",
      "episode: 032\t steps: 05092\t total steps:285660\t epsilon: 0.74\n",
      "episode: 033\t steps: 10000\t total steps:295660\t epsilon: 0.73\n",
      "episode: 034\t steps: 10000\t total steps:305660\t epsilon: 0.72\n",
      "episode: 035\t steps: 10000\t total steps:315660\t epsilon: 0.72\n",
      "episode: 036\t steps: 09055\t total steps:324715\t epsilon: 0.71\n",
      "episode: 037\t steps: 10000\t total steps:334715\t epsilon: 0.70\n",
      "episode: 038\t steps: 10000\t total steps:344715\t epsilon: 0.69\n",
      "episode: 039\t steps: 10000\t total steps:354715\t epsilon: 0.68\n",
      "episode: 040\t steps: 10000\t total steps:364715\t epsilon: 0.67\n",
      "episode: 041\t steps: 10000\t total steps:374715\t epsilon: 0.66\n",
      "episode: 042\t steps: 10000\t total steps:384715\t epsilon: 0.65\n",
      "episode: 043\t steps: 04834\t total steps:389549\t epsilon: 0.65\n",
      "episode: 044\t steps: 02382\t total steps:391931\t epsilon: 0.65\n",
      "episode: 045\t steps: 08314\t total steps:400245\t epsilon: 0.64\n",
      "episode: 046\t steps: 10000\t total steps:410245\t epsilon: 0.63\n",
      "episode: 047\t steps: 10000\t total steps:420245\t epsilon: 0.62\n",
      "episode: 048\t steps: 10000\t total steps:430245\t epsilon: 0.61\n",
      "episode: 049\t steps: 10000\t total steps:440245\t epsilon: 0.60\n",
      "episode: 050\t steps: 10000\t total steps:450245\t epsilon: 0.59\n",
      "Evaluation: 1\t average reward: -7206.5\n",
      "episode: 051\t steps: 06290\t total steps:456535\t epsilon: 0.59\n",
      "episode: 052\t steps: 10000\t total steps:466535\t epsilon: 0.58\n",
      "episode: 053\t steps: 10000\t total steps:476535\t epsilon: 0.57\n",
      "episode: 054\t steps: 10000\t total steps:486535\t epsilon: 0.56\n",
      "episode: 055\t steps: 10000\t total steps:496535\t epsilon: 0.55\n",
      "episode: 056\t steps: 04364\t total steps:500899\t epsilon: 0.55\n",
      "episode: 057\t steps: 09395\t total steps:510294\t epsilon: 0.54\n",
      "episode: 058\t steps: 05280\t total steps:515574\t epsilon: 0.54\n",
      "episode: 059\t steps: 10000\t total steps:525574\t epsilon: 0.53\n",
      "episode: 060\t steps: 10000\t total steps:535574\t epsilon: 0.52\n",
      "episode: 061\t steps: 10000\t total steps:545574\t epsilon: 0.51\n",
      "episode: 062\t steps: 01882\t total steps:547456\t epsilon: 0.51\n",
      "episode: 063\t steps: 00851\t total steps:548307\t epsilon: 0.51\n",
      "episode: 064\t steps: 10000\t total steps:558307\t epsilon: 0.50\n",
      "episode: 065\t steps: 01832\t total steps:560139\t epsilon: 0.50\n",
      "episode: 066\t steps: 09307\t total steps:569446\t epsilon: 0.49\n",
      "episode: 067\t steps: 10000\t total steps:579446\t epsilon: 0.48\n",
      "episode: 068\t steps: 03692\t total steps:583138\t epsilon: 0.48\n",
      "episode: 069\t steps: 06168\t total steps:589306\t epsilon: 0.47\n",
      "episode: 070\t steps: 04809\t total steps:594115\t epsilon: 0.47\n",
      "episode: 071\t steps: 05824\t total steps:599939\t epsilon: 0.46\n",
      "episode: 072\t steps: 10000\t total steps:609939\t epsilon: 0.45\n",
      "episode: 073\t steps: 10000\t total steps:619939\t epsilon: 0.44\n",
      "episode: 074\t steps: 10000\t total steps:629939\t epsilon: 0.43\n",
      "episode: 075\t steps: 07272\t total steps:637211\t epsilon: 0.43\n",
      "episode: 076\t steps: 05589\t total steps:642800\t epsilon: 0.42\n",
      "episode: 077\t steps: 10000\t total steps:652800\t epsilon: 0.41\n",
      "episode: 078\t steps: 10000\t total steps:662800\t epsilon: 0.40\n",
      "episode: 079\t steps: 06848\t total steps:669648\t epsilon: 0.40\n",
      "episode: 080\t steps: 08473\t total steps:678121\t epsilon: 0.39\n",
      "episode: 081\t steps: 10000\t total steps:688121\t epsilon: 0.38\n",
      "episode: 082\t steps: 10000\t total steps:698121\t epsilon: 0.37\n",
      "episode: 083\t steps: 10000\t total steps:708121\t epsilon: 0.36\n",
      "episode: 084\t steps: 10000\t total steps:718121\t epsilon: 0.35\n",
      "episode: 085\t steps: 09051\t total steps:727172\t epsilon: 0.35\n",
      "episode: 086\t steps: 09160\t total steps:736332\t epsilon: 0.34\n",
      "episode: 087\t steps: 01457\t total steps:737789\t epsilon: 0.34\n",
      "episode: 088\t steps: 05265\t total steps:743054\t epsilon: 0.33\n",
      "episode: 089\t steps: 08874\t total steps:751928\t epsilon: 0.32\n",
      "episode: 090\t steps: 10000\t total steps:761928\t epsilon: 0.31\n",
      "episode: 091\t steps: 10000\t total steps:771928\t epsilon: 0.31\n",
      "episode: 092\t steps: 06010\t total steps:777938\t epsilon: 0.30\n",
      "episode: 093\t steps: 10000\t total steps:787938\t epsilon: 0.29\n",
      "episode: 094\t steps: 10000\t total steps:797938\t epsilon: 0.28\n",
      "episode: 095\t steps: 07945\t total steps:805883\t epsilon: 0.27\n",
      "episode: 096\t steps: 09312\t total steps:815195\t epsilon: 0.27\n",
      "episode: 097\t steps: 05827\t total steps:821022\t epsilon: 0.26\n",
      "episode: 098\t steps: 03258\t total steps:824280\t epsilon: 0.26\n",
      "episode: 099\t steps: 05595\t total steps:829875\t epsilon: 0.25\n",
      "episode: 100\t steps: 02798\t total steps:832673\t epsilon: 0.25\n",
      "Evaluation: 2\t average reward: -8746.0\n",
      "episode: 101\t steps: 02864\t total steps:835537\t epsilon: 0.25\n",
      "episode: 102\t steps: 01310\t total steps:836847\t epsilon: 0.25\n",
      "episode: 103\t steps: 01329\t total steps:838176\t epsilon: 0.25\n",
      "episode: 104\t steps: 02732\t total steps:840908\t epsilon: 0.24\n",
      "episode: 105\t steps: 10000\t total steps:850908\t epsilon: 0.23\n",
      "episode: 106\t steps: 03580\t total steps:854488\t epsilon: 0.23\n",
      "episode: 107\t steps: 10000\t total steps:864488\t epsilon: 0.22\n",
      "episode: 108\t steps: 02030\t total steps:866518\t epsilon: 0.22\n",
      "episode: 109\t steps: 10000\t total steps:876518\t epsilon: 0.21\n",
      "episode: 110\t steps: 02004\t total steps:878522\t epsilon: 0.21\n",
      "episode: 111\t steps: 07434\t total steps:885956\t epsilon: 0.20\n",
      "episode: 112\t steps: 02567\t total steps:888523\t epsilon: 0.20\n",
      "episode: 113\t steps: 01995\t total steps:890518\t epsilon: 0.20\n",
      "episode: 114\t steps: 10000\t total steps:900518\t epsilon: 0.19\n",
      "episode: 115\t steps: 06759\t total steps:907277\t epsilon: 0.18\n",
      "episode: 116\t steps: 00879\t total steps:908156\t epsilon: 0.18\n",
      "episode: 117\t steps: 03138\t total steps:911294\t epsilon: 0.18\n",
      "episode: 118\t steps: 04012\t total steps:915306\t epsilon: 0.18\n",
      "episode: 119\t steps: 01182\t total steps:916488\t epsilon: 0.18\n",
      "episode: 120\t steps: 02646\t total steps:919134\t epsilon: 0.17\n",
      "episode: 121\t steps: 07335\t total steps:926469\t epsilon: 0.17\n",
      "episode: 122\t steps: 00782\t total steps:927251\t epsilon: 0.17\n",
      "episode: 123\t steps: 04612\t total steps:931863\t epsilon: 0.16\n",
      "episode: 124\t steps: 10000\t total steps:941863\t epsilon: 0.15\n",
      "episode: 125\t steps: 05058\t total steps:946921\t epsilon: 0.15\n",
      "episode: 126\t steps: 03867\t total steps:950788\t epsilon: 0.14\n",
      "episode: 127\t steps: 00584\t total steps:951372\t epsilon: 0.14\n",
      "episode: 128\t steps: 00316\t total steps:951688\t epsilon: 0.14\n",
      "episode: 129\t steps: 04985\t total steps:956673\t epsilon: 0.14\n",
      "episode: 130\t steps: 02425\t total steps:959098\t epsilon: 0.14\n",
      "episode: 131\t steps: 03873\t total steps:962971\t epsilon: 0.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 132\t steps: 03303\t total steps:966274\t epsilon: 0.13\n",
      "episode: 133\t steps: 05670\t total steps:971944\t epsilon: 0.13\n",
      "episode: 134\t steps: 04898\t total steps:976842\t epsilon: 0.12\n",
      "episode: 135\t steps: 01614\t total steps:978456\t epsilon: 0.12\n",
      "episode: 136\t steps: 03358\t total steps:981814\t epsilon: 0.12\n",
      "episode: 137\t steps: 04152\t total steps:985966\t epsilon: 0.11\n",
      "episode: 138\t steps: 04935\t total steps:990901\t epsilon: 0.11\n",
      "episode: 139\t steps: 07687\t total steps:998588\t epsilon: 0.10\n",
      "episode: 140\t steps: 01952\t total steps:1000540\t epsilon: 0.10\n",
      "episode: 141\t steps: 00927\t total steps:1001467\t epsilon: 0.10\n",
      "episode: 142\t steps: 01887\t total steps:1003354\t epsilon: 0.10\n",
      "episode: 143\t steps: 01941\t total steps:1005295\t epsilon: 0.10\n",
      "episode: 144\t steps: 01120\t total steps:1006415\t epsilon: 0.10\n",
      "episode: 145\t steps: 02433\t total steps:1008848\t epsilon: 0.10\n",
      "episode: 146\t steps: 01411\t total steps:1010259\t epsilon: 0.10\n",
      "episode: 147\t steps: 02130\t total steps:1012389\t epsilon: 0.10\n",
      "episode: 148\t steps: 01650\t total steps:1014039\t epsilon: 0.10\n",
      "episode: 149\t steps: 00697\t total steps:1014736\t epsilon: 0.10\n",
      "episode: 150\t steps: 01787\t total steps:1016523\t epsilon: 0.10\n",
      "Evaluation: 3\t average reward: -10000.0\n",
      "episode: 151\t steps: 01207\t total steps:1017730\t epsilon: 0.10\n",
      "episode: 152\t steps: 01281\t total steps:1019011\t epsilon: 0.10\n",
      "episode: 153\t steps: 02439\t total steps:1021450\t epsilon: 0.10\n",
      "episode: 154\t steps: 04162\t total steps:1025612\t epsilon: 0.10\n",
      "episode: 155\t steps: 08602\t total steps:1034214\t epsilon: 0.10\n",
      "episode: 156\t steps: 02518\t total steps:1036732\t epsilon: 0.10\n",
      "episode: 157\t steps: 02334\t total steps:1039066\t epsilon: 0.10\n",
      "episode: 158\t steps: 04919\t total steps:1043985\t epsilon: 0.10\n",
      "episode: 159\t steps: 01095\t total steps:1045080\t epsilon: 0.10\n",
      "episode: 160\t steps: 02045\t total steps:1047125\t epsilon: 0.10\n",
      "episode: 161\t steps: 05470\t total steps:1052595\t epsilon: 0.10\n",
      "episode: 162\t steps: 01772\t total steps:1054367\t epsilon: 0.10\n",
      "episode: 163\t steps: 00227\t total steps:1054594\t epsilon: 0.10\n",
      "episode: 164\t steps: 06805\t total steps:1061399\t epsilon: 0.10\n",
      "episode: 165\t steps: 09079\t total steps:1070478\t epsilon: 0.10\n",
      "episode: 166\t steps: 02890\t total steps:1073368\t epsilon: 0.10\n",
      "episode: 167\t steps: 01398\t total steps:1074766\t epsilon: 0.10\n",
      "episode: 168\t steps: 00585\t total steps:1075351\t epsilon: 0.10\n",
      "episode: 169\t steps: 00583\t total steps:1075934\t epsilon: 0.10\n",
      "episode: 170\t steps: 01320\t total steps:1077254\t epsilon: 0.10\n",
      "episode: 171\t steps: 02706\t total steps:1079960\t epsilon: 0.10\n",
      "episode: 172\t steps: 01464\t total steps:1081424\t epsilon: 0.10\n",
      "episode: 173\t steps: 05547\t total steps:1086971\t epsilon: 0.10\n",
      "episode: 174\t steps: 03981\t total steps:1090952\t epsilon: 0.10\n",
      "episode: 175\t steps: 10000\t total steps:1100952\t epsilon: 0.10\n",
      "episode: 176\t steps: 00999\t total steps:1101951\t epsilon: 0.10\n",
      "episode: 177\t steps: 01273\t total steps:1103224\t epsilon: 0.10\n",
      "episode: 178\t steps: 10000\t total steps:1113224\t epsilon: 0.10\n",
      "episode: 179\t steps: 03132\t total steps:1116356\t epsilon: 0.10\n",
      "episode: 180\t steps: 05482\t total steps:1121838\t epsilon: 0.10\n",
      "episode: 181\t steps: 06983\t total steps:1128821\t epsilon: 0.10\n",
      "episode: 182\t steps: 02020\t total steps:1130841\t epsilon: 0.10\n",
      "episode: 183\t steps: 05205\t total steps:1136046\t epsilon: 0.10\n",
      "episode: 184\t steps: 10000\t total steps:1146046\t epsilon: 0.10\n",
      "episode: 185\t steps: 04335\t total steps:1150381\t epsilon: 0.10\n",
      "episode: 186\t steps: 01233\t total steps:1151614\t epsilon: 0.10\n",
      "episode: 187\t steps: 07349\t total steps:1158963\t epsilon: 0.10\n",
      "episode: 188\t steps: 04753\t total steps:1163716\t epsilon: 0.10\n",
      "episode: 189\t steps: 00731\t total steps:1164447\t epsilon: 0.10\n",
      "episode: 190\t steps: 10000\t total steps:1174447\t epsilon: 0.10\n",
      "episode: 191\t steps: 01280\t total steps:1175727\t epsilon: 0.10\n",
      "episode: 192\t steps: 06937\t total steps:1182664\t epsilon: 0.10\n",
      "episode: 193\t steps: 02111\t total steps:1184775\t epsilon: 0.10\n",
      "episode: 194\t steps: 02781\t total steps:1187556\t epsilon: 0.10\n",
      "episode: 195\t steps: 09897\t total steps:1197453\t epsilon: 0.10\n",
      "episode: 196\t steps: 02067\t total steps:1199520\t epsilon: 0.10\n",
      "episode: 197\t steps: 10000\t total steps:1209520\t epsilon: 0.10\n",
      "episode: 198\t steps: 01479\t total steps:1210999\t epsilon: 0.10\n",
      "episode: 199\t steps: 01928\t total steps:1212927\t epsilon: 0.10\n",
      "episode: 200\t steps: 01452\t total steps:1214379\t epsilon: 0.10\n",
      "Evaluation: 4\t average reward: -10000.0\n",
      "episode: 201\t steps: 00383\t total steps:1214762\t epsilon: 0.10\n",
      "episode: 202\t steps: 01730\t total steps:1216492\t epsilon: 0.10\n",
      "episode: 203\t steps: 06369\t total steps:1222861\t epsilon: 0.10\n",
      "episode: 204\t steps: 07534\t total steps:1230395\t epsilon: 0.10\n",
      "episode: 205\t steps: 01230\t total steps:1231625\t epsilon: 0.10\n",
      "episode: 206\t steps: 10000\t total steps:1241625\t epsilon: 0.10\n",
      "episode: 207\t steps: 01417\t total steps:1243042\t epsilon: 0.10\n",
      "episode: 208\t steps: 00542\t total steps:1243584\t epsilon: 0.10\n",
      "episode: 209\t steps: 06314\t total steps:1249898\t epsilon: 0.10\n",
      "episode: 210\t steps: 09116\t total steps:1259014\t epsilon: 0.10\n",
      "episode: 211\t steps: 02287\t total steps:1261301\t epsilon: 0.10\n",
      "episode: 212\t steps: 03155\t total steps:1264456\t epsilon: 0.10\n",
      "episode: 213\t steps: 02290\t total steps:1266746\t epsilon: 0.10\n",
      "episode: 214\t steps: 01002\t total steps:1267748\t epsilon: 0.10\n",
      "episode: 215\t steps: 09991\t total steps:1277739\t epsilon: 0.10\n",
      "episode: 216\t steps: 06355\t total steps:1284094\t epsilon: 0.10\n",
      "episode: 217\t steps: 02812\t total steps:1286906\t epsilon: 0.10\n",
      "episode: 218\t steps: 02089\t total steps:1288995\t epsilon: 0.10\n",
      "episode: 219\t steps: 01538\t total steps:1290533\t epsilon: 0.10\n",
      "episode: 220\t steps: 01697\t total steps:1292230\t epsilon: 0.10\n",
      "episode: 221\t steps: 02403\t total steps:1294633\t epsilon: 0.10\n",
      "episode: 222\t steps: 01882\t total steps:1296515\t epsilon: 0.10\n",
      "episode: 223\t steps: 02441\t total steps:1298956\t epsilon: 0.10\n",
      "episode: 224\t steps: 08476\t total steps:1307432\t epsilon: 0.10\n",
      "episode: 225\t steps: 09112\t total steps:1316544\t epsilon: 0.10\n",
      "episode: 226\t steps: 02354\t total steps:1318898\t epsilon: 0.10\n",
      "episode: 227\t steps: 02869\t total steps:1321767\t epsilon: 0.10\n",
      "episode: 228\t steps: 06085\t total steps:1327852\t epsilon: 0.10\n",
      "episode: 229\t steps: 01411\t total steps:1329263\t epsilon: 0.10\n",
      "episode: 230\t steps: 02980\t total steps:1332243\t epsilon: 0.10\n",
      "episode: 231\t steps: 01932\t total steps:1334175\t epsilon: 0.10\n",
      "episode: 232\t steps: 01424\t total steps:1335599\t epsilon: 0.10\n",
      "episode: 233\t steps: 01257\t total steps:1336856\t epsilon: 0.10\n",
      "episode: 234\t steps: 04402\t total steps:1341258\t epsilon: 0.10\n",
      "episode: 235\t steps: 00929\t total steps:1342187\t epsilon: 0.10\n",
      "episode: 236\t steps: 02151\t total steps:1344338\t epsilon: 0.10\n",
      "episode: 237\t steps: 06262\t total steps:1350600\t epsilon: 0.10\n",
      "episode: 238\t steps: 00734\t total steps:1351334\t epsilon: 0.10\n",
      "episode: 239\t steps: 04685\t total steps:1356019\t epsilon: 0.10\n",
      "episode: 240\t steps: 06166\t total steps:1362185\t epsilon: 0.10\n",
      "episode: 241\t steps: 04029\t total steps:1366214\t epsilon: 0.10\n",
      "episode: 242\t steps: 03027\t total steps:1369241\t epsilon: 0.10\n",
      "episode: 243\t steps: 04528\t total steps:1373769\t epsilon: 0.10\n",
      "episode: 244\t steps: 01177\t total steps:1374946\t epsilon: 0.10\n",
      "episode: 245\t steps: 02824\t total steps:1377770\t epsilon: 0.10\n",
      "episode: 246\t steps: 00615\t total steps:1378385\t epsilon: 0.10\n",
      "episode: 247\t steps: 05712\t total steps:1384097\t epsilon: 0.10\n",
      "episode: 248\t steps: 05416\t total steps:1389513\t epsilon: 0.10\n",
      "episode: 249\t steps: 03344\t total steps:1392857\t epsilon: 0.10\n",
      "episode: 250\t steps: 00627\t total steps:1393484\t epsilon: 0.10\n",
      "Evaluation: 5\t average reward: -10000.0\n",
      "episode: 251\t steps: 05267\t total steps:1398751\t epsilon: 0.10\n",
      "episode: 252\t steps: 01248\t total steps:1399999\t epsilon: 0.10\n",
      "episode: 253\t steps: 00677\t total steps:1400676\t epsilon: 0.10\n",
      "episode: 254\t steps: 05096\t total steps:1405772\t epsilon: 0.10\n",
      "episode: 255\t steps: 02917\t total steps:1408689\t epsilon: 0.10\n",
      "episode: 256\t steps: 02887\t total steps:1411576\t epsilon: 0.10\n",
      "episode: 257\t steps: 01567\t total steps:1413143\t epsilon: 0.10\n",
      "episode: 258\t steps: 10000\t total steps:1423143\t epsilon: 0.10\n",
      "episode: 259\t steps: 06208\t total steps:1429351\t epsilon: 0.10\n",
      "episode: 260\t steps: 03340\t total steps:1432691\t epsilon: 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 261\t steps: 01510\t total steps:1434201\t epsilon: 0.10\n",
      "episode: 262\t steps: 08929\t total steps:1443130\t epsilon: 0.10\n",
      "episode: 263\t steps: 10000\t total steps:1453130\t epsilon: 0.10\n",
      "episode: 264\t steps: 03848\t total steps:1456978\t epsilon: 0.10\n",
      "episode: 265\t steps: 01828\t total steps:1458806\t epsilon: 0.10\n",
      "episode: 266\t steps: 01673\t total steps:1460479\t epsilon: 0.10\n",
      "episode: 267\t steps: 01166\t total steps:1461645\t epsilon: 0.10\n",
      "episode: 268\t steps: 02522\t total steps:1464167\t epsilon: 0.10\n",
      "episode: 269\t steps: 05327\t total steps:1469494\t epsilon: 0.10\n",
      "episode: 270\t steps: 03743\t total steps:1473237\t epsilon: 0.10\n",
      "episode: 271\t steps: 04273\t total steps:1477510\t epsilon: 0.10\n",
      "episode: 272\t steps: 02544\t total steps:1480054\t epsilon: 0.10\n",
      "episode: 273\t steps: 02169\t total steps:1482223\t epsilon: 0.10\n",
      "episode: 274\t steps: 01739\t total steps:1483962\t epsilon: 0.10\n",
      "episode: 275\t steps: 00705\t total steps:1484667\t epsilon: 0.10\n",
      "episode: 276\t steps: 00746\t total steps:1485413\t epsilon: 0.10\n",
      "episode: 277\t steps: 01025\t total steps:1486438\t epsilon: 0.10\n",
      "episode: 278\t steps: 03869\t total steps:1490307\t epsilon: 0.10\n",
      "episode: 279\t steps: 02151\t total steps:1492458\t epsilon: 0.10\n",
      "episode: 280\t steps: 03998\t total steps:1496456\t epsilon: 0.10\n",
      "episode: 281\t steps: 01619\t total steps:1498075\t epsilon: 0.10\n",
      "episode: 282\t steps: 01604\t total steps:1499679\t epsilon: 0.10\n",
      "episode: 283\t steps: 07612\t total steps:1507291\t epsilon: 0.10\n",
      "episode: 284\t steps: 01574\t total steps:1508865\t epsilon: 0.10\n",
      "episode: 285\t steps: 01237\t total steps:1510102\t epsilon: 0.10\n",
      "episode: 286\t steps: 06310\t total steps:1516412\t epsilon: 0.10\n",
      "episode: 287\t steps: 01707\t total steps:1518119\t epsilon: 0.10\n",
      "episode: 288\t steps: 09161\t total steps:1527280\t epsilon: 0.10\n",
      "episode: 289\t steps: 04919\t total steps:1532199\t epsilon: 0.10\n",
      "episode: 290\t steps: 10000\t total steps:1542199\t epsilon: 0.10\n",
      "episode: 291\t steps: 02448\t total steps:1544647\t epsilon: 0.10\n",
      "episode: 292\t steps: 01455\t total steps:1546102\t epsilon: 0.10\n",
      "episode: 293\t steps: 01924\t total steps:1548026\t epsilon: 0.10\n",
      "episode: 294\t steps: 03713\t total steps:1551739\t epsilon: 0.10\n",
      "episode: 295\t steps: 01270\t total steps:1553009\t epsilon: 0.10\n",
      "episode: 296\t steps: 08600\t total steps:1561609\t epsilon: 0.10\n",
      "episode: 297\t steps: 01865\t total steps:1563474\t epsilon: 0.10\n",
      "episode: 298\t steps: 10000\t total steps:1573474\t epsilon: 0.10\n",
      "episode: 299\t steps: 04872\t total steps:1578346\t epsilon: 0.10\n",
      "episode: 300\t steps: 02701\t total steps:1581047\t epsilon: 0.10\n",
      "Evaluation: 6\t average reward: -7589.3\n",
      "episode: 301\t steps: 02760\t total steps:1583807\t epsilon: 0.10\n",
      "episode: 302\t steps: 03341\t total steps:1587148\t epsilon: 0.10\n",
      "episode: 303\t steps: 02062\t total steps:1589210\t epsilon: 0.10\n",
      "episode: 304\t steps: 01116\t total steps:1590326\t epsilon: 0.10\n",
      "episode: 305\t steps: 10000\t total steps:1600326\t epsilon: 0.10\n",
      "episode: 306\t steps: 03328\t total steps:1603654\t epsilon: 0.10\n",
      "episode: 307\t steps: 01336\t total steps:1604990\t epsilon: 0.10\n",
      "episode: 308\t steps: 03293\t total steps:1608283\t epsilon: 0.10\n",
      "episode: 309\t steps: 03466\t total steps:1611749\t epsilon: 0.10\n",
      "episode: 310\t steps: 01997\t total steps:1613746\t epsilon: 0.10\n",
      "episode: 311\t steps: 00859\t total steps:1614605\t epsilon: 0.10\n",
      "episode: 312\t steps: 10000\t total steps:1624605\t epsilon: 0.10\n",
      "episode: 313\t steps: 01202\t total steps:1625807\t epsilon: 0.10\n",
      "episode: 314\t steps: 10000\t total steps:1635807\t epsilon: 0.10\n",
      "episode: 315\t steps: 02979\t total steps:1638786\t epsilon: 0.10\n",
      "episode: 316\t steps: 06952\t total steps:1645738\t epsilon: 0.10\n",
      "episode: 317\t steps: 01317\t total steps:1647055\t epsilon: 0.10\n",
      "episode: 318\t steps: 02412\t total steps:1649467\t epsilon: 0.10\n",
      "episode: 319\t steps: 07247\t total steps:1656714\t epsilon: 0.10\n",
      "episode: 320\t steps: 01008\t total steps:1657722\t epsilon: 0.10\n",
      "episode: 321\t steps: 02649\t total steps:1660371\t epsilon: 0.10\n",
      "episode: 322\t steps: 09650\t total steps:1670021\t epsilon: 0.10\n",
      "episode: 323\t steps: 01263\t total steps:1671284\t epsilon: 0.10\n",
      "episode: 324\t steps: 04538\t total steps:1675822\t epsilon: 0.10\n",
      "episode: 325\t steps: 06271\t total steps:1682093\t epsilon: 0.10\n",
      "episode: 326\t steps: 01532\t total steps:1683625\t epsilon: 0.10\n",
      "episode: 327\t steps: 06060\t total steps:1689685\t epsilon: 0.10\n",
      "episode: 328\t steps: 02004\t total steps:1691689\t epsilon: 0.10\n",
      "episode: 329\t steps: 01745\t total steps:1693434\t epsilon: 0.10\n",
      "episode: 330\t steps: 04715\t total steps:1698149\t epsilon: 0.10\n",
      "episode: 331\t steps: 04426\t total steps:1702575\t epsilon: 0.10\n",
      "episode: 332\t steps: 02769\t total steps:1705344\t epsilon: 0.10\n",
      "episode: 333\t steps: 01673\t total steps:1707017\t epsilon: 0.10\n",
      "episode: 334\t steps: 03321\t total steps:1710338\t epsilon: 0.10\n",
      "episode: 335\t steps: 03195\t total steps:1713533\t epsilon: 0.10\n",
      "episode: 336\t steps: 00705\t total steps:1714238\t epsilon: 0.10\n",
      "episode: 337\t steps: 01171\t total steps:1715409\t epsilon: 0.10\n",
      "episode: 338\t steps: 02232\t total steps:1717641\t epsilon: 0.10\n",
      "episode: 339\t steps: 02213\t total steps:1719854\t epsilon: 0.10\n",
      "episode: 340\t steps: 00519\t total steps:1720373\t epsilon: 0.10\n",
      "episode: 341\t steps: 03083\t total steps:1723456\t epsilon: 0.10\n",
      "episode: 342\t steps: 00844\t total steps:1724300\t epsilon: 0.10\n",
      "episode: 343\t steps: 00896\t total steps:1725196\t epsilon: 0.10\n",
      "episode: 344\t steps: 03008\t total steps:1728204\t epsilon: 0.10\n",
      "episode: 345\t steps: 03099\t total steps:1731303\t epsilon: 0.10\n",
      "episode: 346\t steps: 03655\t total steps:1734958\t epsilon: 0.10\n",
      "episode: 347\t steps: 01602\t total steps:1736560\t epsilon: 0.10\n",
      "episode: 348\t steps: 00902\t total steps:1737462\t epsilon: 0.10\n",
      "episode: 349\t steps: 09677\t total steps:1747139\t epsilon: 0.10\n",
      "episode: 350\t steps: 03695\t total steps:1750834\t epsilon: 0.10\n",
      "Evaluation: 7\t average reward: -9462.2\n",
      "episode: 351\t steps: 02627\t total steps:1753461\t epsilon: 0.10\n",
      "episode: 352\t steps: 04598\t total steps:1758059\t epsilon: 0.10\n",
      "episode: 353\t steps: 01944\t total steps:1760003\t epsilon: 0.10\n",
      "episode: 354\t steps: 04604\t total steps:1764607\t epsilon: 0.10\n",
      "episode: 355\t steps: 03111\t total steps:1767718\t epsilon: 0.10\n",
      "episode: 356\t steps: 05150\t total steps:1772868\t epsilon: 0.10\n",
      "episode: 357\t steps: 05698\t total steps:1778566\t epsilon: 0.10\n",
      "episode: 358\t steps: 07520\t total steps:1786086\t epsilon: 0.10\n",
      "episode: 359\t steps: 02895\t total steps:1788981\t epsilon: 0.10\n",
      "episode: 360\t steps: 02463\t total steps:1791444\t epsilon: 0.10\n",
      "episode: 361\t steps: 00678\t total steps:1792122\t epsilon: 0.10\n",
      "episode: 362\t steps: 05571\t total steps:1797693\t epsilon: 0.10\n",
      "episode: 363\t steps: 01688\t total steps:1799381\t epsilon: 0.10\n",
      "episode: 364\t steps: 06312\t total steps:1805693\t epsilon: 0.10\n",
      "episode: 365\t steps: 02066\t total steps:1807759\t epsilon: 0.10\n",
      "episode: 366\t steps: 02907\t total steps:1810666\t epsilon: 0.10\n",
      "episode: 367\t steps: 00439\t total steps:1811105\t epsilon: 0.10\n",
      "episode: 368\t steps: 03293\t total steps:1814398\t epsilon: 0.10\n",
      "episode: 369\t steps: 03316\t total steps:1817714\t epsilon: 0.10\n",
      "episode: 370\t steps: 02148\t total steps:1819862\t epsilon: 0.10\n",
      "episode: 371\t steps: 02538\t total steps:1822400\t epsilon: 0.10\n",
      "episode: 372\t steps: 04478\t total steps:1826878\t epsilon: 0.10\n",
      "episode: 373\t steps: 04866\t total steps:1831744\t epsilon: 0.10\n",
      "episode: 374\t steps: 05596\t total steps:1837340\t epsilon: 0.10\n",
      "episode: 375\t steps: 05877\t total steps:1843217\t epsilon: 0.10\n",
      "episode: 376\t steps: 04947\t total steps:1848164\t epsilon: 0.10\n",
      "episode: 377\t steps: 01969\t total steps:1850133\t epsilon: 0.10\n",
      "episode: 378\t steps: 00788\t total steps:1850921\t epsilon: 0.10\n",
      "episode: 379\t steps: 05912\t total steps:1856833\t epsilon: 0.10\n",
      "episode: 380\t steps: 08469\t total steps:1865302\t epsilon: 0.10\n",
      "episode: 381\t steps: 07177\t total steps:1872479\t epsilon: 0.10\n",
      "episode: 382\t steps: 00747\t total steps:1873226\t epsilon: 0.10\n",
      "episode: 383\t steps: 01402\t total steps:1874628\t epsilon: 0.10\n",
      "episode: 384\t steps: 03208\t total steps:1877836\t epsilon: 0.10\n",
      "episode: 385\t steps: 10000\t total steps:1887836\t epsilon: 0.10\n",
      "episode: 386\t steps: 07252\t total steps:1895088\t epsilon: 0.10\n",
      "episode: 387\t steps: 06184\t total steps:1901272\t epsilon: 0.10\n",
      "episode: 388\t steps: 04465\t total steps:1905737\t epsilon: 0.10\n",
      "episode: 389\t steps: 03779\t total steps:1909516\t epsilon: 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 390\t steps: 03503\t total steps:1913019\t epsilon: 0.10\n",
      "episode: 391\t steps: 01282\t total steps:1914301\t epsilon: 0.10\n",
      "episode: 392\t steps: 03317\t total steps:1917618\t epsilon: 0.10\n",
      "episode: 393\t steps: 03039\t total steps:1920657\t epsilon: 0.10\n",
      "episode: 394\t steps: 02504\t total steps:1923161\t epsilon: 0.10\n",
      "episode: 395\t steps: 10000\t total steps:1933161\t epsilon: 0.10\n",
      "episode: 396\t steps: 07456\t total steps:1940617\t epsilon: 0.10\n",
      "episode: 397\t steps: 01463\t total steps:1942080\t epsilon: 0.10\n",
      "episode: 398\t steps: 10000\t total steps:1952080\t epsilon: 0.10\n",
      "episode: 399\t steps: 03089\t total steps:1955169\t epsilon: 0.10\n",
      "episode: 400\t steps: 00331\t total steps:1955500\t epsilon: 0.10\n",
      "Evaluation: 8\t average reward: -8658.9\n",
      "episode: 401\t steps: 03526\t total steps:1959026\t epsilon: 0.10\n",
      "episode: 402\t steps: 04098\t total steps:1963124\t epsilon: 0.10\n",
      "episode: 403\t steps: 09095\t total steps:1972219\t epsilon: 0.10\n",
      "episode: 404\t steps: 04900\t total steps:1977119\t epsilon: 0.10\n",
      "episode: 405\t steps: 01211\t total steps:1978330\t epsilon: 0.10\n",
      "episode: 406\t steps: 03004\t total steps:1981334\t epsilon: 0.10\n",
      "episode: 407\t steps: 01565\t total steps:1982899\t epsilon: 0.10\n",
      "episode: 408\t steps: 03866\t total steps:1986765\t epsilon: 0.10\n",
      "episode: 409\t steps: 04941\t total steps:1991706\t epsilon: 0.10\n",
      "episode: 410\t steps: 01579\t total steps:1993285\t epsilon: 0.10\n",
      "episode: 411\t steps: 04267\t total steps:1997552\t epsilon: 0.10\n",
      "episode: 412\t steps: 06378\t total steps:2003930\t epsilon: 0.10\n",
      "episode: 413\t steps: 07758\t total steps:2011688\t epsilon: 0.10\n",
      "episode: 414\t steps: 10000\t total steps:2021688\t epsilon: 0.10\n",
      "episode: 415\t steps: 03433\t total steps:2025121\t epsilon: 0.10\n",
      "episode: 416\t steps: 02359\t total steps:2027480\t epsilon: 0.10\n",
      "episode: 417\t steps: 02460\t total steps:2029940\t epsilon: 0.10\n",
      "episode: 418\t steps: 02711\t total steps:2032651\t epsilon: 0.10\n",
      "episode: 419\t steps: 04029\t total steps:2036680\t epsilon: 0.10\n",
      "episode: 420\t steps: 05860\t total steps:2042540\t epsilon: 0.10\n",
      "episode: 421\t steps: 08157\t total steps:2050697\t epsilon: 0.10\n",
      "episode: 422\t steps: 00733\t total steps:2051430\t epsilon: 0.10\n",
      "episode: 423\t steps: 02163\t total steps:2053593\t epsilon: 0.10\n",
      "episode: 424\t steps: 01184\t total steps:2054777\t epsilon: 0.10\n",
      "episode: 425\t steps: 08388\t total steps:2063165\t epsilon: 0.10\n",
      "episode: 426\t steps: 03237\t total steps:2066402\t epsilon: 0.10\n",
      "episode: 427\t steps: 10000\t total steps:2076402\t epsilon: 0.10\n",
      "episode: 428\t steps: 01655\t total steps:2078057\t epsilon: 0.10\n",
      "episode: 429\t steps: 01739\t total steps:2079796\t epsilon: 0.10\n",
      "episode: 430\t steps: 02220\t total steps:2082016\t epsilon: 0.10\n",
      "episode: 431\t steps: 01577\t total steps:2083593\t epsilon: 0.10\n",
      "episode: 432\t steps: 02436\t total steps:2086029\t epsilon: 0.10\n",
      "episode: 433\t steps: 02268\t total steps:2088297\t epsilon: 0.10\n",
      "episode: 434\t steps: 02779\t total steps:2091076\t epsilon: 0.10\n",
      "episode: 435\t steps: 01138\t total steps:2092214\t epsilon: 0.10\n",
      "episode: 436\t steps: 02443\t total steps:2094657\t epsilon: 0.10\n",
      "episode: 437\t steps: 03472\t total steps:2098129\t epsilon: 0.10\n",
      "episode: 438\t steps: 10000\t total steps:2108129\t epsilon: 0.10\n",
      "episode: 439\t steps: 04139\t total steps:2112268\t epsilon: 0.10\n",
      "episode: 440\t steps: 07501\t total steps:2119769\t epsilon: 0.10\n",
      "episode: 441\t steps: 01855\t total steps:2121624\t epsilon: 0.10\n",
      "episode: 442\t steps: 01339\t total steps:2122963\t epsilon: 0.10\n",
      "episode: 443\t steps: 04998\t total steps:2127961\t epsilon: 0.10\n",
      "episode: 444\t steps: 01773\t total steps:2129734\t epsilon: 0.10\n",
      "episode: 445\t steps: 01065\t total steps:2130799\t epsilon: 0.10\n",
      "episode: 446\t steps: 04165\t total steps:2134964\t epsilon: 0.10\n",
      "episode: 447\t steps: 03506\t total steps:2138470\t epsilon: 0.10\n",
      "episode: 448\t steps: 09613\t total steps:2148083\t epsilon: 0.10\n",
      "episode: 449\t steps: 04252\t total steps:2152335\t epsilon: 0.10\n",
      "episode: 450\t steps: 02652\t total steps:2154987\t epsilon: 0.10\n",
      "Evaluation: 9\t average reward: -8526.1\n",
      "episode: 451\t steps: 02686\t total steps:2157673\t epsilon: 0.10\n",
      "episode: 452\t steps: 01402\t total steps:2159075\t epsilon: 0.10\n",
      "episode: 453\t steps: 03578\t total steps:2162653\t epsilon: 0.10\n",
      "episode: 454\t steps: 07474\t total steps:2170127\t epsilon: 0.10\n",
      "episode: 455\t steps: 04237\t total steps:2174364\t epsilon: 0.10\n",
      "episode: 456\t steps: 04259\t total steps:2178623\t epsilon: 0.10\n",
      "episode: 457\t steps: 02740\t total steps:2181363\t epsilon: 0.10\n",
      "episode: 458\t steps: 02459\t total steps:2183822\t epsilon: 0.10\n",
      "episode: 459\t steps: 03349\t total steps:2187171\t epsilon: 0.10\n",
      "episode: 460\t steps: 05552\t total steps:2192723\t epsilon: 0.10\n",
      "episode: 461\t steps: 02305\t total steps:2195028\t epsilon: 0.10\n",
      "episode: 462\t steps: 02005\t total steps:2197033\t epsilon: 0.10\n",
      "episode: 463\t steps: 02736\t total steps:2199769\t epsilon: 0.10\n",
      "episode: 464\t steps: 00983\t total steps:2200752\t epsilon: 0.10\n",
      "episode: 465\t steps: 03804\t total steps:2204556\t epsilon: 0.10\n",
      "episode: 466\t steps: 03340\t total steps:2207896\t epsilon: 0.10\n",
      "episode: 467\t steps: 03490\t total steps:2211386\t epsilon: 0.10\n",
      "episode: 468\t steps: 03962\t total steps:2215348\t epsilon: 0.10\n",
      "episode: 469\t steps: 02557\t total steps:2217905\t epsilon: 0.10\n",
      "episode: 470\t steps: 01482\t total steps:2219387\t epsilon: 0.10\n",
      "episode: 471\t steps: 05671\t total steps:2225058\t epsilon: 0.10\n",
      "episode: 472\t steps: 06255\t total steps:2231313\t epsilon: 0.10\n",
      "episode: 473\t steps: 05207\t total steps:2236520\t epsilon: 0.10\n",
      "episode: 474\t steps: 03135\t total steps:2239655\t epsilon: 0.10\n",
      "episode: 475\t steps: 03935\t total steps:2243590\t epsilon: 0.10\n",
      "episode: 476\t steps: 01414\t total steps:2245004\t epsilon: 0.10\n",
      "episode: 477\t steps: 05381\t total steps:2250385\t epsilon: 0.10\n",
      "episode: 478\t steps: 01988\t total steps:2252373\t epsilon: 0.10\n",
      "episode: 479\t steps: 00493\t total steps:2252866\t epsilon: 0.10\n",
      "episode: 480\t steps: 01823\t total steps:2254689\t epsilon: 0.10\n",
      "episode: 481\t steps: 02388\t total steps:2257077\t epsilon: 0.10\n",
      "episode: 482\t steps: 02511\t total steps:2259588\t epsilon: 0.10\n",
      "episode: 483\t steps: 02703\t total steps:2262291\t epsilon: 0.10\n",
      "episode: 484\t steps: 03153\t total steps:2265444\t epsilon: 0.10\n",
      "episode: 485\t steps: 04357\t total steps:2269801\t epsilon: 0.10\n",
      "episode: 486\t steps: 05921\t total steps:2275722\t epsilon: 0.10\n",
      "episode: 487\t steps: 07120\t total steps:2282842\t epsilon: 0.10\n",
      "episode: 488\t steps: 02872\t total steps:2285714\t epsilon: 0.10\n",
      "episode: 489\t steps: 03307\t total steps:2289021\t epsilon: 0.10\n",
      "episode: 490\t steps: 04297\t total steps:2293318\t epsilon: 0.10\n",
      "episode: 491\t steps: 10000\t total steps:2303318\t epsilon: 0.10\n",
      "episode: 492\t steps: 01015\t total steps:2304333\t epsilon: 0.10\n",
      "episode: 493\t steps: 00844\t total steps:2305177\t epsilon: 0.10\n",
      "episode: 494\t steps: 10000\t total steps:2315177\t epsilon: 0.10\n",
      "episode: 495\t steps: 10000\t total steps:2325177\t epsilon: 0.10\n",
      "episode: 496\t steps: 07672\t total steps:2332849\t epsilon: 0.10\n",
      "episode: 497\t steps: 00467\t total steps:2333316\t epsilon: 0.10\n",
      "episode: 498\t steps: 04878\t total steps:2338194\t epsilon: 0.10\n",
      "episode: 499\t steps: 10000\t total steps:2348194\t epsilon: 0.10\n",
      "episode: 500\t steps: 03404\t total steps:2351598\t epsilon: 0.10\n",
      "Evaluation: 10\t average reward: -8715.9\n",
      "episode: 501\t steps: 00903\t total steps:2352501\t epsilon: 0.10\n",
      "episode: 502\t steps: 05207\t total steps:2357708\t epsilon: 0.10\n",
      "episode: 503\t steps: 03113\t total steps:2360821\t epsilon: 0.10\n",
      "episode: 504\t steps: 00542\t total steps:2361363\t epsilon: 0.10\n",
      "episode: 505\t steps: 07698\t total steps:2369061\t epsilon: 0.10\n",
      "episode: 506\t steps: 04538\t total steps:2373599\t epsilon: 0.10\n",
      "episode: 507\t steps: 01065\t total steps:2374664\t epsilon: 0.10\n",
      "episode: 508\t steps: 01504\t total steps:2376168\t epsilon: 0.10\n",
      "episode: 509\t steps: 10000\t total steps:2386168\t epsilon: 0.10\n",
      "episode: 510\t steps: 01668\t total steps:2387836\t epsilon: 0.10\n",
      "episode: 511\t steps: 06314\t total steps:2394150\t epsilon: 0.10\n",
      "episode: 512\t steps: 06731\t total steps:2400881\t epsilon: 0.10\n",
      "episode: 513\t steps: 00574\t total steps:2401455\t epsilon: 0.10\n",
      "episode: 514\t steps: 01676\t total steps:2403131\t epsilon: 0.10\n",
      "episode: 515\t steps: 08590\t total steps:2411721\t epsilon: 0.10\n",
      "episode: 516\t steps: 07588\t total steps:2419309\t epsilon: 0.10\n",
      "episode: 517\t steps: 02629\t total steps:2421938\t epsilon: 0.10\n",
      "episode: 518\t steps: 01771\t total steps:2423709\t epsilon: 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 519\t steps: 00562\t total steps:2424271\t epsilon: 0.10\n",
      "episode: 520\t steps: 05605\t total steps:2429876\t epsilon: 0.10\n",
      "episode: 521\t steps: 01423\t total steps:2431299\t epsilon: 0.10\n",
      "episode: 522\t steps: 02414\t total steps:2433713\t epsilon: 0.10\n",
      "episode: 523\t steps: 02121\t total steps:2435834\t epsilon: 0.10\n",
      "episode: 524\t steps: 02273\t total steps:2438107\t epsilon: 0.10\n",
      "episode: 525\t steps: 01230\t total steps:2439337\t epsilon: 0.10\n",
      "episode: 526\t steps: 04476\t total steps:2443813\t epsilon: 0.10\n",
      "episode: 527\t steps: 01052\t total steps:2444865\t epsilon: 0.10\n",
      "episode: 528\t steps: 02570\t total steps:2447435\t epsilon: 0.10\n",
      "episode: 529\t steps: 01743\t total steps:2449178\t epsilon: 0.10\n",
      "episode: 530\t steps: 01843\t total steps:2451021\t epsilon: 0.10\n",
      "episode: 531\t steps: 00630\t total steps:2451651\t epsilon: 0.10\n",
      "episode: 532\t steps: 01053\t total steps:2452704\t epsilon: 0.10\n",
      "episode: 533\t steps: 02034\t total steps:2454738\t epsilon: 0.10\n",
      "episode: 534\t steps: 01493\t total steps:2456231\t epsilon: 0.10\n",
      "episode: 535\t steps: 01595\t total steps:2457826\t epsilon: 0.10\n",
      "episode: 536\t steps: 05248\t total steps:2463074\t epsilon: 0.10\n",
      "episode: 537\t steps: 00727\t total steps:2463801\t epsilon: 0.10\n",
      "episode: 538\t steps: 05514\t total steps:2469315\t epsilon: 0.10\n",
      "episode: 539\t steps: 04314\t total steps:2473629\t epsilon: 0.10\n",
      "episode: 540\t steps: 05473\t total steps:2479102\t epsilon: 0.10\n",
      "episode: 541\t steps: 03069\t total steps:2482171\t epsilon: 0.10\n",
      "episode: 542\t steps: 01277\t total steps:2483448\t epsilon: 0.10\n",
      "episode: 543\t steps: 02293\t total steps:2485741\t epsilon: 0.10\n",
      "episode: 544\t steps: 02128\t total steps:2487869\t epsilon: 0.10\n",
      "episode: 545\t steps: 10000\t total steps:2497869\t epsilon: 0.10\n",
      "episode: 546\t steps: 01272\t total steps:2499141\t epsilon: 0.10\n",
      "episode: 547\t steps: 01953\t total steps:2501094\t epsilon: 0.10\n",
      "episode: 548\t steps: 06763\t total steps:2507857\t epsilon: 0.10\n",
      "episode: 549\t steps: 02821\t total steps:2510678\t epsilon: 0.10\n",
      "episode: 550\t steps: 02545\t total steps:2513223\t epsilon: 0.10\n",
      "Evaluation: 11\t average reward: -10000.0\n",
      "episode: 551\t steps: 06846\t total steps:2520069\t epsilon: 0.10\n",
      "episode: 552\t steps: 04976\t total steps:2525045\t epsilon: 0.10\n",
      "episode: 553\t steps: 01801\t total steps:2526846\t epsilon: 0.10\n",
      "episode: 554\t steps: 05874\t total steps:2532720\t epsilon: 0.10\n",
      "episode: 555\t steps: 00780\t total steps:2533500\t epsilon: 0.10\n",
      "episode: 556\t steps: 00838\t total steps:2534338\t epsilon: 0.10\n",
      "episode: 557\t steps: 02370\t total steps:2536708\t epsilon: 0.10\n",
      "episode: 558\t steps: 04523\t total steps:2541231\t epsilon: 0.10\n",
      "episode: 559\t steps: 02005\t total steps:2543236\t epsilon: 0.10\n",
      "episode: 560\t steps: 04618\t total steps:2547854\t epsilon: 0.10\n",
      "episode: 561\t steps: 02807\t total steps:2550661\t epsilon: 0.10\n",
      "episode: 562\t steps: 06388\t total steps:2557049\t epsilon: 0.10\n",
      "episode: 563\t steps: 04363\t total steps:2561412\t epsilon: 0.10\n",
      "episode: 564\t steps: 03669\t total steps:2565081\t epsilon: 0.10\n",
      "episode: 565\t steps: 06571\t total steps:2571652\t epsilon: 0.10\n",
      "episode: 566\t steps: 10000\t total steps:2581652\t epsilon: 0.10\n",
      "episode: 567\t steps: 02725\t total steps:2584377\t epsilon: 0.10\n",
      "episode: 568\t steps: 03344\t total steps:2587721\t epsilon: 0.10\n",
      "episode: 569\t steps: 07636\t total steps:2595357\t epsilon: 0.10\n",
      "episode: 570\t steps: 05140\t total steps:2600497\t epsilon: 0.10\n",
      "episode: 571\t steps: 05713\t total steps:2606210\t epsilon: 0.10\n",
      "episode: 572\t steps: 04902\t total steps:2611112\t epsilon: 0.10\n",
      "episode: 573\t steps: 01764\t total steps:2612876\t epsilon: 0.10\n",
      "episode: 574\t steps: 04627\t total steps:2617503\t epsilon: 0.10\n",
      "episode: 575\t steps: 06233\t total steps:2623736\t epsilon: 0.10\n",
      "episode: 576\t steps: 01548\t total steps:2625284\t epsilon: 0.10\n",
      "episode: 577\t steps: 00747\t total steps:2626031\t epsilon: 0.10\n",
      "episode: 578\t steps: 01917\t total steps:2627948\t epsilon: 0.10\n",
      "episode: 579\t steps: 10000\t total steps:2637948\t epsilon: 0.10\n",
      "episode: 580\t steps: 06418\t total steps:2644366\t epsilon: 0.10\n",
      "episode: 581\t steps: 03737\t total steps:2648103\t epsilon: 0.10\n",
      "episode: 582\t steps: 10000\t total steps:2658103\t epsilon: 0.10\n",
      "episode: 583\t steps: 01267\t total steps:2659370\t epsilon: 0.10\n",
      "episode: 584\t steps: 10000\t total steps:2669370\t epsilon: 0.10\n",
      "episode: 585\t steps: 05368\t total steps:2674738\t epsilon: 0.10\n",
      "episode: 586\t steps: 01428\t total steps:2676166\t epsilon: 0.10\n",
      "episode: 587\t steps: 03089\t total steps:2679255\t epsilon: 0.10\n",
      "episode: 588\t steps: 00831\t total steps:2680086\t epsilon: 0.10\n",
      "episode: 589\t steps: 03416\t total steps:2683502\t epsilon: 0.10\n",
      "episode: 590\t steps: 05934\t total steps:2689436\t epsilon: 0.10\n",
      "episode: 591\t steps: 01485\t total steps:2690921\t epsilon: 0.10\n",
      "episode: 592\t steps: 02210\t total steps:2693131\t epsilon: 0.10\n",
      "episode: 593\t steps: 02005\t total steps:2695136\t epsilon: 0.10\n",
      "episode: 594\t steps: 02434\t total steps:2697570\t epsilon: 0.10\n",
      "episode: 595\t steps: 03225\t total steps:2700795\t epsilon: 0.10\n",
      "episode: 596\t steps: 10000\t total steps:2710795\t epsilon: 0.10\n",
      "episode: 597\t steps: 05356\t total steps:2716151\t epsilon: 0.10\n",
      "episode: 598\t steps: 01009\t total steps:2717160\t epsilon: 0.10\n",
      "episode: 599\t steps: 01498\t total steps:2718658\t epsilon: 0.10\n",
      "episode: 600\t steps: 02424\t total steps:2721082\t epsilon: 0.10\n",
      "Evaluation: 12\t average reward: -8699.4\n",
      "episode: 601\t steps: 08218\t total steps:2729300\t epsilon: 0.10\n",
      "episode: 602\t steps: 00513\t total steps:2729813\t epsilon: 0.10\n",
      "episode: 603\t steps: 01026\t total steps:2730839\t epsilon: 0.10\n",
      "episode: 604\t steps: 01906\t total steps:2732745\t epsilon: 0.10\n",
      "episode: 605\t steps: 02867\t total steps:2735612\t epsilon: 0.10\n",
      "episode: 606\t steps: 04770\t total steps:2740382\t epsilon: 0.10\n",
      "episode: 607\t steps: 01442\t total steps:2741824\t epsilon: 0.10\n",
      "episode: 608\t steps: 07706\t total steps:2749530\t epsilon: 0.10\n",
      "episode: 609\t steps: 04889\t total steps:2754419\t epsilon: 0.10\n",
      "episode: 610\t steps: 01267\t total steps:2755686\t epsilon: 0.10\n",
      "episode: 611\t steps: 00802\t total steps:2756488\t epsilon: 0.10\n",
      "episode: 612\t steps: 03396\t total steps:2759884\t epsilon: 0.10\n",
      "episode: 613\t steps: 07495\t total steps:2767379\t epsilon: 0.10\n",
      "episode: 614\t steps: 02536\t total steps:2769915\t epsilon: 0.10\n",
      "episode: 615\t steps: 01675\t total steps:2771590\t epsilon: 0.10\n",
      "episode: 616\t steps: 02837\t total steps:2774427\t epsilon: 0.10\n",
      "episode: 617\t steps: 01525\t total steps:2775952\t epsilon: 0.10\n",
      "episode: 618\t steps: 07806\t total steps:2783758\t epsilon: 0.10\n",
      "episode: 619\t steps: 05259\t total steps:2789017\t epsilon: 0.10\n",
      "episode: 620\t steps: 07532\t total steps:2796549\t epsilon: 0.10\n",
      "episode: 621\t steps: 04461\t total steps:2801010\t epsilon: 0.10\n",
      "episode: 622\t steps: 04293\t total steps:2805303\t epsilon: 0.10\n",
      "episode: 623\t steps: 03870\t total steps:2809173\t epsilon: 0.10\n",
      "episode: 624\t steps: 02706\t total steps:2811879\t epsilon: 0.10\n",
      "episode: 625\t steps: 10000\t total steps:2821879\t epsilon: 0.10\n",
      "episode: 626\t steps: 03580\t total steps:2825459\t epsilon: 0.10\n",
      "episode: 627\t steps: 06686\t total steps:2832145\t epsilon: 0.10\n",
      "episode: 628\t steps: 04553\t total steps:2836698\t epsilon: 0.10\n",
      "episode: 629\t steps: 00891\t total steps:2837589\t epsilon: 0.10\n",
      "episode: 630\t steps: 03934\t total steps:2841523\t epsilon: 0.10\n",
      "episode: 631\t steps: 01555\t total steps:2843078\t epsilon: 0.10\n",
      "episode: 632\t steps: 08444\t total steps:2851522\t epsilon: 0.10\n",
      "episode: 633\t steps: 01585\t total steps:2853107\t epsilon: 0.10\n",
      "episode: 634\t steps: 05054\t total steps:2858161\t epsilon: 0.10\n",
      "episode: 635\t steps: 01955\t total steps:2860116\t epsilon: 0.10\n",
      "episode: 636\t steps: 01097\t total steps:2861213\t epsilon: 0.10\n",
      "episode: 637\t steps: 01236\t total steps:2862449\t epsilon: 0.10\n",
      "episode: 638\t steps: 06817\t total steps:2869266\t epsilon: 0.10\n",
      "episode: 639\t steps: 01179\t total steps:2870445\t epsilon: 0.10\n",
      "episode: 640\t steps: 07361\t total steps:2877806\t epsilon: 0.10\n",
      "episode: 641\t steps: 03272\t total steps:2881078\t epsilon: 0.10\n",
      "episode: 642\t steps: 02682\t total steps:2883760\t epsilon: 0.10\n",
      "episode: 643\t steps: 09673\t total steps:2893433\t epsilon: 0.10\n",
      "episode: 644\t steps: 04947\t total steps:2898380\t epsilon: 0.10\n",
      "episode: 645\t steps: 01998\t total steps:2900378\t epsilon: 0.10\n",
      "episode: 646\t steps: 01939\t total steps:2902317\t epsilon: 0.10\n",
      "episode: 647\t steps: 02599\t total steps:2904916\t epsilon: 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 648\t steps: 01618\t total steps:2906534\t epsilon: 0.10\n",
      "episode: 649\t steps: 01753\t total steps:2908287\t epsilon: 0.10\n",
      "episode: 650\t steps: 02714\t total steps:2911001\t epsilon: 0.10\n",
      "Evaluation: 13\t average reward: -10000.0\n",
      "episode: 651\t steps: 01315\t total steps:2912316\t epsilon: 0.10\n",
      "episode: 652\t steps: 02319\t total steps:2914635\t epsilon: 0.10\n",
      "episode: 653\t steps: 07026\t total steps:2921661\t epsilon: 0.10\n",
      "episode: 654\t steps: 01020\t total steps:2922681\t epsilon: 0.10\n",
      "episode: 655\t steps: 01240\t total steps:2923921\t epsilon: 0.10\n",
      "episode: 656\t steps: 04483\t total steps:2928404\t epsilon: 0.10\n",
      "episode: 657\t steps: 01736\t total steps:2930140\t epsilon: 0.10\n",
      "episode: 658\t steps: 02838\t total steps:2932978\t epsilon: 0.10\n",
      "episode: 659\t steps: 00938\t total steps:2933916\t epsilon: 0.10\n",
      "episode: 660\t steps: 04243\t total steps:2938159\t epsilon: 0.10\n",
      "episode: 661\t steps: 05974\t total steps:2944133\t epsilon: 0.10\n",
      "episode: 662\t steps: 01243\t total steps:2945376\t epsilon: 0.10\n",
      "episode: 663\t steps: 10000\t total steps:2955376\t epsilon: 0.10\n",
      "episode: 664\t steps: 02967\t total steps:2958343\t epsilon: 0.10\n",
      "episode: 665\t steps: 10000\t total steps:2968343\t epsilon: 0.10\n",
      "episode: 666\t steps: 02547\t total steps:2970890\t epsilon: 0.10\n",
      "episode: 667\t steps: 01022\t total steps:2971912\t epsilon: 0.10\n",
      "episode: 668\t steps: 00719\t total steps:2972631\t epsilon: 0.10\n",
      "episode: 669\t steps: 02402\t total steps:2975033\t epsilon: 0.10\n",
      "episode: 670\t steps: 05047\t total steps:2980080\t epsilon: 0.10\n",
      "episode: 671\t steps: 10000\t total steps:2990080\t epsilon: 0.10\n",
      "episode: 672\t steps: 10000\t total steps:3000080\t epsilon: 0.10\n",
      "episode: 673\t steps: 10000\t total steps:3010080\t epsilon: 0.10\n",
      "episode: 674\t steps: 05015\t total steps:3015095\t epsilon: 0.10\n",
      "episode: 675\t steps: 01495\t total steps:3016590\t epsilon: 0.10\n",
      "episode: 676\t steps: 05810\t total steps:3022400\t epsilon: 0.10\n",
      "episode: 677\t steps: 03830\t total steps:3026230\t epsilon: 0.10\n",
      "episode: 678\t steps: 05686\t total steps:3031916\t epsilon: 0.10\n",
      "episode: 679\t steps: 03514\t total steps:3035430\t epsilon: 0.10\n",
      "episode: 680\t steps: 06942\t total steps:3042372\t epsilon: 0.10\n",
      "episode: 681\t steps: 06222\t total steps:3048594\t epsilon: 0.10\n",
      "episode: 682\t steps: 00844\t total steps:3049438\t epsilon: 0.10\n",
      "episode: 683\t steps: 07282\t total steps:3056720\t epsilon: 0.10\n",
      "episode: 684\t steps: 06546\t total steps:3063266\t epsilon: 0.10\n",
      "episode: 685\t steps: 06231\t total steps:3069497\t epsilon: 0.10\n",
      "episode: 686\t steps: 10000\t total steps:3079497\t epsilon: 0.10\n",
      "episode: 687\t steps: 05724\t total steps:3085221\t epsilon: 0.10\n",
      "episode: 688\t steps: 01782\t total steps:3087003\t epsilon: 0.10\n",
      "episode: 689\t steps: 04108\t total steps:3091111\t epsilon: 0.10\n",
      "episode: 690\t steps: 01207\t total steps:3092318\t epsilon: 0.10\n",
      "episode: 691\t steps: 03920\t total steps:3096238\t epsilon: 0.10\n",
      "episode: 692\t steps: 02031\t total steps:3098269\t epsilon: 0.10\n",
      "episode: 693\t steps: 01031\t total steps:3099300\t epsilon: 0.10\n",
      "episode: 694\t steps: 02957\t total steps:3102257\t epsilon: 0.10\n",
      "episode: 695\t steps: 01104\t total steps:3103361\t epsilon: 0.10\n",
      "episode: 696\t steps: 10000\t total steps:3113361\t epsilon: 0.10\n",
      "episode: 697\t steps: 07934\t total steps:3121295\t epsilon: 0.10\n",
      "episode: 698\t steps: 01767\t total steps:3123062\t epsilon: 0.10\n",
      "episode: 699\t steps: 10000\t total steps:3133062\t epsilon: 0.10\n",
      "episode: 700\t steps: 08893\t total steps:3141955\t epsilon: 0.10\n",
      "Evaluation: 14\t average reward: -10000.0\n",
      "episode: 701\t steps: 04506\t total steps:3146461\t epsilon: 0.10\n",
      "episode: 702\t steps: 01754\t total steps:3148215\t epsilon: 0.10\n",
      "episode: 703\t steps: 03309\t total steps:3151524\t epsilon: 0.10\n",
      "episode: 704\t steps: 00496\t total steps:3152020\t epsilon: 0.10\n",
      "episode: 705\t steps: 01280\t total steps:3153300\t epsilon: 0.10\n",
      "episode: 706\t steps: 10000\t total steps:3163300\t epsilon: 0.10\n",
      "episode: 707\t steps: 02197\t total steps:3165497\t epsilon: 0.10\n",
      "episode: 708\t steps: 00816\t total steps:3166313\t epsilon: 0.10\n",
      "episode: 709\t steps: 05456\t total steps:3171769\t epsilon: 0.10\n",
      "episode: 710\t steps: 09889\t total steps:3181658\t epsilon: 0.10\n",
      "episode: 711\t steps: 08316\t total steps:3189974\t epsilon: 0.10\n",
      "episode: 712\t steps: 01033\t total steps:3191007\t epsilon: 0.10\n",
      "episode: 713\t steps: 05637\t total steps:3196644\t epsilon: 0.10\n",
      "episode: 714\t steps: 04043\t total steps:3200687\t epsilon: 0.10\n",
      "episode: 715\t steps: 02571\t total steps:3203258\t epsilon: 0.10\n",
      "episode: 716\t steps: 00522\t total steps:3203780\t epsilon: 0.10\n",
      "episode: 717\t steps: 01678\t total steps:3205458\t epsilon: 0.10\n",
      "episode: 718\t steps: 02410\t total steps:3207868\t epsilon: 0.10\n",
      "episode: 719\t steps: 03443\t total steps:3211311\t epsilon: 0.10\n",
      "episode: 720\t steps: 03073\t total steps:3214384\t epsilon: 0.10\n",
      "episode: 721\t steps: 02406\t total steps:3216790\t epsilon: 0.10\n",
      "episode: 722\t steps: 03302\t total steps:3220092\t epsilon: 0.10\n",
      "episode: 723\t steps: 07029\t total steps:3227121\t epsilon: 0.10\n",
      "episode: 724\t steps: 02709\t total steps:3229830\t epsilon: 0.10\n",
      "episode: 725\t steps: 05628\t total steps:3235458\t epsilon: 0.10\n",
      "episode: 726\t steps: 06480\t total steps:3241938\t epsilon: 0.10\n",
      "episode: 727\t steps: 05900\t total steps:3247838\t epsilon: 0.10\n",
      "episode: 728\t steps: 02084\t total steps:3249922\t epsilon: 0.10\n",
      "episode: 729\t steps: 01590\t total steps:3251512\t epsilon: 0.10\n",
      "episode: 730\t steps: 03863\t total steps:3255375\t epsilon: 0.10\n",
      "episode: 731\t steps: 02111\t total steps:3257486\t epsilon: 0.10\n",
      "episode: 732\t steps: 03502\t total steps:3260988\t epsilon: 0.10\n",
      "episode: 733\t steps: 02057\t total steps:3263045\t epsilon: 0.10\n",
      "episode: 734\t steps: 01958\t total steps:3265003\t epsilon: 0.10\n",
      "episode: 735\t steps: 04844\t total steps:3269847\t epsilon: 0.10\n",
      "episode: 736\t steps: 04869\t total steps:3274716\t epsilon: 0.10\n",
      "episode: 737\t steps: 08060\t total steps:3282776\t epsilon: 0.10\n",
      "episode: 738\t steps: 02583\t total steps:3285359\t epsilon: 0.10\n",
      "episode: 739\t steps: 02758\t total steps:3288117\t epsilon: 0.10\n",
      "episode: 740\t steps: 03054\t total steps:3291171\t epsilon: 0.10\n",
      "episode: 741\t steps: 10000\t total steps:3301171\t epsilon: 0.10\n",
      "episode: 742\t steps: 02156\t total steps:3303327\t epsilon: 0.10\n",
      "episode: 743\t steps: 01420\t total steps:3304747\t epsilon: 0.10\n",
      "episode: 744\t steps: 01088\t total steps:3305835\t epsilon: 0.10\n",
      "episode: 745\t steps: 01511\t total steps:3307346\t epsilon: 0.10\n",
      "episode: 746\t steps: 06594\t total steps:3313940\t epsilon: 0.10\n",
      "episode: 747\t steps: 02383\t total steps:3316323\t epsilon: 0.10\n",
      "episode: 748\t steps: 02712\t total steps:3319035\t epsilon: 0.10\n",
      "episode: 749\t steps: 00516\t total steps:3319551\t epsilon: 0.10\n",
      "episode: 750\t steps: 01555\t total steps:3321106\t epsilon: 0.10\n",
      "Evaluation: 15\t average reward: -10000.0\n",
      "episode: 751\t steps: 00828\t total steps:3321934\t epsilon: 0.10\n",
      "episode: 752\t steps: 00948\t total steps:3322882\t epsilon: 0.10\n",
      "episode: 753\t steps: 00961\t total steps:3323843\t epsilon: 0.10\n",
      "episode: 754\t steps: 03414\t total steps:3327257\t epsilon: 0.10\n",
      "episode: 755\t steps: 01596\t total steps:3328853\t epsilon: 0.10\n",
      "episode: 756\t steps: 02488\t total steps:3331341\t epsilon: 0.10\n",
      "episode: 757\t steps: 01728\t total steps:3333069\t epsilon: 0.10\n",
      "episode: 758\t steps: 06551\t total steps:3339620\t epsilon: 0.10\n",
      "episode: 759\t steps: 06138\t total steps:3345758\t epsilon: 0.10\n",
      "episode: 760\t steps: 01282\t total steps:3347040\t epsilon: 0.10\n",
      "episode: 761\t steps: 08417\t total steps:3355457\t epsilon: 0.10\n",
      "episode: 762\t steps: 01568\t total steps:3357025\t epsilon: 0.10\n",
      "episode: 763\t steps: 01497\t total steps:3358522\t epsilon: 0.10\n",
      "episode: 764\t steps: 00894\t total steps:3359416\t epsilon: 0.10\n",
      "episode: 765\t steps: 03754\t total steps:3363170\t epsilon: 0.10\n",
      "episode: 766\t steps: 01211\t total steps:3364381\t epsilon: 0.10\n",
      "episode: 767\t steps: 00638\t total steps:3365019\t epsilon: 0.10\n",
      "episode: 768\t steps: 01203\t total steps:3366222\t epsilon: 0.10\n",
      "episode: 769\t steps: 01355\t total steps:3367577\t epsilon: 0.10\n",
      "episode: 770\t steps: 01602\t total steps:3369179\t epsilon: 0.10\n",
      "episode: 771\t steps: 00977\t total steps:3370156\t epsilon: 0.10\n",
      "episode: 772\t steps: 05327\t total steps:3375483\t epsilon: 0.10\n",
      "episode: 773\t steps: 04928\t total steps:3380411\t epsilon: 0.10\n",
      "episode: 774\t steps: 04111\t total steps:3384522\t epsilon: 0.10\n",
      "episode: 775\t steps: 01951\t total steps:3386473\t epsilon: 0.10\n",
      "episode: 776\t steps: 09299\t total steps:3395772\t epsilon: 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 777\t steps: 01584\t total steps:3397356\t epsilon: 0.10\n",
      "episode: 778\t steps: 07283\t total steps:3404639\t epsilon: 0.10\n",
      "episode: 779\t steps: 01308\t total steps:3405947\t epsilon: 0.10\n",
      "episode: 780\t steps: 01134\t total steps:3407081\t epsilon: 0.10\n",
      "episode: 781\t steps: 03068\t total steps:3410149\t epsilon: 0.10\n",
      "episode: 782\t steps: 02044\t total steps:3412193\t epsilon: 0.10\n",
      "episode: 783\t steps: 00922\t total steps:3413115\t epsilon: 0.10\n",
      "episode: 784\t steps: 03131\t total steps:3416246\t epsilon: 0.10\n",
      "episode: 785\t steps: 10000\t total steps:3426246\t epsilon: 0.10\n",
      "episode: 786\t steps: 01539\t total steps:3427785\t epsilon: 0.10\n",
      "episode: 787\t steps: 02546\t total steps:3430331\t epsilon: 0.10\n",
      "episode: 788\t steps: 05730\t total steps:3436061\t epsilon: 0.10\n",
      "episode: 789\t steps: 09037\t total steps:3445098\t epsilon: 0.10\n",
      "episode: 790\t steps: 03773\t total steps:3448871\t epsilon: 0.10\n",
      "episode: 791\t steps: 00484\t total steps:3449355\t epsilon: 0.10\n",
      "episode: 792\t steps: 08108\t total steps:3457463\t epsilon: 0.10\n",
      "episode: 793\t steps: 05043\t total steps:3462506\t epsilon: 0.10\n",
      "episode: 794\t steps: 09279\t total steps:3471785\t epsilon: 0.10\n",
      "episode: 795\t steps: 06365\t total steps:3478150\t epsilon: 0.10\n",
      "episode: 796\t steps: 07189\t total steps:3485339\t epsilon: 0.10\n",
      "episode: 797\t steps: 01338\t total steps:3486677\t epsilon: 0.10\n",
      "episode: 798\t steps: 02378\t total steps:3489055\t epsilon: 0.10\n",
      "episode: 799\t steps: 03094\t total steps:3492149\t epsilon: 0.10\n",
      "episode: 800\t steps: 07618\t total steps:3499767\t epsilon: 0.10\n",
      "Evaluation: 16\t average reward: -9595.8\n",
      "episode: 801\t steps: 07254\t total steps:3507021\t epsilon: 0.10\n",
      "episode: 802\t steps: 02941\t total steps:3509962\t epsilon: 0.10\n",
      "episode: 803\t steps: 02253\t total steps:3512215\t epsilon: 0.10\n",
      "episode: 804\t steps: 00884\t total steps:3513099\t epsilon: 0.10\n",
      "episode: 805\t steps: 10000\t total steps:3523099\t epsilon: 0.10\n",
      "episode: 806\t steps: 02940\t total steps:3526039\t epsilon: 0.10\n",
      "episode: 807\t steps: 01996\t total steps:3528035\t epsilon: 0.10\n",
      "episode: 808\t steps: 04165\t total steps:3532200\t epsilon: 0.10\n",
      "episode: 809\t steps: 01970\t total steps:3534170\t epsilon: 0.10\n",
      "episode: 810\t steps: 05606\t total steps:3539776\t epsilon: 0.10\n",
      "episode: 811\t steps: 08295\t total steps:3548071\t epsilon: 0.10\n",
      "episode: 812\t steps: 04155\t total steps:3552226\t epsilon: 0.10\n",
      "episode: 813\t steps: 08959\t total steps:3561185\t epsilon: 0.10\n",
      "episode: 814\t steps: 07155\t total steps:3568340\t epsilon: 0.10\n",
      "episode: 815\t steps: 07347\t total steps:3575687\t epsilon: 0.10\n",
      "episode: 816\t steps: 03965\t total steps:3579652\t epsilon: 0.10\n",
      "episode: 817\t steps: 10000\t total steps:3589652\t epsilon: 0.10\n",
      "episode: 818\t steps: 06163\t total steps:3595815\t epsilon: 0.10\n",
      "episode: 819\t steps: 02842\t total steps:3598657\t epsilon: 0.10\n",
      "episode: 820\t steps: 01404\t total steps:3600061\t epsilon: 0.10\n",
      "episode: 821\t steps: 01090\t total steps:3601151\t epsilon: 0.10\n",
      "episode: 822\t steps: 06060\t total steps:3607211\t epsilon: 0.10\n",
      "episode: 823\t steps: 02522\t total steps:3609733\t epsilon: 0.10\n",
      "episode: 824\t steps: 02745\t total steps:3612478\t epsilon: 0.10\n",
      "episode: 825\t steps: 06143\t total steps:3618621\t epsilon: 0.10\n",
      "episode: 826\t steps: 03461\t total steps:3622082\t epsilon: 0.10\n",
      "episode: 827\t steps: 07933\t total steps:3630015\t epsilon: 0.10\n",
      "episode: 828\t steps: 10000\t total steps:3640015\t epsilon: 0.10\n",
      "episode: 829\t steps: 02008\t total steps:3642023\t epsilon: 0.10\n",
      "episode: 830\t steps: 08858\t total steps:3650881\t epsilon: 0.10\n",
      "episode: 831\t steps: 01642\t total steps:3652523\t epsilon: 0.10\n",
      "episode: 832\t steps: 04789\t total steps:3657312\t epsilon: 0.10\n",
      "episode: 833\t steps: 02917\t total steps:3660229\t epsilon: 0.10\n",
      "episode: 834\t steps: 04513\t total steps:3664742\t epsilon: 0.10\n",
      "episode: 835\t steps: 01700\t total steps:3666442\t epsilon: 0.10\n",
      "episode: 836\t steps: 05278\t total steps:3671720\t epsilon: 0.10\n",
      "episode: 837\t steps: 03867\t total steps:3675587\t epsilon: 0.10\n",
      "episode: 838\t steps: 01744\t total steps:3677331\t epsilon: 0.10\n",
      "episode: 839\t steps: 04342\t total steps:3681673\t epsilon: 0.10\n",
      "episode: 840\t steps: 02560\t total steps:3684233\t epsilon: 0.10\n",
      "episode: 841\t steps: 05096\t total steps:3689329\t epsilon: 0.10\n",
      "episode: 842\t steps: 10000\t total steps:3699329\t epsilon: 0.10\n",
      "episode: 843\t steps: 06562\t total steps:3705891\t epsilon: 0.10\n",
      "episode: 844\t steps: 07238\t total steps:3713129\t epsilon: 0.10\n",
      "episode: 845\t steps: 07167\t total steps:3720296\t epsilon: 0.10\n",
      "episode: 846\t steps: 08294\t total steps:3728590\t epsilon: 0.10\n",
      "episode: 847\t steps: 04012\t total steps:3732602\t epsilon: 0.10\n",
      "episode: 848\t steps: 08037\t total steps:3740639\t epsilon: 0.10\n",
      "episode: 849\t steps: 01537\t total steps:3742176\t epsilon: 0.10\n",
      "episode: 850\t steps: 03246\t total steps:3745422\t epsilon: 0.10\n",
      "Evaluation: 17\t average reward: -10000.0\n",
      "episode: 851\t steps: 06424\t total steps:3751846\t epsilon: 0.10\n",
      "episode: 852\t steps: 05881\t total steps:3757727\t epsilon: 0.10\n",
      "episode: 853\t steps: 00841\t total steps:3758568\t epsilon: 0.10\n",
      "episode: 854\t steps: 03849\t total steps:3762417\t epsilon: 0.10\n",
      "episode: 855\t steps: 01136\t total steps:3763553\t epsilon: 0.10\n",
      "episode: 856\t steps: 10000\t total steps:3773553\t epsilon: 0.10\n",
      "episode: 857\t steps: 03277\t total steps:3776830\t epsilon: 0.10\n",
      "episode: 858\t steps: 07639\t total steps:3784469\t epsilon: 0.10\n",
      "episode: 859\t steps: 09461\t total steps:3793930\t epsilon: 0.10\n",
      "episode: 860\t steps: 10000\t total steps:3803930\t epsilon: 0.10\n",
      "episode: 861\t steps: 10000\t total steps:3813930\t epsilon: 0.10\n",
      "episode: 862\t steps: 04872\t total steps:3818802\t epsilon: 0.10\n",
      "episode: 863\t steps: 02796\t total steps:3821598\t epsilon: 0.10\n",
      "episode: 864\t steps: 00732\t total steps:3822330\t epsilon: 0.10\n",
      "episode: 865\t steps: 02188\t total steps:3824518\t epsilon: 0.10\n",
      "episode: 866\t steps: 05992\t total steps:3830510\t epsilon: 0.10\n",
      "episode: 867\t steps: 03993\t total steps:3834503\t epsilon: 0.10\n",
      "episode: 868\t steps: 01593\t total steps:3836096\t epsilon: 0.10\n",
      "episode: 869\t steps: 04045\t total steps:3840141\t epsilon: 0.10\n",
      "episode: 870\t steps: 09834\t total steps:3849975\t epsilon: 0.10\n",
      "episode: 871\t steps: 05453\t total steps:3855428\t epsilon: 0.10\n",
      "episode: 872\t steps: 02761\t total steps:3858189\t epsilon: 0.10\n",
      "episode: 873\t steps: 02254\t total steps:3860443\t epsilon: 0.10\n",
      "episode: 874\t steps: 01604\t total steps:3862047\t epsilon: 0.10\n",
      "episode: 875\t steps: 06932\t total steps:3868979\t epsilon: 0.10\n",
      "episode: 876\t steps: 10000\t total steps:3878979\t epsilon: 0.10\n",
      "episode: 877\t steps: 04122\t total steps:3883101\t epsilon: 0.10\n",
      "episode: 878\t steps: 03255\t total steps:3886356\t epsilon: 0.10\n",
      "episode: 879\t steps: 00593\t total steps:3886949\t epsilon: 0.10\n",
      "episode: 880\t steps: 03121\t total steps:3890070\t epsilon: 0.10\n",
      "episode: 881\t steps: 03620\t total steps:3893690\t epsilon: 0.10\n",
      "episode: 882\t steps: 04249\t total steps:3897939\t epsilon: 0.10\n",
      "episode: 883\t steps: 03969\t total steps:3901908\t epsilon: 0.10\n",
      "episode: 884\t steps: 10000\t total steps:3911908\t epsilon: 0.10\n",
      "episode: 885\t steps: 01782\t total steps:3913690\t epsilon: 0.10\n",
      "episode: 886\t steps: 02291\t total steps:3915981\t epsilon: 0.10\n",
      "episode: 887\t steps: 04385\t total steps:3920366\t epsilon: 0.10\n",
      "episode: 888\t steps: 08492\t total steps:3928858\t epsilon: 0.10\n",
      "episode: 889\t steps: 03671\t total steps:3932529\t epsilon: 0.10\n",
      "episode: 890\t steps: 00700\t total steps:3933229\t epsilon: 0.10\n",
      "episode: 891\t steps: 05096\t total steps:3938325\t epsilon: 0.10\n",
      "episode: 892\t steps: 03320\t total steps:3941645\t epsilon: 0.10\n",
      "episode: 893\t steps: 03024\t total steps:3944669\t epsilon: 0.10\n",
      "episode: 894\t steps: 00919\t total steps:3945588\t epsilon: 0.10\n",
      "episode: 895\t steps: 03670\t total steps:3949258\t epsilon: 0.10\n",
      "episode: 896\t steps: 01932\t total steps:3951190\t epsilon: 0.10\n",
      "episode: 897\t steps: 08202\t total steps:3959392\t epsilon: 0.10\n",
      "episode: 898\t steps: 02766\t total steps:3962158\t epsilon: 0.10\n",
      "episode: 899\t steps: 04010\t total steps:3966168\t epsilon: 0.10\n",
      "episode: 900\t steps: 01657\t total steps:3967825\t epsilon: 0.10\n",
      "Evaluation: 18\t average reward: -10000.0\n",
      "episode: 901\t steps: 00941\t total steps:3968766\t epsilon: 0.10\n",
      "episode: 902\t steps: 03725\t total steps:3972491\t epsilon: 0.10\n",
      "episode: 903\t steps: 10000\t total steps:3982491\t epsilon: 0.10\n",
      "episode: 904\t steps: 03113\t total steps:3985604\t epsilon: 0.10\n",
      "episode: 905\t steps: 04842\t total steps:3990446\t epsilon: 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 906\t steps: 03240\t total steps:3993686\t epsilon: 0.10\n",
      "episode: 907\t steps: 10000\t total steps:4003686\t epsilon: 0.10\n",
      "episode: 908\t steps: 01666\t total steps:4005352\t epsilon: 0.10\n",
      "episode: 909\t steps: 05222\t total steps:4010574\t epsilon: 0.10\n",
      "episode: 910\t steps: 10000\t total steps:4020574\t epsilon: 0.10\n",
      "episode: 911\t steps: 08171\t total steps:4028745\t epsilon: 0.10\n",
      "episode: 912\t steps: 01249\t total steps:4029994\t epsilon: 0.10\n",
      "episode: 913\t steps: 10000\t total steps:4039994\t epsilon: 0.10\n",
      "episode: 914\t steps: 03121\t total steps:4043115\t epsilon: 0.10\n",
      "episode: 915\t steps: 05187\t total steps:4048302\t epsilon: 0.10\n",
      "episode: 916\t steps: 06531\t total steps:4054833\t epsilon: 0.10\n",
      "episode: 917\t steps: 10000\t total steps:4064833\t epsilon: 0.10\n",
      "episode: 918\t steps: 03337\t total steps:4068170\t epsilon: 0.10\n",
      "episode: 919\t steps: 09449\t total steps:4077619\t epsilon: 0.10\n",
      "episode: 920\t steps: 04573\t total steps:4082192\t epsilon: 0.10\n",
      "episode: 921\t steps: 01007\t total steps:4083199\t epsilon: 0.10\n",
      "episode: 922\t steps: 05801\t total steps:4089000\t epsilon: 0.10\n",
      "episode: 923\t steps: 04727\t total steps:4093727\t epsilon: 0.10\n",
      "episode: 924\t steps: 05656\t total steps:4099383\t epsilon: 0.10\n",
      "episode: 925\t steps: 01429\t total steps:4100812\t epsilon: 0.10\n",
      "episode: 926\t steps: 05574\t total steps:4106386\t epsilon: 0.10\n",
      "episode: 927\t steps: 00475\t total steps:4106861\t epsilon: 0.10\n",
      "episode: 928\t steps: 03992\t total steps:4110853\t epsilon: 0.10\n",
      "episode: 929\t steps: 02570\t total steps:4113423\t epsilon: 0.10\n",
      "episode: 930\t steps: 10000\t total steps:4123423\t epsilon: 0.10\n",
      "episode: 931\t steps: 02819\t total steps:4126242\t epsilon: 0.10\n",
      "episode: 932\t steps: 04556\t total steps:4130798\t epsilon: 0.10\n",
      "episode: 933\t steps: 02768\t total steps:4133566\t epsilon: 0.10\n",
      "episode: 934\t steps: 05413\t total steps:4138979\t epsilon: 0.10\n",
      "episode: 935\t steps: 03714\t total steps:4142693\t epsilon: 0.10\n",
      "episode: 936\t steps: 04405\t total steps:4147098\t epsilon: 0.10\n",
      "episode: 937\t steps: 01176\t total steps:4148274\t epsilon: 0.10\n",
      "episode: 938\t steps: 00718\t total steps:4148992\t epsilon: 0.10\n",
      "episode: 939\t steps: 02642\t total steps:4151634\t epsilon: 0.10\n",
      "episode: 940\t steps: 01720\t total steps:4153354\t epsilon: 0.10\n",
      "episode: 941\t steps: 04513\t total steps:4157867\t epsilon: 0.10\n",
      "episode: 942\t steps: 03230\t total steps:4161097\t epsilon: 0.10\n",
      "episode: 943\t steps: 09086\t total steps:4170183\t epsilon: 0.10\n",
      "episode: 944\t steps: 03524\t total steps:4173707\t epsilon: 0.10\n",
      "episode: 945\t steps: 02915\t total steps:4176622\t epsilon: 0.10\n",
      "episode: 946\t steps: 02634\t total steps:4179256\t epsilon: 0.10\n",
      "episode: 947\t steps: 06297\t total steps:4185553\t epsilon: 0.10\n",
      "episode: 948\t steps: 01512\t total steps:4187065\t epsilon: 0.10\n",
      "episode: 949\t steps: 01961\t total steps:4189026\t epsilon: 0.10\n",
      "episode: 950\t steps: 05239\t total steps:4194265\t epsilon: 0.10\n",
      "Evaluation: 19\t average reward: -10000.0\n",
      "episode: 951\t steps: 00808\t total steps:4195073\t epsilon: 0.10\n",
      "episode: 952\t steps: 07145\t total steps:4202218\t epsilon: 0.10\n",
      "episode: 953\t steps: 02802\t total steps:4205020\t epsilon: 0.10\n",
      "episode: 954\t steps: 01105\t total steps:4206125\t epsilon: 0.10\n",
      "episode: 955\t steps: 01837\t total steps:4207962\t epsilon: 0.10\n",
      "episode: 956\t steps: 03224\t total steps:4211186\t epsilon: 0.10\n",
      "episode: 957\t steps: 01238\t total steps:4212424\t epsilon: 0.10\n",
      "episode: 958\t steps: 02469\t total steps:4214893\t epsilon: 0.10\n",
      "episode: 959\t steps: 01138\t total steps:4216031\t epsilon: 0.10\n",
      "episode: 960\t steps: 01878\t total steps:4217909\t epsilon: 0.10\n",
      "episode: 961\t steps: 03378\t total steps:4221287\t epsilon: 0.10\n",
      "episode: 962\t steps: 04265\t total steps:4225552\t epsilon: 0.10\n",
      "episode: 963\t steps: 04678\t total steps:4230230\t epsilon: 0.10\n",
      "episode: 964\t steps: 01599\t total steps:4231829\t epsilon: 0.10\n",
      "episode: 965\t steps: 02969\t total steps:4234798\t epsilon: 0.10\n",
      "episode: 966\t steps: 01922\t total steps:4236720\t epsilon: 0.10\n",
      "episode: 967\t steps: 08785\t total steps:4245505\t epsilon: 0.10\n",
      "episode: 968\t steps: 00829\t total steps:4246334\t epsilon: 0.10\n",
      "episode: 969\t steps: 03170\t total steps:4249504\t epsilon: 0.10\n",
      "episode: 970\t steps: 05484\t total steps:4254988\t epsilon: 0.10\n",
      "episode: 971\t steps: 01261\t total steps:4256249\t epsilon: 0.10\n",
      "episode: 972\t steps: 04458\t total steps:4260707\t epsilon: 0.10\n",
      "episode: 973\t steps: 01981\t total steps:4262688\t epsilon: 0.10\n",
      "episode: 974\t steps: 02834\t total steps:4265522\t epsilon: 0.10\n",
      "episode: 975\t steps: 05119\t total steps:4270641\t epsilon: 0.10\n",
      "episode: 976\t steps: 04728\t total steps:4275369\t epsilon: 0.10\n",
      "episode: 977\t steps: 00914\t total steps:4276283\t epsilon: 0.10\n",
      "episode: 978\t steps: 01250\t total steps:4277533\t epsilon: 0.10\n",
      "episode: 979\t steps: 01996\t total steps:4279529\t epsilon: 0.10\n",
      "episode: 980\t steps: 05862\t total steps:4285391\t epsilon: 0.10\n",
      "episode: 981\t steps: 08989\t total steps:4294380\t epsilon: 0.10\n",
      "episode: 982\t steps: 06532\t total steps:4300912\t epsilon: 0.10\n",
      "episode: 983\t steps: 10000\t total steps:4310912\t epsilon: 0.10\n",
      "episode: 984\t steps: 08118\t total steps:4319030\t epsilon: 0.10\n",
      "episode: 985\t steps: 01464\t total steps:4320494\t epsilon: 0.10\n",
      "episode: 986\t steps: 10000\t total steps:4330494\t epsilon: 0.10\n",
      "episode: 987\t steps: 05217\t total steps:4335711\t epsilon: 0.10\n",
      "episode: 988\t steps: 02992\t total steps:4338703\t epsilon: 0.10\n",
      "episode: 989\t steps: 10000\t total steps:4348703\t epsilon: 0.10\n",
      "episode: 990\t steps: 02965\t total steps:4351668\t epsilon: 0.10\n",
      "episode: 991\t steps: 04348\t total steps:4356016\t epsilon: 0.10\n",
      "episode: 992\t steps: 00773\t total steps:4356789\t epsilon: 0.10\n",
      "episode: 993\t steps: 04693\t total steps:4361482\t epsilon: 0.10\n",
      "episode: 994\t steps: 06694\t total steps:4368176\t epsilon: 0.10\n",
      "episode: 995\t steps: 02484\t total steps:4370660\t epsilon: 0.10\n",
      "episode: 996\t steps: 06010\t total steps:4376670\t epsilon: 0.10\n",
      "episode: 997\t steps: 04303\t total steps:4380973\t epsilon: 0.10\n",
      "episode: 998\t steps: 10000\t total steps:4390973\t epsilon: 0.10\n",
      "episode: 999\t steps: 06001\t total steps:4396974\t epsilon: 0.10\n",
      "Evaluation: 19\t average reward: -10000.0\n",
      "episode: 1000\t steps: 04523\t total steps:4401497\t epsilon: 0.10\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(best_policy, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/DQN_MLP_best.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(final_policy, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/DQN_MLP_final.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavetxt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/td_error_DQN_MLP.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtd_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m np\u001b[38;5;241m.\u001b[39msavetxt(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/steps_DQN_MLP_txt\u001b[39m\u001b[38;5;124m'\u001b[39m, total_steps_list)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# np.savetxt(f'data/q_values_DQN.txt', q_measures)\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msavetxt\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py:1533\u001b[0m, in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfname must be a string or file handle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1533\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1535\u001b[0m     \u001b[38;5;66;03m# Handle 1-dimensional arrays\u001b[39;00m\n\u001b[1;32m   1536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py:956\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 956\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "n_training_episodes = 1000\n",
    "gamma = 0.99\n",
    "learning_rate = 0.00025  # 0.1\n",
    "max_training_steps = 10000\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.1\n",
    "eval_epsilon = 0.05\n",
    "# epsilon_frame = 100000\n",
    "\n",
    "# replay memory parameters\n",
    "replay_size = 200000\n",
    "batch_size = 32\n",
    "\n",
    "# fixed target network\n",
    "fixed_target = True\n",
    "copy_target = 10000\n",
    "\n",
    "debug = True\n",
    "\n",
    "epsilon_frame = 1000000\n",
    "\n",
    "min_replay = 80000\n",
    "\n",
    "transform = scale_and_resize()\n",
    "\n",
    "env = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
    "\n",
    "car = TrainMountainCar(n_training_episodes=n_training_episodes, gamma=gamma, learning_rate=learning_rate,\n",
    "                       epsilon_max=epsilon_max, epsilon_min=epsilon_min, min_replay=min_replay,\n",
    "                       max_steps=max_training_steps, batch_size=batch_size, fixed_target=fixed_target,\n",
    "                       copy_target=copy_target, debug=debug, env=env, epsilon_frame=epsilon_frame,\n",
    "                       eval_epsilon=eval_epsilon)\n",
    "\n",
    "total_rewards, total_steps_list, best_policy, evaluations, td_error, final_policy = car.train()\n",
    "\n",
    "# save best policy as well as steps and q measures\n",
    "torch.save(best_policy, 'data/DQN_MLP_best.pth')\n",
    "torch.save(final_policy, 'data/DQN_MLP_final.pth')\n",
    "np.savetxt(f'data/td_error_DQN_MLP.txt', td_error)\n",
    "np.savetxt(f'data/steps_DQN_MLP_txt', total_steps_list)\n",
    "# np.savetxt(f'data/q_values_DQN.txt', q_measures)\n",
    "np.savetxt(f'data/eval_DQN_<MLP.txt', evaluations)\n",
    "\n",
    "# Plot steps per episode\n",
    "plt.plot(np.arange(len(total_steps_list)) + 1, total_steps_list, zorder=0, label='training')\n",
    "x = np.arange(50, n_training_episodes+1, 50)\n",
    "plt.scatter(x, [-e for e in evaluations], color='r', marker='x', zorder=1, label='evaluations')\n",
    "N = 10\n",
    "steps_mean = running_mean(total_steps_list, N)\n",
    "plt.plot(np.arange(len(steps_mean)) + 1, steps_mean, zorder=0, label='running average')\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps')\n",
    "plt.title('Steps per Episode - DQN_MLP')\n",
    "plt.savefig('plots/steps_DQN_MLP.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot q measures per episode\n",
    "# plt.plot(np.arange(len(q_measures)) + 1, q_measures)\n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Average Q')\n",
    "# plt.title('Average Q measure over sampled states')\n",
    "# plt.savefig('plots/q_measures_DQN.png')\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f7ef4c",
   "metadata": {},
   "source": [
    "## DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab0f184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 001\t steps: 02307\t total steps:002308\t epsilon: 1.00\n",
      "episode: 002\t steps: 10000\t total steps:012308\t epsilon: 0.98\n",
      "episode: 003\t steps: 04551\t total steps:016859\t epsilon: 0.97\n",
      "episode: 004\t steps: 10000\t total steps:026859\t epsilon: 0.95\n",
      "episode: 005\t steps: 10000\t total steps:036859\t epsilon: 0.93\n",
      "episode: 006\t steps: 10000\t total steps:046859\t epsilon: 0.92\n",
      "episode: 007\t steps: 10000\t total steps:056859\t epsilon: 0.90\n",
      "episode: 008\t steps: 10000\t total steps:066859\t epsilon: 0.88\n",
      "episode: 009\t steps: 10000\t total steps:076859\t epsilon: 0.86\n",
      "episode: 010\t steps: 10000\t total steps:086859\t epsilon: 0.84\n",
      "episode: 011\t steps: 04503\t total steps:091362\t epsilon: 0.84\n",
      "episode: 012\t steps: 06360\t total steps:097722\t epsilon: 0.82\n",
      "episode: 013\t steps: 07937\t total steps:105659\t epsilon: 0.81\n",
      "episode: 014\t steps: 01681\t total steps:107340\t epsilon: 0.81\n",
      "episode: 015\t steps: 10000\t total steps:117340\t epsilon: 0.79\n",
      "episode: 016\t steps: 04707\t total steps:122047\t epsilon: 0.78\n",
      "episode: 017\t steps: 06196\t total steps:128243\t epsilon: 0.77\n",
      "episode: 018\t steps: 06600\t total steps:134843\t epsilon: 0.76\n",
      "episode: 019\t steps: 02132\t total steps:136975\t epsilon: 0.75\n",
      "episode: 020\t steps: 04850\t total steps:141825\t epsilon: 0.74\n",
      "episode: 021\t steps: 09390\t total steps:151215\t epsilon: 0.73\n",
      "episode: 022\t steps: 06746\t total steps:157961\t epsilon: 0.72\n",
      "episode: 023\t steps: 05797\t total steps:163758\t epsilon: 0.71\n",
      "episode: 024\t steps: 10000\t total steps:173758\t epsilon: 0.69\n",
      "episode: 025\t steps: 10000\t total steps:183758\t epsilon: 0.67\n",
      "episode: 026\t steps: 09023\t total steps:192781\t epsilon: 0.65\n",
      "episode: 027\t steps: 07699\t total steps:200480\t epsilon: 0.64\n",
      "episode: 028\t steps: 05932\t total steps:206412\t epsilon: 0.63\n",
      "episode: 029\t steps: 03632\t total steps:210044\t epsilon: 0.62\n",
      "episode: 030\t steps: 10000\t total steps:220044\t epsilon: 0.60\n",
      "episode: 031\t steps: 05206\t total steps:225250\t epsilon: 0.59\n",
      "episode: 032\t steps: 01579\t total steps:226829\t epsilon: 0.59\n",
      "episode: 033\t steps: 02059\t total steps:228888\t epsilon: 0.59\n",
      "episode: 034\t steps: 04735\t total steps:233623\t epsilon: 0.58\n",
      "episode: 035\t steps: 05064\t total steps:238687\t epsilon: 0.57\n",
      "episode: 036\t steps: 04038\t total steps:242725\t epsilon: 0.56\n",
      "episode: 037\t steps: 05057\t total steps:247782\t epsilon: 0.55\n",
      "episode: 038\t steps: 07104\t total steps:254886\t epsilon: 0.54\n",
      "episode: 039\t steps: 04722\t total steps:259608\t epsilon: 0.53\n",
      "episode: 040\t steps: 10000\t total steps:269608\t epsilon: 0.51\n",
      "episode: 041\t steps: 03664\t total steps:273272\t epsilon: 0.51\n",
      "episode: 042\t steps: 03491\t total steps:276763\t epsilon: 0.50\n",
      "episode: 043\t steps: 06638\t total steps:283401\t epsilon: 0.49\n",
      "episode: 044\t steps: 08527\t total steps:291928\t epsilon: 0.47\n",
      "episode: 045\t steps: 09992\t total steps:301920\t epsilon: 0.46\n",
      "episode: 046\t steps: 10000\t total steps:311920\t epsilon: 0.44\n",
      "episode: 047\t steps: 03168\t total steps:315088\t epsilon: 0.43\n",
      "episode: 048\t steps: 01931\t total steps:317019\t epsilon: 0.43\n",
      "episode: 049\t steps: 01608\t total steps:318627\t epsilon: 0.43\n",
      "episode: 050\t steps: 05770\t total steps:324397\t epsilon: 0.42\n",
      "Evaluation: 1\t average reward: -9922.6\n",
      "episode: 051\t steps: 01249\t total steps:325646\t epsilon: 0.41\n",
      "episode: 052\t steps: 01891\t total steps:327537\t epsilon: 0.41\n",
      "episode: 053\t steps: 09257\t total steps:336794\t epsilon: 0.39\n",
      "episode: 054\t steps: 08755\t total steps:345549\t epsilon: 0.38\n",
      "episode: 055\t steps: 01704\t total steps:347253\t epsilon: 0.37\n",
      "episode: 056\t steps: 06525\t total steps:353778\t epsilon: 0.36\n",
      "episode: 057\t steps: 10000\t total steps:363778\t epsilon: 0.35\n",
      "episode: 058\t steps: 01862\t total steps:365640\t epsilon: 0.34\n",
      "episode: 059\t steps: 04763\t total steps:370403\t epsilon: 0.33\n",
      "episode: 060\t steps: 00931\t total steps:371334\t epsilon: 0.33\n",
      "episode: 061\t steps: 00645\t total steps:371979\t epsilon: 0.33\n",
      "episode: 062\t steps: 01730\t total steps:373709\t epsilon: 0.33\n",
      "episode: 063\t steps: 07717\t total steps:381426\t epsilon: 0.31\n",
      "episode: 064\t steps: 08437\t total steps:389863\t epsilon: 0.30\n",
      "episode: 065\t steps: 04217\t total steps:394080\t epsilon: 0.29\n",
      "episode: 066\t steps: 03925\t total steps:398005\t epsilon: 0.28\n",
      "episode: 067\t steps: 00394\t total steps:398399\t epsilon: 0.28\n",
      "episode: 068\t steps: 01274\t total steps:399673\t epsilon: 0.28\n",
      "episode: 069\t steps: 08015\t total steps:407688\t epsilon: 0.27\n",
      "episode: 070\t steps: 09716\t total steps:417404\t epsilon: 0.25\n",
      "episode: 071\t steps: 03829\t total steps:421233\t epsilon: 0.24\n",
      "episode: 072\t steps: 02479\t total steps:423712\t epsilon: 0.24\n",
      "episode: 073\t steps: 01506\t total steps:425218\t epsilon: 0.23\n",
      "episode: 074\t steps: 09413\t total steps:434631\t epsilon: 0.22\n",
      "episode: 075\t steps: 05299\t total steps:439930\t epsilon: 0.21\n",
      "episode: 076\t steps: 01595\t total steps:441525\t epsilon: 0.21\n",
      "episode: 077\t steps: 02016\t total steps:443541\t epsilon: 0.20\n",
      "episode: 078\t steps: 01320\t total steps:444861\t epsilon: 0.20\n",
      "episode: 079\t steps: 01720\t total steps:446581\t epsilon: 0.20\n",
      "episode: 080\t steps: 06487\t total steps:453068\t epsilon: 0.18\n",
      "episode: 081\t steps: 01164\t total steps:454232\t epsilon: 0.18\n",
      "episode: 082\t steps: 01235\t total steps:455467\t epsilon: 0.18\n",
      "episode: 083\t steps: 00904\t total steps:456371\t epsilon: 0.18\n",
      "episode: 084\t steps: 02403\t total steps:458774\t epsilon: 0.17\n",
      "episode: 085\t steps: 02412\t total steps:461186\t epsilon: 0.17\n",
      "episode: 086\t steps: 05622\t total steps:466808\t epsilon: 0.16\n",
      "episode: 087\t steps: 02298\t total steps:469106\t epsilon: 0.16\n",
      "episode: 088\t steps: 02110\t total steps:471216\t epsilon: 0.15\n",
      "episode: 089\t steps: 03936\t total steps:475152\t epsilon: 0.14\n",
      "episode: 090\t steps: 01518\t total steps:476670\t epsilon: 0.14\n",
      "episode: 091\t steps: 04422\t total steps:481092\t epsilon: 0.13\n",
      "episode: 092\t steps: 01150\t total steps:482242\t epsilon: 0.13\n",
      "episode: 093\t steps: 04175\t total steps:486417\t epsilon: 0.12\n",
      "episode: 094\t steps: 01172\t total steps:487589\t epsilon: 0.12\n",
      "episode: 095\t steps: 09738\t total steps:497327\t epsilon: 0.10\n",
      "episode: 096\t steps: 01646\t total steps:498973\t epsilon: 0.10\n",
      "episode: 097\t steps: 00336\t total steps:499309\t epsilon: 0.10\n",
      "episode: 098\t steps: 07316\t total steps:506625\t epsilon: 0.10\n",
      "episode: 099\t steps: 01981\t total steps:508606\t epsilon: 0.10\n",
      "episode: 100\t steps: 01418\t total steps:510024\t epsilon: 0.10\n",
      "Evaluation: 2\t average reward: -10000.0\n",
      "episode: 101\t steps: 02895\t total steps:512919\t epsilon: 0.10\n",
      "episode: 102\t steps: 03400\t total steps:516319\t epsilon: 0.10\n",
      "episode: 103\t steps: 03923\t total steps:520242\t epsilon: 0.10\n",
      "episode: 104\t steps: 01833\t total steps:522075\t epsilon: 0.10\n",
      "episode: 105\t steps: 02972\t total steps:525047\t epsilon: 0.10\n",
      "episode: 106\t steps: 01069\t total steps:526116\t epsilon: 0.10\n",
      "episode: 107\t steps: 01957\t total steps:528073\t epsilon: 0.10\n",
      "episode: 108\t steps: 05350\t total steps:533423\t epsilon: 0.10\n",
      "episode: 109\t steps: 03826\t total steps:537249\t epsilon: 0.10\n",
      "episode: 110\t steps: 03842\t total steps:541091\t epsilon: 0.10\n",
      "episode: 111\t steps: 00860\t total steps:541951\t epsilon: 0.10\n",
      "episode: 112\t steps: 00782\t total steps:542733\t epsilon: 0.10\n",
      "episode: 113\t steps: 04889\t total steps:547622\t epsilon: 0.10\n",
      "episode: 114\t steps: 03057\t total steps:550679\t epsilon: 0.10\n",
      "episode: 115\t steps: 00302\t total steps:550981\t epsilon: 0.10\n",
      "episode: 116\t steps: 01175\t total steps:552156\t epsilon: 0.10\n",
      "episode: 117\t steps: 01176\t total steps:553332\t epsilon: 0.10\n",
      "episode: 118\t steps: 00833\t total steps:554165\t epsilon: 0.10\n",
      "episode: 119\t steps: 00507\t total steps:554672\t epsilon: 0.10\n",
      "episode: 120\t steps: 01543\t total steps:556215\t epsilon: 0.10\n",
      "episode: 121\t steps: 01854\t total steps:558069\t epsilon: 0.10\n",
      "episode: 122\t steps: 00660\t total steps:558729\t epsilon: 0.10\n",
      "episode: 123\t steps: 01853\t total steps:560582\t epsilon: 0.10\n",
      "episode: 124\t steps: 04999\t total steps:565581\t epsilon: 0.10\n",
      "episode: 125\t steps: 03077\t total steps:568658\t epsilon: 0.10\n",
      "episode: 126\t steps: 01572\t total steps:570230\t epsilon: 0.10\n",
      "episode: 127\t steps: 05213\t total steps:575443\t epsilon: 0.10\n",
      "episode: 128\t steps: 02902\t total steps:578345\t epsilon: 0.10\n",
      "episode: 129\t steps: 05658\t total steps:584003\t epsilon: 0.10\n",
      "episode: 130\t steps: 08209\t total steps:592212\t epsilon: 0.10\n",
      "episode: 131\t steps: 02491\t total steps:594703\t epsilon: 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 132\t steps: 03067\t total steps:597770\t epsilon: 0.10\n",
      "episode: 133\t steps: 00475\t total steps:598245\t epsilon: 0.10\n",
      "episode: 134\t steps: 03576\t total steps:601821\t epsilon: 0.10\n",
      "episode: 135\t steps: 03558\t total steps:605379\t epsilon: 0.10\n",
      "episode: 136\t steps: 00904\t total steps:606283\t epsilon: 0.10\n",
      "episode: 137\t steps: 06388\t total steps:612671\t epsilon: 0.10\n",
      "episode: 138\t steps: 03143\t total steps:615814\t epsilon: 0.10\n",
      "episode: 139\t steps: 01442\t total steps:617256\t epsilon: 0.10\n",
      "episode: 140\t steps: 06444\t total steps:623700\t epsilon: 0.10\n",
      "episode: 141\t steps: 02344\t total steps:626044\t epsilon: 0.10\n",
      "episode: 142\t steps: 02239\t total steps:628283\t epsilon: 0.10\n",
      "episode: 143\t steps: 01177\t total steps:629460\t epsilon: 0.10\n",
      "episode: 144\t steps: 00746\t total steps:630206\t epsilon: 0.10\n",
      "episode: 145\t steps: 05541\t total steps:635747\t epsilon: 0.10\n",
      "episode: 146\t steps: 02937\t total steps:638684\t epsilon: 0.10\n",
      "episode: 147\t steps: 04574\t total steps:643258\t epsilon: 0.10\n",
      "episode: 148\t steps: 09210\t total steps:652468\t epsilon: 0.10\n",
      "episode: 149\t steps: 01784\t total steps:654252\t epsilon: 0.10\n",
      "episode: 150\t steps: 02434\t total steps:656686\t epsilon: 0.10\n",
      "Evaluation: 3\t average reward: -10000.0\n",
      "episode: 151\t steps: 03083\t total steps:659769\t epsilon: 0.10\n",
      "episode: 152\t steps: 06126\t total steps:665895\t epsilon: 0.10\n",
      "episode: 153\t steps: 01006\t total steps:666901\t epsilon: 0.10\n",
      "episode: 154\t steps: 04765\t total steps:671666\t epsilon: 0.10\n",
      "episode: 155\t steps: 02721\t total steps:674387\t epsilon: 0.10\n",
      "episode: 156\t steps: 04031\t total steps:678418\t epsilon: 0.10\n",
      "episode: 157\t steps: 05689\t total steps:684107\t epsilon: 0.10\n",
      "episode: 158\t steps: 01831\t total steps:685938\t epsilon: 0.10\n",
      "episode: 159\t steps: 04897\t total steps:690835\t epsilon: 0.10\n",
      "episode: 160\t steps: 01758\t total steps:692593\t epsilon: 0.10\n",
      "episode: 161\t steps: 07118\t total steps:699711\t epsilon: 0.10\n",
      "episode: 162\t steps: 01983\t total steps:701694\t epsilon: 0.10\n",
      "episode: 163\t steps: 02018\t total steps:703712\t epsilon: 0.10\n",
      "episode: 164\t steps: 04294\t total steps:708006\t epsilon: 0.10\n",
      "episode: 165\t steps: 01287\t total steps:709293\t epsilon: 0.10\n",
      "episode: 166\t steps: 01568\t total steps:710861\t epsilon: 0.10\n",
      "episode: 167\t steps: 10000\t total steps:720861\t epsilon: 0.10\n",
      "episode: 168\t steps: 03499\t total steps:724360\t epsilon: 0.10\n",
      "episode: 169\t steps: 00602\t total steps:724962\t epsilon: 0.10\n",
      "episode: 170\t steps: 04045\t total steps:729007\t epsilon: 0.10\n",
      "episode: 171\t steps: 05003\t total steps:734010\t epsilon: 0.10\n",
      "episode: 172\t steps: 02757\t total steps:736767\t epsilon: 0.10\n",
      "episode: 173\t steps: 08350\t total steps:745117\t epsilon: 0.10\n",
      "episode: 174\t steps: 00898\t total steps:746015\t epsilon: 0.10\n",
      "episode: 175\t steps: 03462\t total steps:749477\t epsilon: 0.10\n",
      "episode: 176\t steps: 01597\t total steps:751074\t epsilon: 0.10\n",
      "episode: 177\t steps: 00720\t total steps:751794\t epsilon: 0.10\n",
      "episode: 178\t steps: 04435\t total steps:756229\t epsilon: 0.10\n",
      "episode: 179\t steps: 10000\t total steps:766229\t epsilon: 0.10\n",
      "episode: 180\t steps: 01979\t total steps:768208\t epsilon: 0.10\n",
      "episode: 181\t steps: 03085\t total steps:771293\t epsilon: 0.10\n",
      "episode: 182\t steps: 01163\t total steps:772456\t epsilon: 0.10\n",
      "episode: 183\t steps: 01942\t total steps:774398\t epsilon: 0.10\n",
      "episode: 184\t steps: 02429\t total steps:776827\t epsilon: 0.10\n",
      "episode: 185\t steps: 02179\t total steps:779006\t epsilon: 0.10\n",
      "episode: 186\t steps: 03067\t total steps:782073\t epsilon: 0.10\n",
      "episode: 187\t steps: 06386\t total steps:788459\t epsilon: 0.10\n",
      "episode: 188\t steps: 04570\t total steps:793029\t epsilon: 0.10\n",
      "episode: 189\t steps: 09284\t total steps:802313\t epsilon: 0.10\n",
      "episode: 190\t steps: 01573\t total steps:803886\t epsilon: 0.10\n",
      "episode: 191\t steps: 02969\t total steps:806855\t epsilon: 0.10\n",
      "episode: 192\t steps: 05046\t total steps:811901\t epsilon: 0.10\n",
      "episode: 193\t steps: 04179\t total steps:816080\t epsilon: 0.10\n",
      "episode: 194\t steps: 04989\t total steps:821069\t epsilon: 0.10\n",
      "episode: 195\t steps: 00812\t total steps:821881\t epsilon: 0.10\n",
      "episode: 196\t steps: 01354\t total steps:823235\t epsilon: 0.10\n",
      "episode: 197\t steps: 04330\t total steps:827565\t epsilon: 0.10\n",
      "episode: 198\t steps: 00688\t total steps:828253\t epsilon: 0.10\n",
      "episode: 199\t steps: 04702\t total steps:832955\t epsilon: 0.10\n",
      "episode: 200\t steps: 02482\t total steps:835437\t epsilon: 0.10\n",
      "Evaluation: 4\t average reward: -10000.0\n",
      "episode: 201\t steps: 01500\t total steps:836937\t epsilon: 0.10\n",
      "episode: 202\t steps: 00862\t total steps:837799\t epsilon: 0.10\n",
      "episode: 203\t steps: 05133\t total steps:842932\t epsilon: 0.10\n",
      "episode: 204\t steps: 02692\t total steps:845624\t epsilon: 0.10\n",
      "episode: 205\t steps: 05907\t total steps:851531\t epsilon: 0.10\n",
      "episode: 206\t steps: 06505\t total steps:858036\t epsilon: 0.10\n",
      "episode: 207\t steps: 01004\t total steps:859040\t epsilon: 0.10\n",
      "episode: 208\t steps: 02437\t total steps:861477\t epsilon: 0.10\n",
      "episode: 209\t steps: 02754\t total steps:864231\t epsilon: 0.10\n",
      "episode: 210\t steps: 01491\t total steps:865722\t epsilon: 0.10\n"
     ]
    }
   ],
   "source": [
    "#  Hyperparameters\n",
    "n_training_episodes = 1000\n",
    "gamma = 0.99\n",
    "learning_rate = 0.00025  # 0.1\n",
    "max_training_steps = 10000\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.1\n",
    "eval_epsilon = 0.05\n",
    "# epsilon_frame = 100000\n",
    "\n",
    "# replay memory parameters\n",
    "replay_size = 200000\n",
    "batch_size = 32\n",
    "\n",
    "# fixed target network\n",
    "fixed_target = True\n",
    "copy_target = 10000\n",
    "\n",
    "debug = True\n",
    "\n",
    "double = True\n",
    "\n",
    "epsilon_frame = 500000\n",
    "min_replay = 80000\n",
    "\n",
    "\n",
    "transform = scale_and_resize()\n",
    "\n",
    "env = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
    "\n",
    "car = TrainMountainCar(n_training_episodes=n_training_episodes, gamma=gamma, learning_rate=learning_rate,\n",
    "                       epsilon_max=epsilon_max, epsilon_min=epsilon_min,double=double, min_replay=min_replay,\n",
    "                       max_steps=max_training_steps, batch_size=batch_size, fixed_target=fixed_target,\n",
    "                       copy_target=copy_target, debug=debug, env=env, epsilon_frame=epsilon_frame,\n",
    "                       eval_epsilon=eval_epsilon)\n",
    "\n",
    "total_rewards, total_steps_list, best_policy, evaluations, td_error, final_policy = car.train()\n",
    "\n",
    "# save best policy as well as steps and q measures\n",
    "torch.save(best_policy, 'data/DDQN_MLP_best.pth')\n",
    "torch.save(final_policy, 'data/DDQN_MLP_final.pth')\n",
    "np.savetxt(f'data/td_error_DDQN_MLP.txt', td_error)\n",
    "np.savetxt(f'data/steps_DDQN_MLP_txt', total_steps_list)\n",
    "# np.savetxt(f'data/q_values_DDQN_MLP.txt', q_measures)\n",
    "np.savetxt(f'data/eval_DDQN_MLP.txt', evaluations)\n",
    "\n",
    "# Plot steps per episode\n",
    "plt.plot(np.arange(len(total_steps_list)) + 1, total_steps_list, zorder=0, label='training')\n",
    "x = np.arange(50, n_training_episodes+1, 50)\n",
    "plt.scatter(x, [-e for e in evaluations], color='r', marker='x', zorder=1, label='evaluations')\n",
    "N = 10\n",
    "steps_mean = running_mean(total_steps_list, N)\n",
    "plt.plot(np.arange(len(steps_mean)) + 1, steps_mean, zorder=0, label='running average')\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps')\n",
    "plt.title('Steps per Episode - DDQN_MLP')\n",
    "plt.savefig('plots/steps_DDQN_MLP.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot q measures per episode\n",
    "# plt.plot(np.arange(len(q_measures)) + 1, q_measures)\n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Average Q')\n",
    "# plt.title('Average Q measure over sampled states')\n",
    "# plt.savefig('plots/q_measures_DRQN.png')\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c893640a",
   "metadata": {},
   "source": [
    "## Prioritized_DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "n_training_episodes = 1000\n",
    "gamma = 0.99\n",
    "learning_rate = 0.00025/4  # 0.1\n",
    "max_training_steps = 10000\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.01\n",
    "epsilon_frame = 500000\n",
    "\n",
    "# replay memory parameters\n",
    "replay_size = 200000\n",
    "batch_size = 32\n",
    "min_replay = 80000\n",
    "\n",
    "\n",
    "# fixed target network\n",
    "fixed_target = True\n",
    "copy_target = 30000\n",
    "\n",
    "\n",
    "debug = True\n",
    "double = True\n",
    "prioritized = True\n",
    "\n",
    "env = gym.make('MountainCar-v0', render_mode='rgb_array')\n",
    "\n",
    "\n",
    "car = TrainMountainCar(n_training_episodes=n_training_episodes, gamma=gamma, learning_rate=learning_rate,\n",
    "                       epsilon_max=epsilon_max, epsilon_min=epsilon_min, epsilon_frame=epsilon_frame,\n",
    "                       max_steps=max_training_steps, batch_size=batch_size, fixed_target=fixed_target,\n",
    "                       copy_target=copy_target, replay_size=replay_size, double=double, prioritized=prioritized,\n",
    "                       debug=debug, min_replay=min_replay)\n",
    "\n",
    "total_rewards, total_steps_list, best_policy, evaluations, td_error, final_policy = car.train()\n",
    "\n",
    "torch.save(best_policy, 'data/Prioritized_DDQN_MLP_best.pth')\n",
    "torch.save(final_policy, 'data/Prioritized_DDQN_MLP_final.pth')\n",
    "np.savetxt(f'data/td_error_Prioritized_DDQN_MLP.txt', td_error)\n",
    "np.savetxt(f'data/steps_Prioritized_DDQN_MLP.txt', total_steps_list)\n",
    "# np.savetxt(f'data/q_values_Prioritized_DDQN_MLP.txt', q_measures)\n",
    "np.savetxt(f'data/eval_Prioritized_DDQN_MLP.txt', evaluations)\n",
    "\n",
    "plt.plot(np.arange(len(total_steps_list)) + 1, total_steps_list, zorder=0, label='evaluations')\n",
    "x = np.arange(50, n_training_episodes+1, 50)\n",
    "plt.scatter(x, [-e for e in evaluations], color='r', marker='x', zorder=1, label='evaluations')\n",
    "N = 10\n",
    "steps_mean = running_mean(total_steps_list, N)\n",
    "plt.plot(np.arange(len(steps_mean)) + 1, steps_mean, zorder=0, label='running average')\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps')\n",
    "plt.title('Steps per Episode - Prioritized_DDQN_MLP')\n",
    "plt.savefig('plots/steps_Prioritized_DDQN_MLP.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75cb8df",
   "metadata": {},
   "source": [
    "## Dueling DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d6f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Hyperparameters\n",
    "n_training_episodes = 1000\n",
    "gamma = 0.99\n",
    "learning_rate = 0.00025  # 0.1\n",
    "max_training_steps = 10000\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.1\n",
    "eval_epsilon = 0.05\n",
    "\n",
    "# replay memory parameters\n",
    "replay_size = 200000\n",
    "batch_size = 32\n",
    "min_memory = 80000\n",
    "\n",
    "\n",
    "# fixed target network\n",
    "fixed_target = True\n",
    "copy_target = 30000\n",
    "\n",
    "\n",
    "debug = True\n",
    "double = True       # DDQN\n",
    "dueling = True      # Dueling Network\n",
    "\n",
    "car = TrainMountainCar(n_training_episodes=n_training_episodes, gamma=gamma, learning_rate=learning_rate,\n",
    "                       epsilon_max=epsilon_max, epsilon_min=epsilon_min, min_memory=min_memory,\n",
    "                       max_steps=max_training_steps, batch_size=batch_size, fixed_target=fixed_target,\n",
    "                       copy_target=copy_target, replay_size=replay_size, double=double, dueling=dueling, debug=debug,\n",
    "                       eval_epsilon=eval_epsilon)\n",
    "\n",
    "total_rewards, total_steps_list, best_policy, evaluations, td_error, final_policy = car.train()\n",
    "\n",
    "torch.save(best_policy, 'data/Dueling_DDQN_MLP_best.pth')\n",
    "torch.save(final_policy, 'data/Dueling_DDQN_MLP_final.pth')\n",
    "np.savetxt(f'data/steps_Dueling_DDQN_MLP.txt', total_steps_list)\n",
    "# np.savetxt(f'data/q_values_Dueling_DDQN_MLP.txt', q_measures)\n",
    "np.savetxt(f'data/eval_Dueling_DDQN_MLP.txt', evaluations)\n",
    "np.savetxt(f'data/td_error_Dueling_DDQN_MLP.txt', td_error)\n",
    "\n",
    "plt.plot(np.arange(len(total_steps_list)) + 1, total_steps_list, zorder=0, label='training')\n",
    "x = np.arange(50, n_training_episodes, 50)\n",
    "plt.scatter(x, [-e*4 for e in evaluations], color='r', marker='x', zorder=1, label='evaluations')\n",
    "N = 10\n",
    "steps_mean = running_mean(total_steps_list, N)\n",
    "plt.plot(np.arange(len(steps_mean)) + 1, steps_mean, zorder=0, label='running average')\n",
    "plt.legend()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps')\n",
    "plt.title('Steps per Episode - Dueling_DDQN_MLP')\n",
    "plt.savefig('plots/steps_Dueling_DDQN_MLP.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed22470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
